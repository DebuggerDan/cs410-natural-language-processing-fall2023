{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS410: Natural Language Processing, Fall 2023\n",
    "## A1: Sentiment Analysis Text Classification, Dan Jang - 10/9/2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description of Assignment\n",
    "\n",
    "##### Introduction\n",
    "\n",
    "##### Data Preparation\n",
    "A dataset containing product customer reviews, which is named the \"Multilingual Amazon Reviews Corpus\", in a json container format, with several columns. The assignment will focus on a smaller subset of the original dataset, where we will focus on __two (2) columns__:\n",
    "* \"review_title\" - self-explanatory\n",
    "* \"stars\" - an integer, either 1 or 5, where the former indicates \"negative\" and 5 indicates \"positive.\"\n",
    "\n",
    "There will be a training set & a test set.\n",
    "\n",
    "We will load the dataset using Python & use respective libraries to implement our text-classification model.\n",
    "\n",
    "Optionally, we will preprocess the data if needed, e.g. case-formating.\n",
    "##### Feature Engineering\n",
    "We will choose a set of classifiers to focus on in our text-classification model, e.g. *n*-grams, num words, cue words, repeated punctuation, etc.\n",
    "\n",
    "##### Text Classification Model\n",
    "To build our text-classification model, we will __follow these steps__:\n",
    "* Any *two* chosen suitable algorithms for text classification.\n",
    "* Vectorization of the text data (conversion of text for numerical features).\n",
    "* Training of the text-classification model using the training dataset, \"sentiment_train.json.\"\n",
    "* Evaluation of our text-classification model using the testing dataset, \"sentiment_test.json.\"\n",
    "\n",
    "##### Results & Analysis\n",
    "A detailed analysis of the model's performance by comparing the results from the output of our two algorithms, where we will __include the following__:\n",
    "* *F1-score* or other relevant metrics.\n",
    "* Confusion matrix.\n",
    "* Any challenges or limitations of the text-classification model/task.\n",
    "* Suggestions for improvement in the performance of the text-classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Implementation: Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome, this is the main program for A1: Sentiment Analysis Text Classification.\n",
      "Written by Dan J. for CS410: Natural Language Processing, Fall 2023.\n",
      "\n",
      "We will use two classification algorithms:\n",
      "1. Gaussian Näive Bayes\n",
      "& 2. Logistic Regression,\n",
      "...to create a text-classifier to guess negative or positive sentimentiality based on various text-reviews of products.\n",
      "Setting up & downloading the NLTK Vader sentiment analysis classifier & lexicon...\n",
      "...Vader has arrived.\n",
      "Loading the training & testing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Dan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded the training & testing datasets!\n",
      "\n",
      "Now applying NLTK Vader sentiment analysis to the training dataset...\n",
      "...Vader has been applied to the training set.\n",
      "Now applying NLTK Vader sentiment analysis to the testing dataset...\n",
      "...Vader has been applied to the testing set.\n",
      "-----\n",
      "\n",
      "Running algorithms on le training & testing datasets (without classifiers)...\n",
      "Running Gaussian Näive Bayes algorithm, version A...\n",
      "..First algorithm is done!\n",
      "Running Logistic Regression algorithm, version A...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tools\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..Second algorithm is done!\n",
      "...All Done!\n",
      "-----\n",
      "\n",
      "Here are le results [Version A ('control'), non-classification]...\n",
      "\n",
      "Algorithm #1, Version A: Gaussian Näive Bayes Performance, Metrics, & Results:\n",
      "...Accuracy was found to be,  59.199999999999996 %,\n",
      "...F1 Score was found to be:  0.3664596273291925 ,\n",
      "...with a Confusion Matrix: \n",
      " [[948  52]\n",
      " [764 236]] ,\n",
      "...& lastly, the classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.95      0.70      1000\n",
      "           1       0.82      0.24      0.37      1000\n",
      "\n",
      "    accuracy                           0.59      2000\n",
      "   macro avg       0.69      0.59      0.53      2000\n",
      "weighted avg       0.69      0.59      0.53      2000\n",
      "\n",
      "-----\n",
      "\n",
      "Algorithm #2, Version A: Logistic Regression Performance, Metrics, & Results:\n",
      "...Accuracy was found to be,  59.199999999999996 %,\n",
      "...F1 Score was found to be:  0.3664596273291925 ,\n",
      "...with a Confusion Matrix: \n",
      " [[948  52]\n",
      " [764 236]] ,\n",
      "...& lastly, the classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.95      0.70      1000\n",
      "           1       0.82      0.24      0.37      1000\n",
      "\n",
      "    accuracy                           0.59      2000\n",
      "   macro avg       0.69      0.59      0.53      2000\n",
      "weighted avg       0.69      0.59      0.53      2000\n",
      "\n",
      "-----\n",
      "\n",
      "-----\n",
      "\n",
      "Running algorithms on le training & testing datasets (with NLTK Vader classifier & truncated training dataset based on 20k rows)...\n",
      "Running Gaussian Näive Bayes algorithm, version B...\n",
      "..First algorithm is done!\n",
      "Running Logistic Regression algorithm, version B...\n",
      "..Second algorithm is done!\n",
      "...All Done!\n",
      "-----\n",
      "\n",
      "Here are le results [Version B, NLTK Vader sentiment analysis classification]...\n",
      "\n",
      "Algorithm #1, Version B: Gaussian Näive Bayes Performance, Metrics, & Results:\n",
      "...Accuracy was found to be,  62.5 %,\n",
      "...F1 Score was found to be:  0.4627507163323782 ,\n",
      "...with a Confusion Matrix: \n",
      " [[927  73]\n",
      " [677 323]] ,\n",
      "...& lastly, the classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.93      0.71      1000\n",
      "           1       0.82      0.32      0.46      1000\n",
      "\n",
      "    accuracy                           0.62      2000\n",
      "   macro avg       0.70      0.62      0.59      2000\n",
      "weighted avg       0.70      0.62      0.59      2000\n",
      "\n",
      "-----\n",
      "\n",
      "Algorithm #2, Version B: Logistic Regression Performance, Metrics, & Results:\n",
      "...Accuracy was found to be,  92.15 %,\n",
      "...F1 Score was found to be:  0.9217737917289487 ,\n",
      "...with a Confusion Matrix: \n",
      " [[918  82]\n",
      " [ 75 925]] ,\n",
      "...& lastly, the classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92      1000\n",
      "           1       0.92      0.93      0.92      1000\n",
      "\n",
      "    accuracy                           0.92      2000\n",
      "   macro avg       0.92      0.92      0.92      2000\n",
      "weighted avg       0.92      0.92      0.92      2000\n",
      "\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### CS410: Natural Language Processing, Fall 2023 - 10/9/2023\n",
    "##### A1: Sentiment Analysis Text Classification, Dan Jang\n",
    "#### Objective: Exploring Natural Language Processing (NLP), by building a text-classifier\n",
    "#### for a text classification task, predicting whether a piece of text is \"positive\" or \"negative.\"\n",
    "\n",
    "### 0.) Libraries\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import json\n",
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plot\n",
    "\n",
    "# ### 1.) Main Program Wrapper, a1_text_classifer\n",
    "#class a1_text_classifer(object):\n",
    "\n",
    "### 1.2.a) Gaussian Näive Bayes algorithm using sklearn.naive_bayes.GaussianNB\n",
    "### https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\n",
    "### Returns four (4) thingys:\n",
    "# I.) accuracy_score,\n",
    "# II.) f1_score,\n",
    "# III.) confusion_matrix,\n",
    "# & IV.) classification_report.\n",
    "def algo_one(xtrain, ytrain, xtest, ytest):\n",
    "    gbayes = GaussianNB()\n",
    "    \n",
    "    gbayes.fit(xtrain, ytrain)\n",
    "    predictionresults = gbayes.predict(xtest)\n",
    "    \n",
    "    return accuracy_score(ytest, predictionresults), f1_score(ytest, predictionresults), confusion_matrix(ytest, predictionresults), classification_report(ytest, predictionresults)\n",
    "    \n",
    "### 1.2.b) Logistic Regression algorithm using sklearn.linear_model.LogisticRegression\n",
    "### https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "### Returns four (4) thingys:\n",
    "# I.) accuracy_score,\n",
    "# II.) f1_score,\n",
    "# III.) confusion_matrix,\n",
    "# & IV.) classification_report.\n",
    "def algo_two(xtrain, ytrain, xtest, ytest):\n",
    "    lreg = LogisticRegression()\n",
    "    \n",
    "    lreg.fit(xtrain, ytrain)\n",
    "    predictionresults = lreg.predict(xtest)\n",
    "    \n",
    "    return accuracy_score(ytest, predictionresults), f1_score(ytest, predictionresults), confusion_matrix(ytest, predictionresults), classification_report(ytest, predictionresults)\n",
    "\n",
    "### 1.3.) NLTK Vader Lexicon-based Sentiment Analysis Classifier\n",
    "### https://www.nltk.org/_modules/nltk/sentiment/vader.html\n",
    "## sith_holocron = le sentiment intensity analyzer from NLTK\n",
    "## theforce = le sentiment score\n",
    "## aura = le text that is to be analyzed by [darth]vader from NLTK\n",
    "def darth(aura):\n",
    "    sith_holocron = SentimentIntensityAnalyzer()\n",
    "    theforce = sith_holocron.polarity_scores(aura)\n",
    "    # Debug Statement #2\n",
    "    #print(theforce['compound'])\n",
    "    return theforce['compound']\n",
    "\n",
    "def main(): #trainfile, testfile):\n",
    "    print(\"Welcome, this is the main program for A1: Sentiment Analysis Text Classification.\")\n",
    "    print(\"Written by Dan J. for CS410: Natural Language Processing, Fall 2023.\")\n",
    "    print(\"\\nWe will use two classification algorithms:\\n1. Gaussian Näive Bayes\\n& 2. Logistic Regression,\\n...to create a text-classifier to guess negative or positive sentimentiality based on various text-reviews of products.\")\n",
    "    \n",
    "    ## https://www.nltk.org/_modules/nltk/sentiment/vader.html\n",
    "    print(\"Setting up & downloading the NLTK Vader sentiment analysis classifier & lexicon...\")\n",
    "    nltk.download('vader_lexicon')\n",
    "    print(\"...Vader has arrived.\")\n",
    "    \n",
    "    ## For converting accuracy to percent\n",
    "    percentness = float(100)\n",
    "    \n",
    "    ## 1.0.) Constants, Variables, & Datasets\n",
    "    \n",
    "    # trainfile = str(trainfile)\n",
    "    # testfile = str(testfile)\n",
    "    traindata = []\n",
    "    testdata = []\n",
    "    \n",
    "    # 1.0.I.A) Debug Statements #1a for dataset loading times:\n",
    "    print(\"Loading the training & testing datasets...\")\n",
    "    # with open(trainfile, \"r\") as trainfile:\n",
    "    with open(\"sentiment_train.json\", \"r\") as trainfile:\n",
    "        #traindata = json.load(trainfile)\n",
    "        for row in trainfile:\n",
    "            traindata.append(json.loads(row))\n",
    "        \n",
    "    trainframe = pandas.DataFrame(traindata)\n",
    "        \n",
    "    # with open(testfile, \"r\") as testfile:\n",
    "    with open(\"sentiment_test.json\", \"r\") as testfile:\n",
    "        #testdata = json.load(testfile)\n",
    "        for row in testfile:\n",
    "            testdata.append(json.loads(row))\n",
    "        \n",
    "    testframe = pandas.DataFrame(testdata)\n",
    "\n",
    "    # 1.0.I.B) Debug Statements #1b for dataset loading times:\n",
    "    print(\"Successfully loaded the training & testing datasets!\\n\")\n",
    "    \n",
    "    ## 1.0.1.) Initial Preprocessing of the training & testing data\n",
    "    ## First, we isolate our two (2) columns, \"review_title\" & \"stars.\"\n",
    "    ## Second, we will convert values in the \"stars\" column so that 1 [negative] = 0 & 5 [positive] = 1.\n",
    "    ## This will allow us to make the negative or positive sentiment a binary value-based thingy.\n",
    "    trainframe = trainframe[['review_title', 'stars']]\n",
    "    trainframe['stars'] = trainframe['stars'].apply(lambda x: 1 if x == 5 else 0)\n",
    "    \n",
    "    testframe = testframe[['review_title', 'stars']]\n",
    "    testframe['stars'] = testframe['stars'].apply(lambda x: 1 if x == 5 else 0)\n",
    "    \n",
    "    ## 1.0.1.) Applying NLTK Vader Sentiment\n",
    "    ## https://www.nltk.org/_modules/nltk/sentiment/vader.html\n",
    "    x2train = trainframe\n",
    "    x2train = x2train[['review_title', 'stars']]\n",
    "    # Have to truncate the training dataset so that it does not crash my computer, heh.\n",
    "    # Using a random_state seed of 2005, which was when Star Wars III was released (when Vader was technically introduced in the prequelz).\n",
    "    x2train = x2train.sample(n=20000, random_state=2005)\n",
    "    print(\"Now applying NLTK Vader sentiment analysis to the training dataset...\")\n",
    "    x2train['nltk_vader_sentiment'] = x2train['review_title'].apply(darth)\n",
    "    print(\"...Vader has been applied to the training set.\")\n",
    "    y2train = x2train['stars']\n",
    "    \n",
    "    x2test = testframe\n",
    "    x2test = x2test[['review_title', 'stars']]\n",
    "    print(\"Now applying NLTK Vader sentiment analysis to the testing dataset...\")\n",
    "    x2test['nltk_vader_sentiment'] = x2test['review_title'].apply(darth)\n",
    "    print(\"...Vader has been applied to the testing set.\")\n",
    "    \n",
    "    ## 1.1.) Vectorization of the text-reviews in the datasets using sklearn.feature_extraction.text.CountVectorizer.\n",
    "    ## As a core component of text-classification, the vectorization process of the text-review data is essential for feature engineering in natural language processing.\n",
    "    ## https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "    vectorization_machine_9000 = CountVectorizer()\n",
    "    xtrain = vectorization_machine_9000.fit_transform(trainframe['review_title'])\n",
    "    xtrain = xtrain.toarray()\n",
    "    ytrain = trainframe['stars']\n",
    "    \n",
    "    xtest = vectorization_machine_9000.transform(testframe['review_title'])\n",
    "    xtest = xtest.toarray()\n",
    "    ytest = testframe['stars']\n",
    "    \n",
    "    ## 1.1.1.) Applying NLTK Vader Sentiment to vectorized data\n",
    "    x2traintext = vectorization_machine_9000.fit_transform(x2train['review_title'])\n",
    "    x2trainsentiment = x2train['nltk_vader_sentiment'].values.reshape(-1,1)\n",
    "    parsed_x2traintext = pandas.DataFrame(x2traintext.toarray())\n",
    "    parsed_x2trainsentiment = pandas.DataFrame(x2trainsentiment)\n",
    "    \n",
    "    x2testtext = vectorization_machine_9000.transform(x2test['review_title'])\n",
    "    x2testsentiment = x2test['nltk_vader_sentiment'].values.reshape(-1,1)\n",
    "    parsed_x2testtext = pandas.DataFrame(x2testtext.toarray())\n",
    "    parsed_x2testsentiment = pandas.DataFrame(x2testsentiment)\n",
    "    \n",
    "    x2train = pandas.concat([parsed_x2traintext, parsed_x2trainsentiment], axis=1)\n",
    "    x2test = pandas.concat([parsed_x2testtext, parsed_x2testsentiment], axis=1)\n",
    "    \n",
    "    ### 1.0.2a) Run Algorithms & Print the Model Results - without classifiers (with vectorization)\n",
    "    print(\"-----\\n\")\n",
    "    print(\"Running algorithms on le training & testing datasets (without classifiers)...\")\n",
    "    print(\"Running Gaussian Näive Bayes algorithm, version A...\")\n",
    "    algo1accuracy, algo1f1, algo1cmatrix, algo1creport = algo_one(xtrain, ytrain, xtest, ytest)\n",
    "    print(\"..First algorithm is done!\")\n",
    "    \n",
    "    print(\"Running Logistic Regression algorithm, version A...\")\n",
    "    algo2accuracy, algo2f1, algo2cmatrix, algo2creport = algo_two(xtrain, ytrain, xtest, ytest)\n",
    "    print(\"..Second algorithm is done!\")\n",
    "    \n",
    "    print(\"...All Done!\")\n",
    "    print(\"-----\\n\")\n",
    "    \n",
    "    print(\"Here are le results [Version A ('control'), non-classification]...\\n\")\n",
    "    print(\"Algorithm #1, Version A: Gaussian Näive Bayes Performance, Metrics, & Results:\")\n",
    "    print(\"...Accuracy was found to be, \", algo1accuracy * percentness, \"%,\")\n",
    "    print(\"...F1 Score was found to be: \", algo1f1, \",\")\n",
    "    print(\"...with a Confusion Matrix: \\n\", algo1cmatrix, \",\")\n",
    "    print(\"...& lastly, the classification Report: \\n\", algo1creport)\n",
    "    print(\"-----\\n\")\n",
    "    \n",
    "    print(\"Algorithm #2, Version A: Logistic Regression Performance, Metrics, & Results:\")\n",
    "    print(\"...Accuracy was found to be, \", algo2accuracy * percentness, \"%,\")\n",
    "    print(\"...F1 Score was found to be: \", algo2f1, \",\")\n",
    "    print(\"...with a Confusion Matrix: \\n\", algo2cmatrix, \",\")\n",
    "    print(\"...& lastly, the classification Report: \\n\", algo2creport)\n",
    "    print(\"-----\\n\")\n",
    "    \n",
    "    ### 1.0.2b) Run Text-Classification Algorithms & Print the Model Results - with NLTK Vader sentiment analysis (& vectorization)\n",
    "    print(\"-----\\n\")\n",
    "    print(\"Running algorithms on le training & testing datasets (with NLTK Vader classifier & truncated training dataset based on 20k rows)...\")\n",
    "    \n",
    "    print(\"Running Gaussian Näive Bayes algorithm, version B...\")\n",
    "    algo1accuracy, algo1f1, algo1cmatrix, algo1creport = algo_one(x2train, y2train, x2test, ytest)\n",
    "    print(\"..First algorithm is done!\")\n",
    "    \n",
    "    print(\"Running Logistic Regression algorithm, version B...\")\n",
    "    algo2accuracy, algo2f1, algo2cmatrix, algo2creport = algo_two(x2train, y2train, x2test, ytest)\n",
    "    print(\"..Second algorithm is done!\")\n",
    "    \n",
    "    print(\"...All Done!\")\n",
    "    print(\"-----\\n\")\n",
    "    \n",
    "    print(\"Here are le results [Version B, NLTK Vader sentiment analysis classification]...\\n\")\n",
    "    print(\"Algorithm #1, Version B: Gaussian Näive Bayes Performance, Metrics, & Results:\")\n",
    "    print(\"...Accuracy was found to be, \", algo1accuracy * percentness, \"%,\")\n",
    "    print(\"...F1 Score was found to be: \", algo1f1, \",\")\n",
    "    print(\"...with a Confusion Matrix: \\n\", algo1cmatrix, \",\")\n",
    "    print(\"...& lastly, the classification Report: \\n\", algo1creport)\n",
    "    print(\"-----\\n\")\n",
    "    \n",
    "    print(\"Algorithm #2, Version B: Logistic Regression Performance, Metrics, & Results:\")\n",
    "    print(\"...Accuracy was found to be, \", algo2accuracy * percentness, \"%,\")\n",
    "    print(\"...F1 Score was found to be: \", algo2f1, \",\")\n",
    "    print(\"...with a Confusion Matrix: \\n\", algo2cmatrix, \",\")\n",
    "    print(\"...& lastly, the classification Report: \\n\", algo2creport)\n",
    "    print(\"-----\\n\")\n",
    "\n",
    "#a1_program = a1_text_classifer(\"sentiment_train.json\", \"sentiment_test.json\")\n",
    "\n",
    "#### Commented out codez\n",
    "# def main():\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text-Classification Model Performance Analysis & Discussion\n",
    "\n",
    "#### Initial Data Results, Metrics, & Analysis\n",
    "\n",
    "#### Comparative Analysis & Discussion\n",
    "It would appear that in Version A, where no classifiers were applied (ergo, serving as our 'control'), the Gaussian Näive Bayes algorithm was accurate only ~56% of the time, which is only ~6% better than the supposed 50-50 chance of guessing between the two, *binary* possibilities, of that being either a \"negative\" or \"positive\" review - where the former is represented by a 0 & latter represented by a 1.\n",
    "\n",
    "Contrarywise, the Logistic Regression algorithm of Version A was accurate ~96% of the time, which showcases a highly significant increase in both relative (in comparsion to Gaussian Näive Bayes's ~56% accuracy) & absolute accuracy.\n",
    "\n",
    "In Version B, where [NLTK Vader](https://www.nltk.org/_modules/nltk/sentiment/vader.html) Sentiment Analysis was applied as a classifer to both the training & testing datasets, we see that \n",
    "\n",
    "#### Text-Classification Challenges & Limitations\n",
    "It would appear that implementing both the Gaussian Näive Bayes & the Logistic Regression algorithms were straightforward as we were able to use [sklearn.naive_bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) & [sklearn.linear_model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) to use these algorithms in our text-classification model & program.\n",
    "\n",
    "One small challenge was the time that NLTK Vader took to process the sentiments, particularly, for the classification of the training dataset, as it has 80k rows, ergo, 80k review_title values to process, which took a good amount of time to process.\n",
    "\n",
    "However, after NLTK Vader was applied & then the algorithms were beginning to process the data, Version B training dataset had to be truncated in the number of rows from 80k to 20k, as my computer was running out & did run out of allocatable RAM (~30 GB+ at the attempt), which shows a possible computational resource-based limitation to using NLTK Vader for sentiment analysis.\n",
    "\n",
    "This made it slightly difficult to run the algorithm, ergo, the program in entirety to figure out bugs. However, this was fixable by initially debugging (then fixing to the smaller number of rows for Version B) with a smaller section of the data, e.g. 2000 rows to match the smaller size of the sentiment_test.json file.\n",
    "\n",
    "\n",
    "#### Discussion for Future Performance & Efficacy Improvements\n",
    "The preprocessing, I do admit, may have been lacking. Besides the memory efficiency by truncating the .json files by only using the two (2) columns, \"review_title\" & \"stars\", I think there could have been better preprocessing to improve performance, e.g. removing punctuation, delimitters, or etc. This was a pertinent thought for improvement whilst awaiting the completion of the NLTK Vader sentiment analysis applying to the training & testing data sets.\n",
    "\n",
    "It also appears that Gaussian Näive Bayes, ran without classifiers or other changes (besides vectorization), performed ~40% weaker than its counterpart, the Logistic Regression algorithm. This might indicate a possible, inherent weakness of using Gaussian Näive Bayes for this specific facet of text-classification, thus, the exploration/usage of alternative, more efficient classification-algorithms may also be a possible method to improve both predictive accuracy & performance.\n",
    "\n",
    "One whimsical thought of mine I had, was to possibly implement a custom classifer that places a high likelihood of review-negativity on the presence of common curse-words in the \"review_title\" & high likelihood of review-positivity on the presence of common 'good-qualifying' words, e.g. 'great', 'awesome', 'amazing', etc. However, this may take some time to tweak correctly & may be suspectible to cultural differences, grammatical quirks, suspectible to lexicon-shifts over time, needing to type out curse-words in a submitted assignment (which would be a little awkward, hehe), & other foreseeable hurdles if it were to be implemented - but would be both interesting & lead to possible performance & accuracy improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References & Resources\n",
    "\n",
    "#### Libraries & Dependencies\n",
    "    matplotlib.pyplot\n",
    "    numpy\n",
    "    pandas\n",
    "[sklearn.naive_bayes.GaussianNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)\n",
    "\n",
    "[sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "\n",
    "[sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "\n",
    "[sklearn.feature_extraction.text.CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "\n",
    "[sklearn.metrics.f1_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)\n",
    "\n",
    "[sklearn.metrics.accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n",
    "\n",
    "[sklearn.metrics.confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n",
    "\n",
    "[sklearn.metrics.classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)\n",
    "\n",
    "[nltk.sentiment.vader.SentimentIntensityAnalyzer](https://www.nltk.org/_modules/nltk/sentiment/vader.html)\n",
    "\n",
    "[nbconvert](https://nbconvert.readthedocs.io/en/latest/)\n",
    "\n",
    "#### References & Credits\n",
    "\n",
    "[*NLP Tutorial for Text Classification in Python* by Vijaya Rani](https://medium.com/analytics-vidhya/nlp-tutorial-for-text-classification-in-python-8f19cd17b49e)\n",
    "\n",
    "[*Using CountVectorizer to Extracting Features from Text* by *GeeksforGeeks*](https://www.geeksforgeeks.org/using-countvectorizer-to-extracting-features-from-text/#)\n",
    "\n",
    "#### Special Thanks\n",
    "\n",
    "[Fixing *sklearn ImportError: No module named _check_build*](https://stackoverflow.com/questions/23062524/sklearn-importerror-no-module-named-check-build)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
