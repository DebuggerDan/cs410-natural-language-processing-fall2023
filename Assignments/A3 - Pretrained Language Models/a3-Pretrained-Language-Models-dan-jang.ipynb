{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS410: Natural Language Processing, Fall 2023\n",
    "## A3: Pretrained Language Models, Dan Jang - 11/12/2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description of Assignment\n",
    "\n",
    "##### Introduction\n",
    "Using the same training & testing datasets from our first & second assignments, in this assignment, *A3: Pretrained Language Models*, we will be exploring & comparing the performance of two, specific ***Pretrained Language Models*** (**PLMs**):\n",
    "\n",
    "1. [***```BERT```***](https://arxiv.org/abs/1810.04805) (*Bidirectional Encoder Representations from Transformers*)\n",
    "\n",
    "&\n",
    "\n",
    "2. ***OpenAI***'s [***```GPT-2```***](https://paperswithcode.com/paper/language-models-are-unsupervised-multitask) (*Generative Pretrained Transformer 2*).\n",
    "\n",
    "Like the previous two assignment, this assignment focuses on implementing a text-classification model that predicts sentiment & the same training/testing datasets, where in A3 specifically, the  (compared A2, where we used **pretrained embeddings** from the ```Word2Vec``` and ```GloVe``` **models**)\n",
    "\n",
    "##### Data Preparation\n",
    "Like our previous two assignments, we will use a pair of training & testing datasets containing product customer reviews, which is named the \"Multilingual Amazon Reviews Corpus\", in a ```.json``` container format, with several columns. The assignment will focus on a smaller subset of the original dataset, where we will focus on __two (2) columns__:\n",
    "1. \"review_title\" - self-explanatory\n",
    "2. \"stars\" - an integer, either 1 or 5, where the former indicates \"negative\" and 5 indicates \"positive.\"\n",
    "\n",
    "There will be a training set & a test set.\n",
    "\n",
    "We will load the dataset using Python & use respective libraries to implement our text-classification model.\n",
    "\n",
    "In contrary to the last two assignments, there will be no preprocessing done manually, except in using each *PLM*'s specific tokenizers to prepare our data for each run of our text-classification model implementations.\n",
    "\n",
    "##### Pretrained Language Models (PLMs)\n",
    "We will use [*HuggingFace*](https://huggingface.co/docs/hub/models-libraries) libraries (e.g. [```transformers```](https://huggingface.co/docs/transformers/index)) to access, then correspondingly experiment with the [***```BERT```***](https://arxiv.org/abs/1810.04805) and [***```GPT-2```***](https://paperswithcode.com/paper/language-models-are-unsupervised-multitask) __pretrained language models__ (***PLMs***), focusing on these aspects:\n",
    "1. __Comparison__ of the two pretrained language models.\n",
    "2. __Model Evaluation, Results, & Analysis__ (and comparison) of our two ***PLMs***.\n",
    "\n",
    "\n",
    "##### Text Classification Model\n",
    "To build our text-classification model, we will __follow these steps__:\n",
    "1. Choosing between [***```BERT```***](https://arxiv.org/abs/1810.04805) and [***```GPT-2```***](https://paperswithcode.com/paper/language-models-are-unsupervised-multitask) ***pretrained language models*** (**PLMs**)\n",
    "2. Using the specific tokenizer used for each respective ***PLM*** to prepare the training & testing text data.\n",
    "3. Training of the text-classification model using the training dataset, \"sentiment_train.json.\"\n",
    "4. Evaluation of our text-classification model using the testing dataset, \"sentiment_test.json.\"\n",
    "\n",
    "##### Results & Analysis\n",
    "A detailed analysis of the model's performance by comparing the results from the output of our two algorithms, where we will __include the following__:\n",
    "1. *F1-score* or other relevant metrics.\n",
    "2. Any challenges or limitations of the text-classification model/task.\n",
    "\n",
    "***Additionally***, we will also try to provide a comparative analysis based on the results from our two previous assignments, our first assignment, *A1: Sentiment Analysis Text Classification* & from our second assignment, *A2: ```Word2Vec``` and ```GloVe``` Embeddings*.\n",
    "\n",
    "Specifically, we recall from our previous two assignments, that we implemented text-classification models based on two suitable algorithms, and in the latter, implemented the usage of ```Word2Vec``` & ```GloVe``` pretrained embeddings to assist in our text-classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries & Constants Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading / Loading the BERT Pretrained Language Model (PLM) through HuggingFace libraries...\n",
      "Loading the BERT Specific Tokenizer...\n",
      "...the BERT (Base, pretrained, uncased tokenization) Pretrained Language Model (PLM) has been downloaded / loaded!\n",
      "Downloading / Loading the GPT-2 Pretrained Language Model (PLM) through HuggingFace libraries...\n",
      "Loading the GPT-2 Specific Tokenizer...\n",
      "...the GPT-2 Pretrained Language Model (PLM) has been downloaded / loaded!\n",
      "...done!\n"
     ]
    }
   ],
   "source": [
    "##### CS410: Natural Language Processing, Fall 2023 - 11/13/2023\n",
    "##### A3: Pretrained Language Models (PLMs), Dan Jang - Initializations: Libraries, Models, Data, & Constants\n",
    "\n",
    "### 0.) Libraries\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import json\n",
    "import pandas\n",
    "#import huggingface_hub\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "## 1.0.) Constants, Variables, & Datasets\n",
    "\n",
    "percentness = float(100)\n",
    "# trainfile = str(trainfile)\n",
    "# testfile = str(testfile)\n",
    "traindata = []\n",
    "testdata = []\n",
    "\n",
    "# Loading the pretrained language models (PLMs), BERT & GPT-2 using HuggingFace libraries!\n",
    "## Bertie -> BERT PLM\n",
    "## Bumblebee -> GPT(ransformer)-2 PLM\n",
    "print(\"Downloading / Loading the BERT Pretrained Language Model (PLM) through HuggingFace libraries...\")\n",
    "bertie = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "print(\"Loading the BERT Specific Tokenizer...\")\n",
    "bertie_tokens = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(\"...the BERT (Base, pretrained, uncased tokenization) Pretrained Language Model (PLM) has been downloaded / loaded!\")\n",
    "\n",
    "print(\"Downloading / Loading the GPT-2 Pretrained Language Model (PLM) through HuggingFace libraries...\")\n",
    "bumblebee = AutoModel.from_pretrained(\"gpt2\")\n",
    "print(\"Loading the GPT-2 Specific Tokenizer...\")\n",
    "bumblebee_tokens = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print(\"...the GPT-2 Pretrained Language Model (PLM) has been downloaded / loaded!\")\n",
    "print(\"...done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Implementation: *Text Classification, with data-processed using respective tokenizers from & with Two (2) Pretrained Language Models*, [***```BERT```***](https://arxiv.org/abs/1810.04805) and [***```GPT-2```***](https://paperswithcode.com/paper/language-models-are-unsupervised-multitask)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome, this is the main program for A2: Word2Vec and GloVe Embeddings.\n",
      "Written by Dan J. for CS410: Natural Language Processing, Fall 2023.\n",
      "\n",
      "We will use one (1) classification algorithm:\n",
      "& 1. Logistic Regression + two (2) pretrained models for embeddings, Word2Vec & GloVe.\n",
      "...to create a text-classifier to guess negative or positive sentimentiality based on various text-reviews of products.\n",
      "\n",
      "Loading the training & testing datasets...\n",
      "Successfully loaded the training & testing datasets!\n",
      "\n",
      "Starting preprocessing of the training dataset...\n",
      "...Preprocessing of the training dataset is complete!\n",
      "Starting preprocessing of the testing dataset...\n",
      "...Preprocessing of the testing dataset is complete!\n",
      "Processing the training & testing datasets for Word2Vec...\n",
      "Word2Vec has been applied to the training & testing datasets!\n",
      "Processing the training & testing datasets for GloVe...\n",
      "GloVe has been applied to the training & testing datasets!\n",
      "-----\n",
      "\n",
      "Running algorithm on le training & testing datasets (with pretrained model embeddings)...\n",
      "Running Logistic Regression algorithm, version A (Word2Vec)...\n",
      "..First embeddings are done!\n",
      "Running Logistic Regression algorithm, version B (GloVe)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tools\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..First embeddings are done!\n",
      "...All Done!\n",
      "-----\n",
      "\n",
      "Here are le results [Logistic Regression /w two (2) pretrained model-based embedding classification]...\n",
      "\n",
      "Logistic Regression Algorithm, Version A: Word2Vec Pretrained Model-based Embeddings Performance, Metrics, & Results:\n",
      "...Accuracy was found to be,  86.3 %,\n",
      "...F1 Score was found to be:  0.86105476673428 ,\n",
      "...with a Confusion Matrix: \n",
      " [[1754  246]\n",
      " [ 302 1698]] ,\n",
      "...& lastly, the classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.88      0.86      2000\n",
      "           1       0.87      0.85      0.86      2000\n",
      "\n",
      "    accuracy                           0.86      4000\n",
      "   macro avg       0.86      0.86      0.86      4000\n",
      "weighted avg       0.86      0.86      0.86      4000\n",
      "\n",
      "-----\n",
      "\n",
      "Logistic Regression Algorithm, Version B: GloVe Pretrained Model-based Embeddings Performance, Metrics, & Results:\n",
      "...Accuracy was found to be,  69.69999999999999 %,\n",
      "...F1 Score was found to be:  0.7313829787234042 ,\n",
      "...with a Confusion Matrix: \n",
      " [[1138  862]\n",
      " [ 350 1650]] ,\n",
      "...& lastly, the classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.57      0.65      2000\n",
      "           1       0.66      0.82      0.73      2000\n",
      "\n",
      "    accuracy                           0.70      4000\n",
      "   macro avg       0.71      0.70      0.69      4000\n",
      "weighted avg       0.71      0.70      0.69      4000\n",
      "\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### CS410: Natural Language Processing, Fall 2023 - 11/13/2023\n",
    "##### A3: Pretrained Language Models (PLMs), Dan Jang - Main Implementation\n",
    "#### Objective: Exploring Natural Language Processing (NLP), by building a text-classifier\n",
    "#### for a text classification task, predicting whether a piece of text is \"positive\" or \"negative.\"\n",
    "#### ...focusing on two (2) pretrained language models (PLMs),\n",
    "#### ...BERT (Bidirectional Encoder Representations from Transformers) & OpenAI's GPT-2 (Generative Pretrained Transformer),\n",
    "#### ...and using the respective toenizers to each PLM to perform the text-classification task as aforementioned\n",
    "\n",
    "### 1.1.a) Logistic Regression algorithm using sklearn.linear_model.LogisticRegression\n",
    "### https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "### Returns four (4) thingys:\n",
    "# I.) accuracy_score,\n",
    "# II.) f1_score,\n",
    "# III.) confusion_matrix,\n",
    "# & IV.) classification_report.\n",
    "def plm(model, xtrain, ytrain, xtest, ytest):\n",
    "    \n",
    "    lreg = LogisticRegression()\n",
    "    \n",
    "    lreg.fit(xtrain, ytrain)\n",
    "    predictionresults = lreg.predict(xtest)\n",
    "    \n",
    "    return accuracy_score(ytest, predictionresults), f1_score(ytest, predictionresults), confusion_matrix(ytest, predictionresults), classification_report(ytest, predictionresults)\n",
    "\n",
    "### A3-Specific: Implementation functions for implementing the two pretrained language models, BERT & GPT-2.\n",
    "## Pretrained Language Model (PLM) Tokenizer Implementation Function\n",
    "def plmodel(words, model, tokenizer):\n",
    "    \n",
    "    wordlist = tokenizer(words, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        results = model(**wordlist)\n",
    "    \n",
    "    return results.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    # wordlist = words.split()\n",
    "    # vecs = [model[w] for w in wordlist if w in model]\n",
    "    # if vecs:\n",
    "    #     return sum(vecs) / len(vecs)\n",
    "    # else:\n",
    "    #     return [0] * model.vector_size\n",
    "    \n",
    "# ## GPT(ransformer)-2 Model Implementation Function\n",
    "# def bumblebee(words, model):\n",
    "#     beds = [avgwordvec(w, model) for w in words]\n",
    "#     return beds\n",
    "\n",
    "def main(): #trainfile, testfile):\n",
    "    print(\"Welcome, this is the main program for A3: Pretrained Language Models.\")\n",
    "    print(\"Written by Dan J. for CS410: Natural Language Processing, Fall 2023.\")\n",
    "    print(\"\\nWe will use two (2) pretrained language models (PLM), BERT & GPT-2.\\n...to create a text-classifier to guess negative or positive sentimentiality based on various text-reviews of products.\")\n",
    "\n",
    "    # 1.0.I.A) Debug Statements #1a for dataset loading times:\n",
    "    print(\"\\nLoading the training & testing datasets...\")\n",
    "    # with open(trainfile, \"r\") as trainfile:\n",
    "    with open(\"sentiment_train.json\", \"r\") as trainfile:\n",
    "        #traindata = json.load(trainfile)\n",
    "        for row in trainfile:\n",
    "            traindata.append(json.loads(row))\n",
    "        \n",
    "    trainframe = pandas.DataFrame(traindata)\n",
    "        \n",
    "    # with open(testfile, \"r\") as testfile:\n",
    "    with open(\"sentiment_test.json\", \"r\") as testfile:\n",
    "        #testdata = json.load(testfile)\n",
    "        for row in testfile:\n",
    "            testdata.append(json.loads(row))\n",
    "        \n",
    "    testframe = pandas.DataFrame(testdata)\n",
    "\n",
    "    # 1.0.I.B) Debug Statements #1b for dataset loading times:\n",
    "    print(\"Successfully loaded the training & testing datasets!\\n\")\n",
    "    \n",
    "    ## 1.0.1.) Initial Preprocessing of the training & testing data\n",
    "    ## First, we isolate our two (2) columns, \"review_title\" & \"stars.\"\n",
    "    ## Second, we will convert values in the \"stars\" column so that 1 [negative] = 0 & 5 [positive] = 1.\n",
    "    ## This will allow us to make the negative or positive sentiment a binary value-based thingy.\n",
    "    trainframe = trainframe[['review_title', 'stars']]\n",
    "    trainframe['stars'] = trainframe['stars'].apply(lambda x: 1 if x == 5 else 0)\n",
    "    \n",
    "    testframe = testframe[['review_title', 'stars']]\n",
    "    testframe['stars'] = testframe['stars'].apply(lambda x: 1 if x == 5 else 0)\n",
    "    \n",
    "    ## A3-Specific: From our Slack channel (#nlp_f23), using tip to only use 25% of training dataset, evenly split\n",
    "    ## Credits to Classmate Will McIntosh from the Slack thread started by classmate Saurav Kumar Singh\n",
    "    ## & also, full credits to classmate Will McIntosh for the following code for GPU usage:\n",
    "    \n",
    "    #### Will McIntosh (11/11/2023):\n",
    "    # Testing\n",
    "    print(f\"Is a GPU available: {torch.cuda.is_available()}\")\n",
    "    #print(f\"Is this instance using a GPU?: {next(model.parameters()).is_cuda}\")\n",
    "    #### From Slack, #nlp_f23.\n",
    "\n",
    "    trainframe = trainframe.sample(frac=1, random_state=69)\n",
    "    trainframe = trainframe.iloc[:int(0.25 * len(trainframe))]\n",
    "    \n",
    "    ## Evenly split frames\n",
    "    x3train1, x3train2 = train_test_split(trainframe, test_size=0.5, random_state=69)\n",
    "    \n",
    "    ## A3-Specific: Applying BERT & GPT-2 PLM Specific Tokenization\n",
    "    print(\"Tokenizing the training & testing datasets for BERT...\")\n",
    "    x2train1 = x3train1['review_title'].apply(lambda x: plmodel(x, bertie, bertie_tokens))\n",
    "    #x2train1 = trainframe['review_title'].apply(lambda x: plmodel(x, bertie, bertie_tokens))\n",
    "    x2test1 = testframe['review_title'].apply(lambda x: plmodel(x, bertie, bertie_tokens))\n",
    "    print(\"BERT Tokenization has been applied to its training & testing datasets!\")\n",
    "    \n",
    "    print(\"Tokenizing the training & testing datasets for GPT-2...\")\n",
    "    x2train2 = x3train2['review_title'].apply(lambda x: plmodel(x, bumblebee, bumblebee_tokens))\n",
    "    #x2train2 = trainframe['review_title'].apply(lambda x: plmodel(x, bumblebee, bumblebee_tokens))\n",
    "    x2test2 = testframe['review_title'].apply(lambda x: plmodel(x, bumblebee, bumblebee_tokens))\n",
    "    print(\"GPT-2's tokenization has been applied to its training & testing datasets!\")\n",
    "    \n",
    "    y2train = trainframe['stars']\n",
    "    \n",
    "    ytest = testframe['stars']\n",
    "\n",
    "    ### 1.0.2b) Run Text-Classification Algorithms & Print the Model Results\n",
    "    print(\"-----\\n\")\n",
    "    print(\"Running text-classification model le training & testing datasets (with pretrained language models, BERT & GPT-2)...\")\n",
    "\n",
    "    \n",
    "    print(\"Running Logistic Regression algorithm, version A.) BERT...\")\n",
    "    bed1accuracy, bed1f1, bed1cmatrix, bed1creport = plm(x2train1.tolist(), y2train, x2test1.tolist(), ytest)\n",
    "    print(\"..The data processing from our first PLM, BERT, is done!\")\n",
    "    \n",
    "    print(\"Running Logistic Regression algorithm, version B.) GPT-2...\")\n",
    "    bed2accuracy, bed2f1, bed2cmatrix, bed2creport = plm(x2train2.tolist(), y2train, x2test2.tolist(), ytest)\n",
    "    print(\"..The data processing from our second PLM, GPT-2, is done!\")\n",
    "    \n",
    "    print(\"...All Done!\")\n",
    "    print(\"-----\\n\")\n",
    "    \n",
    "    print(\"Here are le results [Logistic Regression, with comparative results between two (2) PLMs, BERT & GPT-2]...\\n\")\n",
    "    print(\"Logistic Regression Algorithm, Version A: BERT Pretrained Language Model-based Text-Classification Performance, Metrics, & Results:\")\n",
    "    print(\"...Accuracy was found to be, \", bed1accuracy * percentness, \"%,\")\n",
    "    print(\"...F1 Score was found to be: \", bed1f1, \",\")\n",
    "    print(\"...with a Confusion Matrix: \\n\", bed1cmatrix, \",\")\n",
    "    print(\"...& lastly, the classification Report: \\n\", bed1creport)\n",
    "    print(\"-----\\n\")\n",
    "    \n",
    "    print(\"Logistic Regression Algorithm, Version B: GPT-2 Pretrained Language Model-based Text-Classification Performance, Metrics, & Results:\")\n",
    "    print(\"...Accuracy was found to be, \", bed2accuracy * percentness, \"%,\")\n",
    "    print(\"...F1 Score was found to be: \", bed2f1, \",\")\n",
    "    print(\"...with a Confusion Matrix: \\n\", bed2cmatrix, \",\")\n",
    "    print(\"...& lastly, the classification Report: \\n\", bed2creport)\n",
    "    print(\"-----\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text-Classification Model Performance Analysis & Discussion\n",
    "\n",
    "#### Initial Data Results, Metrics, & Analysis\n",
    "\n",
    "Using Logistic Regression for both Version A, which represents the ```Word2Vec``` model & Version B, which represents the ```GloVe``` model, we saw the following results:\n",
    "\n",
    "Version A, ```Word2Vec``` Model:\n",
    "\n",
    "    Accuracy:  ~86.3%,\n",
    "    F1 Score:  0.86105476673428\n",
    "\n",
    "    Confusion Matrix: \n",
    "    [[1754  246]\n",
    "    [ 302 1698]]\n",
    "\n",
    "    Classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.85      0.88      0.86      2000\n",
    "            1       0.87      0.85      0.86      2000\n",
    "\n",
    "    accuracy                            0.86      4000\n",
    "    macro avg       0.86      0.86      0.86      4000\n",
    "    weighted avg    0.86      0.86      0.86      4000\n",
    "\n",
    "Version B, ```GloVe``` Model:\n",
    "\n",
    "    Accuracy: ~69.69%,\n",
    "    F1 Score: 0.7313829787234042\n",
    "\n",
    "    Confusion Matrix: \n",
    "    [[1138  862]\n",
    "    [ 350 1650]]\n",
    "\n",
    "    Classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "             0       0.76      0.57      0.65      2000\n",
    "             1       0.66      0.82      0.73      2000\n",
    "\n",
    "    accuracy                             0.70      4000\n",
    "    macro avg        0.71      0.70      0.69      4000\n",
    "    weighted avg     0.71      0.70      0.69      4000\n",
    "\n",
    "#### Comparative Analysis & Discussion\n",
    "From the previous assignment, A1: Sentiment Analysis Text Classification, we saw the following results for the Logistic Regression algorithm, for Version A, which only had token vectorization, and Version B, which had the NLTK Vader sentiment analysis classifer applied to it:\n",
    "\n",
    "*From A1 Results:* Logistic Regression Results (Version A) - No Classifiers Applied:\n",
    "\n",
    "    Accuracy: ~92.7%\n",
    "    F1 Score: ~0.92729\n",
    "\n",
    "    Confusion Matrix: \n",
    "        [[923  77]\n",
    "        [ 69 931]]\n",
    "    \n",
    "    Classification Report:\n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.93      0.92      0.93      1000\n",
    "            1       0.92      0.93      0.93      1000\n",
    "\n",
    "    accuracy                            0.93      2000\n",
    "    macro avg       0.93      0.93      0.93      2000\n",
    "    weighted avg    0.93      0.93      0.93      2000\n",
    "\n",
    "*From A1 Results:* Logistic Regression Results (Version B) - NLTK Vader Sentiment Analysis Classifer Applied:\n",
    "\n",
    "    Accuracy:  ~92.8%\n",
    "    F1 Score:  0.9281437125748503\n",
    "\n",
    "    Confusion Matrix: \n",
    "    [[926  74]\n",
    "    [ 70 930]]\n",
    "\n",
    "    Classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.93      0.93      0.93      1000\n",
    "            1       0.93      0.93      0.93      1000\n",
    "\n",
    "    accuracy                            0.93      2000\n",
    "    macro avg       0.93      0.93      0.93      2000\n",
    "    weighted avg    0.93      0.93      0.93      2000\n",
    "\n",
    "It appears that the Logistic Regression faired well each time from **A1 Results**, where we interestingly see decreases in performance in **A2 Results**, when we compare the accuracy of ~86.3% for the ```Word2Vec``` model vs. the ~69.69% for the ```GloVe``` pretrained model embeddings.\n",
    "\n",
    "Recalling from the *A1 Results* analysis:\n",
    "\n",
    "    Contrarywise, the Logistic Regression algorithm of Version A was accurate ~96% of the time,\n",
    "    ...which showcases a highly significant increase in both relative (in comparsion to Gaussian Näive Bayes's ~56% accuracy) & absolute accuracy.\n",
    "\n",
    "    In Version B, where [NLTK Vader](https://www.nltk.org/_modules/nltk/sentiment/vader.html),\n",
    "    ...Sentiment Analysis was applied as a classifer to both the training & testing datasets, we see that there was a very small, modest increase in accuracy,\n",
    "    ...in both algorithms (as seen above for Version B results),\n",
    "    ...where we see an increase of ~0.11% in accuracy for the Gaussian Näive Bayes algorithm\n",
    "    ...& an increase of ~0.01% in accuracy for Logistic Regression.\n",
    "\n",
    "However, although having lower accuracies for both models is, in the grand perspective, not great - it is at least *relatively* good, for the sake of model analysis, that we are able to see more than a ~0.01% in deviation for accuracy between the two (2) pretrained models, such that we can see the possible difference in effectiveness in these two models, for the specific training & testing dataset we have been utilizing.\n",
    "\n",
    "#### Text-Classification Challenges & Limitations\n",
    "The initial challenges were due to the process of implementing the visualization using t-SNE, where one of its dependencies, threadpoolctl, [needed to be upgraded to 3.1.0](https://stackoverflow.com/questions/73283082/t-sne-sklearn-attributeerror-nonetype-object-has-no-attribute-split) in order for the t-SNE ```fit_transform``` to properly recognize & properly analyze the pretrained models.\n",
    "\n",
    "This particular error took the majority of the whole time spent on the assignment, where a back-and-forth re-programming downward spiral occurred, before I had swallowed my pride & resorted to looking up the specific NoneType attribute error on *StackOverflow*, which, much to the thanks of others who have suffered alike, another programmer [had found the solution](https://stackoverflow.com/a/76711294).\n",
    "\n",
    "Other than this procedural challenge, a limitation specific to the implementation of the pretrained model-based embeddings were due to a possible lack of required preprocessing in order to meet or succeed the rates of success as seen from the *A1 Results*, in terms of accuracy.\n",
    "\n",
    "#### Discussion for Future Performance & Efficacy Improvements\n",
    "With more time, I think that there would have most likely been definite improvements in accuracy if preprocessing was much more closely matched to that of the preprocessing that was applied to the training corpus for each of our respective two (2) pretrained models that we've used for embeddings in question.\n",
    "\n",
    "Specifically, for our two (2) ```Word2Vec``` & ```GloVe``` models, upon further reading about the model details & specifications on *Hugging Face* for [word2vec-google-news-300](https://huggingface.co/fse/word2vec-google-news-300) & [glove-wiki-gigaword-300](https://huggingface.co/fse/glove-wiki-gigaword-300) (also the *Stanford* paper, [*GloVe: Global Vectors for Word Representation*](https://nlp.stanford.edu/pubs/glove.pdf)), a little bit more preprocessing may have made a significant difference in reaching higher levels of accuracy & precision, perhaps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References & Resources\n",
    "\n",
    "#### Libraries & Dependencies\n",
    "    numpy\n",
    "    pandas\n",
    "    torch\n",
    "    random\n",
    "\n",
    "[HuggingFace_hub](https://huggingface.co/docs/hub/models-libraries)\n",
    "\n",
    "[*BERT*](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "*OpenAI*'s [*GPT-2*](https://paperswithcode.com/paper/language-models-are-unsupervised-multitask)\n",
    "\n",
    "[*HuggingFace*](https://huggingface.co/docs/hub/models-libraries)'s [*transformers*](https://huggingface.co/docs/transformers/index)\n",
    "\n",
    "```from``` [```transformers```](https://huggingface.co/docs/transformers/index) ```import``` [```AutoModel```](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html)(s)\n",
    "\n",
    "```from``` [```transformers```](https://huggingface.co/docs/transformers/index) ```import``` [```AutoTokenizer```](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#autotokenizer)\n",
    "\n",
    "```from``` [```transformers```](https://huggingface.co/docs/transformers/index) ```import``` [```AutoTokenizer.from_pretrained```](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#transformers.AutoTokenizer.from_pretrained), [```AutoModel.from_pretrained```](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#transformers.AutoModelForPreTraining.from_pretrained)\n",
    "\n",
    "[sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "\n",
    "[sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "\n",
    "[sklearn.feature_extraction.text.CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "\n",
    "[sklearn.metrics.f1_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)\n",
    "\n",
    "[sklearn.metrics.accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n",
    "\n",
    "[sklearn.metrics.confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n",
    "\n",
    "[sklearn.metrics.classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)\n",
    "\n",
    "[nbconvert](https://nbconvert.readthedocs.io/en/latest/)\n",
    "\n",
    "\n",
    "#### References & Credits\n",
    "\n",
    "[*BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by *Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova*](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "[*Language Models are Unsupervised Multitask Learners* by *Alec Radford, Jeffrey Wu, Rewon Child David Luan, Dario Amodei, & Ilya Sutskever* (*OpenAI*)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "\n",
    "Credits to GitHub Copilot & ChatGPT for code implementation assistance.\n",
    "\n",
    "#### Special Thanks\n",
    "\n",
    "[Fixing *sklearn ImportError: No module named _check_build*](https://stackoverflow.com/questions/23062524/sklearn-importerror-no-module-named-check-build)\n",
    "\n",
    "[Fixing super random 'NoneType' has no attribute 'split' error by upgrading t-SNE dependency threadpoolctl to 3.1.0 or above](https://stackoverflow.com/questions/73283082/t-sne-sklearn-attributeerror-nonetype-object-has-no-attribute-split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extra Stuff\n",
    "\n",
    "### **A2 Results**: Raw Output from ```Word2Vec``` & GloVe Embedding Results, using the Logistic Regression Algorithm\n",
    "\n",
    "    Logistic Regression Algorithm, Version A: Word2Vec Pretrained Model-based Embeddings Performance, Metrics, & Results:\n",
    "    ...Accuracy was found to be,  86.3 %,\n",
    "    ...F1 Score was found to be:  0.86105476673428 ,\n",
    "    ...with a Confusion Matrix: \n",
    "    [[1754  246]\n",
    "    [ 302 1698]] ,\n",
    "    ...& lastly, the classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.85      0.88      0.86      2000\n",
    "            1       0.87      0.85      0.86      2000\n",
    "\n",
    "        accuracy                           0.86      4000\n",
    "    macro avg       0.86      0.86      0.86      4000\n",
    "    weighted avg       0.86      0.86      0.86      4000\n",
    "\n",
    "    -----\n",
    "\n",
    "    Logistic Regression Algorithm, Version B: GloVe Pretrained Model-based Embeddings Performance, Metrics, & Results:\n",
    "    ...Accuracy was found to be,  69.69999999999999 %,\n",
    "    ...F1 Score was found to be:  0.7313829787234042 ,\n",
    "    ...with a Confusion Matrix: \n",
    "    [[1138  862]\n",
    "    [ 350 1650]] ,\n",
    "    ...& lastly, the classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.76      0.57      0.65      2000\n",
    "            1       0.66      0.82      0.73      2000\n",
    "\n",
    "        accuracy                           0.70      4000\n",
    "    macro avg       0.71      0.70      0.69      4000\n",
    "    weighted avg       0.71      0.70      0.69      4000\n",
    "\n",
    "    -----\n",
    "\n",
    "#### ***From A1 Results for Reference:*** Initial Full 80k-Row Processing Results Raw Output\n",
    "\n",
    "    Algorithm #1, Version A: Gaussian Näive Bayes Performance, Metrics, & Results:\n",
    "    ...Accuracy was found to be,  59.199999999999996 %,\n",
    "    ...F1 Score was found to be:  0.3664596273291925 ,\n",
    "    ...with a Confusion Matrix: \n",
    "    [[948  52]\n",
    "    [764 236]] ,\n",
    "    ...& lastly, the classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.55      0.95      0.70      1000\n",
    "            1       0.82      0.24      0.37      1000\n",
    "\n",
    "        accuracy                           0.59      2000\n",
    "    macro avg       0.69      0.59      0.53      2000\n",
    "    weighted avg       0.69      0.59      0.53      2000\n",
    "\n",
    "    -----\n",
    "\n",
    "    Algorithm #2, Version A: Logistic Regression Performance, Metrics, & Results:\n",
    "    ...Accuracy was found to be,  92.7 %,\n",
    "    ...F1 Score was found to be:  0.9272908366533865 ,\n",
    "    ...with a Confusion Matrix: \n",
    "    [[923  77]\n",
    "    [ 69 931]] ,\n",
    "    ...& lastly, the classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.93      0.92      0.93      1000\n",
    "            1       0.92      0.93      0.93      1000\n",
    "\n",
    "        accuracy                           0.93      2000\n",
    "    macro avg       0.93      0.93      0.93      2000\n",
    "    weighted avg       0.93      0.93      0.93      2000\n",
    "\n",
    "    -----\n",
    "\n",
    "    Algorithm #1, Version B: Gaussian Näive Bayes Performance, Metrics, & Results:\n",
    "    ...Accuracy was found to be,  59.3 %,\n",
    "    ...F1 Score was found to be:  0.36899224806201547 ,\n",
    "    ...with a Confusion Matrix: \n",
    "    [[948  52]\n",
    "    [762 238]] ,\n",
    "    ...& lastly, the classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.55      0.95      0.70      1000\n",
    "            1       0.82      0.24      0.37      1000\n",
    "\n",
    "        accuracy                           0.59      2000\n",
    "    macro avg       0.69      0.59      0.53      2000\n",
    "    weighted avg       0.69      0.59      0.53      2000\n",
    "\n",
    "    -----\n",
    "\n",
    "    Algorithm #2, Version B: Logistic Regression Performance, Metrics, & Results:\n",
    "    ...Accuracy was found to be,  92.80000000000001 %,\n",
    "    ...F1 Score was found to be:  0.9281437125748503 ,\n",
    "    ...with a Confusion Matrix: \n",
    "    [[926  74]\n",
    "    [ 70 930]] ,\n",
    "    ...& lastly, the classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.93      0.93      0.93      1000\n",
    "            1       0.93      0.93      0.93      1000\n",
    "\n",
    "        accuracy                           0.93      2000\n",
    "    macro avg       0.93      0.93      0.93      2000\n",
    "    weighted avg       0.93      0.93      0.93      2000\n",
    "\n",
    "    -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook a2-Word2Vec-and-gloVe-embeddings-dan-jang.ipynb to pdf\n",
      "[NbConvertApp] Support files will be in a2-Word2Vec-and-gloVe-embeddings-dan-jang_files\\\n",
      "[NbConvertApp] Making directory .\\a2-Word2Vec-and-gloVe-embeddings-dan-jang_files\n",
      "[NbConvertApp] Making directory .\\a2-Word2Vec-and-gloVe-embeddings-dan-jang_files\n",
      "[NbConvertApp] Making directory .\\a2-Word2Vec-and-gloVe-embeddings-dan-jang_files\n",
      "[NbConvertApp] Making directory .\\a2-Word2Vec-and-gloVe-embeddings-dan-jang_files\n",
      "[NbConvertApp] Writing 85396 bytes to notebook.tex\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] Running xelatex 3 times: ['xelatex', 'notebook.tex', '-quiet']\n",
      "[NbConvertApp] Running bibtex 1 time: ['bibtex', 'notebook']\n",
      "[NbConvertApp] WARNING | b had problems, most likely because there were no citations\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 234209 bytes to a2-Word2Vec-and-gloVe-embeddings-dan-jang.pdf\n"
     ]
    }
   ],
   "source": [
    "##### Juypter Notebook -> PDF Conversion thingy\n",
    "\n",
    "#!pip install nbconvert\n",
    "!jupyter nbconvert --to pdf a2-Word2Vec-and-gloVe-embeddings-dan-jang.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
