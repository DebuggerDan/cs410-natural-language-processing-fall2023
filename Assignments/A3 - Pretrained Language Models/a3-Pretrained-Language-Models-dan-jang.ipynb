{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS410: Natural Language Processing, Fall 2023\n",
    "## A3: Pretrained Language Models, Dan Jang - 11/12/2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description of Assignment\n",
    "\n",
    "##### Introduction\n",
    "Using the same training & testing datasets from our first & second assignments, in this assignment, *A3: Pretrained Language Models*, we will be exploring & comparing the performance of two, specific ***Pretrained Language Models*** (**PLMs**):\n",
    "\n",
    "1. [*Google AI*](https://blog.research.google/2018/11/open-sourcing-bert-state-of-art-pre.html)'s [***```BERT```***](https://arxiv.org/abs/1810.04805) (*Bidirectional Encoder Representations from Transformers*)\n",
    "\n",
    "&\n",
    "\n",
    "2. [*OpenAI*](https://openai.com/research/better-language-models)'s [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) (*Generative Pretrained Transformer 2*).\n",
    "\n",
    "Like the previous two assignment, this assignment focuses on implementing a text-classification model that predicts sentiment & the same training/testing datasets, where in *A3* specifically, we (where, in comparison to our text-classification model approach in our previous *A2* assignment, we used **pretrained embeddings** from the ```Word2Vec``` and ```GloVe``` **models** instead).\n",
    "\n",
    "##### Data Preparation\n",
    "Like our previous two assignments, we will use a pair of training & testing datasets containing product customer reviews, which is named the \"[*Multilingual Amazon Reviews Corpus*](https://arxiv.org/abs/2010.02573)\", in a ```.json``` container format, with several columns. The assignment will focus on a smaller subset of the original dataset, where we will focus on __two (2) columns__:\n",
    "1. \"review_title\" - self-explanatory\n",
    "2. \"stars\" - an integer, either 1 or 5, where the former indicates \"negative\" and 5 indicates \"positive.\"\n",
    "\n",
    "There will be a training set & a test set.\n",
    "\n",
    "We will load the dataset using Python & use respective libraries to implement our text-classification model.\n",
    "\n",
    "In contrary to the last two assignments, there will be no preprocessing done manually, except in using each *PLM*'s specific tokenizers to prepare our data for each run of our text-classification model implementations.\n",
    "\n",
    "##### Implementation of Pretrained Language Models (PLMs)\n",
    "We will use [*HuggingFace*](https://huggingface.co/docs/hub/models-libraries) libraries (e.g. [```transformers```](https://huggingface.co/docs/transformers/index)) to access, then correspondingly experiment with the [***```BERT```***](https://arxiv.org/abs/1810.04805) and [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) __pretrained language models__ (***PLMs***), focusing on these aspects:\n",
    "\n",
    "1. __Comparison__ of the two pretrained language models.\n",
    "\n",
    "2. __Model Evaluation, Results, & Analysis__ (and comparison) of our two ***PLMs***.\n",
    "\n",
    "##### **BERT** (Bidirectional Encoder Representations from Transformers) Pretrained Language Model (PLM)\n",
    "The first ***PLM*** we will be using is [***```BERT```***](https://arxiv.org/abs/1810.04805) (*Bidirectional Encoder Representations from Transformers*), which was first described in a [paper](https://arxiv.org/pdf/1810.04805.pdf) published in May 24th, 2019, by the [*Google AI Language* Team](https://blog.research.google/2018/11/open-sourcing-bert-state-of-art-pre.html), authored by Researchers Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n",
    "\n",
    "[***```BERT```***](https://arxiv.org/abs/1810.04805) (*Bidirectional Encoder Representations from Transformers*) was created-in-mind, to be a model designed particularly for training \"*their own state-of-the-art question answering system*\" - built upon previous natural language processing research in pretraining contextual representations, e.g. [*Semi-Supervised Sequence Learning*](https://arxiv.org/abs/1511.01432), [*Generative Pre-Training*](https://blog.openai.com/language-unsupervised/), etc. ([*Google AI Language*, Devlin & Chang](https://blog.research.google/2018/11/open-sourcing-bert-state-of-art-pre.html), November 2 2018).\n",
    "\n",
    "Moreover, [***```BERT```***](https://arxiv.org/abs/1810.04805) (*Bidirectional Encoder Representations from Transformers*), at the time, this model was the \"*first* ***__deeply directional__***, **unsupervised** *language representation, pretrained using only a plain text corpus*\", using *Wikipedia* as its training source ([2018](https://blog.research.google/2018/11/open-sourcing-bert-state-of-art-pre.html)).\n",
    "\n",
    "![*BERT*'s Neural Network Architecture & Comparison Diagram by *Google AI Team*, November 2nd, 2018](BERTDiagram1-GoogleAITeam-November2nd2018.png)\n",
    "\n",
    "\n",
    "##### OpenAI's **GPT-2** (Generative Pretrained Transformer) Pretrained Language Model (PLM)\n",
    "An almost infamously named ***PLM*** model, [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) (*Generative Pretrained Transformer 2*) is the second predecessor of the ***```ChatGPT```*** model that is known popularly today (as of November 10th, 2023, ***```ChatGPT```*** utilizes specifically for its ***PLM*** models, ***```GPT-3.5 Turbo```*** for free & ***```GPT-4+```*** [/w [*```Vision```*](https://cdn.openai.com/papers/GPTV_System_Card.pdf)] for premium users), our second ***PLM*** ***OpenAI***'s [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) (*Generative Pretrained Transformer 2*).\n",
    "\n",
    "On February 14th, 2019, *OpenAI* describes first in a [blog post](https://openai.com/research/better-language-models) & [technical paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), their announcement & description of their new \"*large-scale unsupervised language model*\", named ***```GPT-2```*** ([*OpenAI*](https://openai.com/research/better-language-models), February 14 2019).\n",
    "\n",
    "In ***```GPT-2```***'s [technical paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), *OpenAI* researchers Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever describes the ***```GPT-2```*** model - a ***__unidirectional__*** model trained on a dataset of *8 million web pages* with **1.5 billion parameters** ([*OpenAI*](https://openai.com/research/better-language-models), [Radford et al.](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), February 14 2019).\n",
    "\n",
    "##### Comparison of the BERT vs. GPT-2 Model Architectures & Designs\n",
    "\n",
    "While both [***```BERT```***](https://arxiv.org/abs/1810.04805) and [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) share great similarities in their overall architecture & design, one of the biggest differences between these two ***PLMs*** lies in the *deeply bidirectional* nature of [***```BERT```***](https://arxiv.org/abs/1810.04805) & the *unidirectional* nature of [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). Specifically, BERT had been traditionally used preferably over GPT-2 in certain natural language processing (NLP) tasks, as it seemed to be designed with a more efficient model design & architecture ([Kehlbeck, et al.](https://bert-vs-gpt2.dbvis.de/), July 31 2021).\n",
    "\n",
    "Furthermore, the nature of embedding spaces in [***```BERT```***](https://arxiv.org/abs/1810.04805) and [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) differ greatly, as shown below of the first layers of each respective ***PLM***:\n",
    "\n",
    "![First Layer of *BERT* & *GPT-2* Embedding Spaces [and of other models] (Kehlbeck, et al., July 31 2021)](BERTAndGPT21stEmbeddingsLayer-Kehlbecketal-July212021.png)\n",
    "\n",
    "Most importantly, with regards to the strengths of our two ***PLMs***, [***```BERT```***](https://arxiv.org/abs/1810.04805) may be better for tasks surrounding contextual understanding of sentences, while [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) may be more geared towards tasks related to generating text & general language tasks ([Fhal](https://medium.com/codepubcast/comparing-bert-gpt-2-and-gpt-3-a-look-at-the-pros-and-cons-of-popular-pre-trained-language-1e32c6f0af9b), January 17 2023).\n",
    "\n",
    "##### Text Classification Model\n",
    "To build our text-classification model, we will __follow these steps__:\n",
    "\n",
    "1. Implementing & setting up our two (2) **Pretrained Language Models** (***PLMs***), [***```BERT```***](https://arxiv.org/abs/1810.04805) and [***```GPT-2```***](C:\\).\n",
    "\n",
    "2. Using the specific tokenizer used for each respective ***PLM*** to prepare the training & testing text data.\n",
    "\n",
    "3. Training of the text-classification model using the specifically-tokenized training dataset for each ***PLM***, based on \"sentiment_train.json.\"\n",
    "\n",
    "4. Evaluation of our text-classification model using the specifically-tokenized testing dataset for each ***PLM***, based on \"sentiment_test.json.\"\n",
    "\n",
    "##### Results & Analysis\n",
    "A detailed analysis of the model's performance by comparing the results from the output of our two algorithms, where we will __include the following__:\n",
    "\n",
    "1. *F1-score* or other relevant metrics.\n",
    "\n",
    "2. Any challenges or limitations of the text-classification model/task.\n",
    "\n",
    "***Additionally***, we will also try to provide a comparative analysis based on the results from our two previous assignments, our first assignment, *A1: Sentiment Analysis Text Classification* & from our second assignment, *A2: ```Word2Vec``` and ```GloVe``` Embeddings*.\n",
    "\n",
    "Specifically, we recall from our previous two assignments, that we first implemented text-classification models based on two suitable algorithms (*A1*), and in our second, implemented the usage of ```Word2Vec``` & ```GloVe``` pretrained embeddings to assist in our text-classification tasks (*A1*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries & Constants Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading / Loading the BERT Pretrained Language Model (PLM) through HuggingFace libraries...\n",
      "Loading the BERT Specific Tokenizer...\n",
      "...the BERT (Base, pretrained, uncased tokenization) Pretrained Language Model (PLM) has been downloaded / loaded!\n",
      "\n",
      "Downloading / Loading the GPT-2 Pretrained Language Model (PLM) through HuggingFace libraries...\n",
      "Loading the GPT-2 Specific Tokenizer...\n",
      "...the GPT-2 Pretrained Language Model (PLM) has been downloaded / loaded!\n",
      "\n",
      "Checking for GPT-2 tokenizer padding token...\n",
      "GPT-2 tokenizer has no padding token!\n",
      "...setting GPT-2 tokenizer padding token...\n",
      "...GPT-2 tokenizer padding token set!\n",
      "\n",
      "\n",
      "Model initialization all done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### CS410: Natural Language Processing, Fall 2023 - 11/13/2023\n",
    "##### A3: Pretrained Language Models (PLMs), Dan Jang - Initializations: Libraries, Models, Data, & Constants\n",
    "\n",
    "### 0.) Libraries\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import json\n",
    "import pandas\n",
    "#import huggingface_hub\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "## 1.0.) Constants, Variables, & Datasets\n",
    "\n",
    "percentness = float(100)\n",
    "# trainfile = str(trainfile)\n",
    "# testfile = str(testfile)\n",
    "traindata = []\n",
    "testdata = []\n",
    "\n",
    "# Loading the pretrained language models (PLMs), BERT & GPT-2 using HuggingFace libraries!\n",
    "## Bertie -> BERT PLM\n",
    "## Bumblebee -> GPT(ransformer)-2 PLM\n",
    "print(\"Downloading / Loading the BERT Pretrained Language Model (PLM) through HuggingFace libraries...\")\n",
    "bertie = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "print(\"Loading the BERT Specific Tokenizer...\")\n",
    "bertie_tokens = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(\"...the BERT (Base, pretrained, uncased tokenization) Pretrained Language Model (PLM) has been downloaded / loaded!\\n\")\n",
    "\n",
    "print(\"Downloading / Loading the GPT-2 Pretrained Language Model (PLM) through HuggingFace libraries...\")\n",
    "bumblebee = AutoModel.from_pretrained(\"gpt2\")\n",
    "print(\"Loading the GPT-2 Specific Tokenizer...\")\n",
    "bumblebee_tokens = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print(\"...the GPT-2 Pretrained Language Model (PLM) has been downloaded / loaded!\\n\")\n",
    "print(\"Checking for GPT-2 tokenizer padding token...\")\n",
    "if bumblebee_tokens.pad_token is None:\n",
    "    print(\"GPT-2 tokenizer has no padding token!\")\n",
    "    print(\"...setting GPT-2 tokenizer padding token...\")\n",
    "    bumblebee_tokens.pad_token = bumblebee_tokens.eos_token\n",
    "    print(\"...GPT-2 tokenizer padding token set!\")\n",
    "print(\"\\n\\nModel initialization all done!\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Implementation: *Text Classification, with data-processed using respective tokenizers from & with Two (2) Pretrained Language Models*, [***```BERT```***](https://arxiv.org/abs/1810.04805) and [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome, this is the main program for A3: Pretrained Language Models.\n",
      "Written by Dan J. for CS410: Natural Language Processing, Fall 2023.\n",
      "\n",
      "We will use two (2) pretrained language models (PLM), BERT & GPT-2.\n",
      "...to create a text-classifier to guess negative or positive sentimentiality based on various text-reviews of products.\n",
      "\n",
      "Loading the training & testing datasets...\n",
      "Successfully loaded the training & testing datasets!\n",
      "\n",
      "Is a GPU available: False\n",
      "Tokenizing the training & testing datasets for BERT...\n",
      "BERT Tokenization has been applied to its training & testing datasets!\n",
      "Tokenizing the training & testing datasets for GPT-2...\n",
      "GPT-2's tokenization has been applied to its training & testing datasets!\n",
      "-----\n",
      "\n",
      "Running text-classification model le training & testing datasets (with pretrained language models, BERT & GPT-2)...\n",
      "Running Logistic Regression algorithm, version A.) BERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tools\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..The data processing from our first PLM, BERT, is done!\n",
      "Running Logistic Regression algorithm, version B.) GPT-2...\n",
      "..The data processing from our second PLM, GPT-2, is done!\n",
      "...All Done!\n",
      "-----\n",
      "\n",
      "Here are le results [Logistic Regression, with comparative results between two (2) PLMs, BERT & GPT-2]...\n",
      "\n",
      "Logistic Regression Algorithm, Version A: BERT Pretrained Language Model-based Text-Classification Performance, Metrics, & Results:\n",
      "...Accuracy was found to be,  91.55 %,\n",
      "...F1 Score was found to be:  0.9158785465405676 ,\n",
      "...with a Confusion Matrix: \n",
      " [[911  89]\n",
      " [ 80 920]] ,\n",
      "...& lastly, the classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.92      1000\n",
      "           1       0.91      0.92      0.92      1000\n",
      "\n",
      "    accuracy                           0.92      2000\n",
      "   macro avg       0.92      0.92      0.92      2000\n",
      "weighted avg       0.92      0.92      0.92      2000\n",
      "\n",
      "-----\n",
      "\n",
      "Logistic Regression Algorithm, Version B: GPT-2 Pretrained Language Model-based Text-Classification Performance, Metrics, & Results:\n",
      "...Accuracy was found to be,  87.4 %,\n",
      "...F1 Score was found to be:  0.872598584428716 ,\n",
      "...with a Confusion Matrix: \n",
      " [[885 115]\n",
      " [137 863]] ,\n",
      "...& lastly, the classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.89      0.88      1000\n",
      "           1       0.88      0.86      0.87      1000\n",
      "\n",
      "    accuracy                           0.87      2000\n",
      "   macro avg       0.87      0.87      0.87      2000\n",
      "weighted avg       0.87      0.87      0.87      2000\n",
      "\n",
      "-----\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tools\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "##### CS410: Natural Language Processing, Fall 2023 - 11/13/2023\n",
    "##### A3: Pretrained Language Models (PLMs), Dan Jang - Main Implementation\n",
    "#### Objective: Exploring Natural Language Processing (NLP), by building a text-classifier\n",
    "#### for a text classification task, predicting whether a piece of text is \"positive\" or \"negative.\"\n",
    "#### ...focusing on two (2) pretrained language models (PLMs),\n",
    "#### ...BERT (Bidirectional Encoder Representations from Transformers) & OpenAI's GPT-2 (Generative Pretrained Transformer),\n",
    "#### ...and using the respective toenizers to each PLM to perform the text-classification task as aforementioned\n",
    "\n",
    "### 1.1.a) Logistic Regression algorithm using sklearn.linear_model.LogisticRegression\n",
    "### https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "### Returns four (4) thingys:\n",
    "# I.) accuracy_score,\n",
    "# II.) f1_score,\n",
    "# III.) confusion_matrix,\n",
    "# & IV.) classification_report.\n",
    "def plm(model, xtrain, ytrain, xtest, ytest):\n",
    "    \n",
    "    lreg = LogisticRegression()\n",
    "    \n",
    "    lreg.fit(xtrain, ytrain)\n",
    "    predictionresults = lreg.predict(xtest)\n",
    "    \n",
    "    return accuracy_score(ytest, predictionresults), f1_score(ytest, predictionresults), confusion_matrix(ytest, predictionresults), classification_report(ytest, predictionresults)\n",
    "\n",
    "### A3-Specific: Implementation functions for implementing the two pretrained language models, BERT & GPT-2.\n",
    "## Pretrained Language Model (PLM) Tokenizer Implementation Function\n",
    "def plmodel(words, model, tokenizer):\n",
    "    if tokenizer.pad_token is None:\n",
    "        raise ValueError(\"[Debug A3.1 - PLModel()]: Tokenizer has no padding token for the current model!\")\n",
    "    wordlist = tokenizer(words, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        results = model(**wordlist)\n",
    "    \n",
    "    return results.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    # wordlist = words.split()\n",
    "    # vecs = [model[w] for w in wordlist if w in model]\n",
    "    # if vecs:\n",
    "    #     return sum(vecs) / len(vecs)\n",
    "    # else:\n",
    "    #     return [0] * model.vector_size\n",
    "    \n",
    "# ## GPT(ransformer)-2 Model Implementation Function\n",
    "# def bumblebee(words, model):\n",
    "#     beds = [avgwordvec(w, model) for w in words]\n",
    "#     return beds\n",
    "\n",
    "def main(): #trainfile, testfile):\n",
    "    print(\"Welcome, this is the main program for A3: Pretrained Language Models.\")\n",
    "    print(\"Written by Dan J. for CS410: Natural Language Processing, Fall 2023.\")\n",
    "    print(\"\\nWe will use two (2) pretrained language models (PLM), BERT & GPT-2.\\n...to create a text-classifier to guess negative or positive sentimentiality based on various text-reviews of products.\")\n",
    "\n",
    "    # 1.0.I.A) Debug Statements #1a for dataset loading times:\n",
    "    print(\"\\nLoading the training & testing datasets...\")\n",
    "    # with open(trainfile, \"r\") as trainfile:\n",
    "    with open(\"sentiment_train.json\", \"r\") as trainfile:\n",
    "        #traindata = json.load(trainfile)\n",
    "        for row in trainfile:\n",
    "            traindata.append(json.loads(row))\n",
    "        \n",
    "    trainframe = pandas.DataFrame(traindata)\n",
    "        \n",
    "    # with open(testfile, \"r\") as testfile:\n",
    "    with open(\"sentiment_test.json\", \"r\") as testfile:\n",
    "        #testdata = json.load(testfile)\n",
    "        for row in testfile:\n",
    "            testdata.append(json.loads(row))\n",
    "        \n",
    "    testframe = pandas.DataFrame(testdata)\n",
    "\n",
    "    # 1.0.I.B) Debug Statements #1b for dataset loading times:\n",
    "    print(\"Successfully loaded the training & testing datasets!\\n\")\n",
    "    \n",
    "    ## 1.0.1.) Initial Preprocessing of the training & testing data\n",
    "    ## First, we isolate our two (2) columns, \"review_title\" & \"stars.\"\n",
    "    ## Second, we will convert values in the \"stars\" column so that 1 [negative] = 0 & 5 [positive] = 1.\n",
    "    ## This will allow us to make the negative or positive sentiment a binary value-based thingy.\n",
    "    trainframe = trainframe[['review_title', 'stars']]\n",
    "    trainframe['stars'] = trainframe['stars'].apply(lambda x: 1 if x == 5 else 0)\n",
    "    \n",
    "    testframe = testframe[['review_title', 'stars']]\n",
    "    testframe['stars'] = testframe['stars'].apply(lambda x: 1 if x == 5 else 0)\n",
    "    \n",
    "    ## A3-Specific: From our Slack channel (#nlp_f23), using tip to only use 25% of training dataset, evenly split\n",
    "    ## Credits to Classmate Will McIntosh from the Slack thread started by classmate Saurav Kumar Singh\n",
    "    ## & also, full credits to classmate Will McIntosh for the following code for GPU usage:\n",
    "    \n",
    "    #### Credits to Will McIntosh (11/11/2023):\n",
    "    # Testing\n",
    "    print(f\"Is a GPU available: {torch.cuda.is_available()}\")\n",
    "    #print(f\"Is this instance using a GPU?: {next(model.parameters()).is_cuda}\")\n",
    "    #### From Slack, #nlp_f23.\n",
    "\n",
    "    trainframe = trainframe.sample(frac=1, random_state=69)\n",
    "    trainframe = trainframe.iloc[:int(0.25 * len(trainframe))]\n",
    "    \n",
    "    # y2train = trainframe['stars']\n",
    "    # print(\"[A3 Debug Size-Print #3] y2train\", len(y2train))\n",
    "    \n",
    "    ytest = testframe['stars']\n",
    "    # print(\"[A3 Debug Size-Print #4] ytest\", len(ytest))\n",
    "    \n",
    "    ## Evenly split frames\n",
    "    x3train1, x3train2 = train_test_split(trainframe, test_size=0.5, random_state=69)\n",
    "    \n",
    "    y3train1 = x3train1['stars']\n",
    "    y3train2 = x3train2['stars']\n",
    "    \n",
    "    #print(\"[A3 Debug Size-Print #1] x3train1 & x3train2\", len(x3train1), len(x3train2))\n",
    "    ## A3-Specific: Applying BERT & GPT-2 PLM Specific Tokenization\n",
    "    print(\"Tokenizing the training & testing datasets for BERT...\")\n",
    "    x2train1 = x3train1['review_title'].apply(lambda x: plmodel(x, bertie, bertie_tokens))\n",
    "    #x2train1 = trainframe['review_title'].apply(lambda x: plmodel(x, bertie, bertie_tokens))\n",
    "    x2test1 = testframe['review_title'].apply(lambda x: plmodel(x, bertie, bertie_tokens))\n",
    "    print(\"BERT Tokenization has been applied to its training & testing datasets!\")\n",
    "    \n",
    "    #print(\"[A3 Debug Size-Print #2a] x2train1 & x2test1\", len(x2train1), len(x2test1))\n",
    "    \n",
    "    print(\"Tokenizing the training & testing datasets for GPT-2...\")\n",
    "    x2train2 = x3train2['review_title'].apply(lambda x: plmodel(x, bumblebee, bumblebee_tokens))\n",
    "    #x2train2 = trainframe['review_title'].apply(lambda x: plmodel(x, bumblebee, bumblebee_tokens))\n",
    "    x2test2 = testframe['review_title'].apply(lambda x: plmodel(x, bumblebee, bumblebee_tokens))\n",
    "    print(\"GPT-2's tokenization has been applied to its training & testing datasets!\")\n",
    "    \n",
    "    #print(\"[A3 Debug Size-Print #2b] x2train2 & x2test2\", len(x2train2), len(x2test2))\n",
    "\n",
    "    ### 1.0.2b) Run Text-Classification Algorithms & Print the Model Results\n",
    "    print(\"-----\\n\")\n",
    "    print(\"Running text-classification model le training & testing datasets (with pretrained language models, BERT & GPT-2)...\")\n",
    "\n",
    "    \n",
    "    print(\"Running Logistic Regression algorithm, version A.) BERT...\")\n",
    "    bed1accuracy, bed1f1, bed1cmatrix, bed1creport = plm(bertie, x2train1.tolist(), y3train1, x2test1.tolist(), ytest)\n",
    "    print(\"..The data processing from our first PLM, BERT, is done!\")\n",
    "    \n",
    "    print(\"Running Logistic Regression algorithm, version B.) GPT-2...\")\n",
    "    bed2accuracy, bed2f1, bed2cmatrix, bed2creport = plm(bumblebee, x2train2.tolist(), y3train2, x2test2.tolist(), ytest)\n",
    "    print(\"..The data processing from our second PLM, GPT-2, is done!\")\n",
    "    \n",
    "    print(\"...All Done!\")\n",
    "    print(\"-----\\n\")\n",
    "    \n",
    "    print(\"Here are le results [Logistic Regression, with comparative results between two (2) PLMs, BERT & GPT-2]...\\n\")\n",
    "    print(\"Logistic Regression Algorithm, Version A: BERT Pretrained Language Model-based Text-Classification Performance, Metrics, & Results:\")\n",
    "    print(\"...Accuracy was found to be, \", bed1accuracy * percentness, \"%,\")\n",
    "    print(\"...F1 Score was found to be: \", bed1f1, \",\")\n",
    "    print(\"...with a Confusion Matrix: \\n\", bed1cmatrix, \",\")\n",
    "    print(\"...& lastly, the classification Report: \\n\", bed1creport)\n",
    "    print(\"-----\\n\")\n",
    "    \n",
    "    print(\"Logistic Regression Algorithm, Version B: GPT-2 Pretrained Language Model-based Text-Classification Performance, Metrics, & Results:\")\n",
    "    print(\"...Accuracy was found to be, \", bed2accuracy * percentness, \"%,\")\n",
    "    print(\"...F1 Score was found to be: \", bed2f1, \",\")\n",
    "    print(\"...with a Confusion Matrix: \\n\", bed2cmatrix, \",\")\n",
    "    print(\"...& lastly, the classification Report: \\n\", bed2creport)\n",
    "    print(\"-----\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *A3: Pretrained Language Models (PLMs)*, Text-Classification Model Performance Analysis & Discussion\n",
    "\n",
    "#### Initial Data Results, Metrics, & Analysis\n",
    "\n",
    "Using the Logistic Regression Algorithm like previously for *A2*, this algorithm was used to implement our text-classification model, for the text-classification task in both *Version A* & *B*.\n",
    "\n",
    "As shown below, *Version A* featured [*Google AI*](https://blog.research.google/2018/11/open-sourcing-bert-state-of-art-pre.html)'s [***```BERT```***](https://arxiv.org/abs/1810.04805) model & *Version B* featured [*OpenAI*](https://openai.com/research/better-language-models)'s [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) model - where we saw the following results:\n",
    "\n",
    "##### Version A - ***```BERT```*** Pretrained Language Model (PLM) Results:\n",
    "\n",
    "    Accuracy:  ~91.6%\n",
    "    F1 Score:  0.9158785465405676\n",
    "\n",
    "    Confusion Matrix:\n",
    "    [[911  89]\n",
    "    [ 80 920]]\n",
    "\n",
    "    Classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.92      0.91      0.92      1000\n",
    "            1       0.91      0.92      0.92      1000\n",
    "\n",
    "    accuracy                            0.92      2000\n",
    "    macro avg       0.92      0.92      0.92      2000\n",
    "    weighted avg    0.92      0.92      0.92      2000\n",
    "\n",
    "##### Version B - ***```GPT-2```*** Pretrained Language Model (PLM) Results:\n",
    "    Accuracy:  87.4%\n",
    "    F1 Score:  0.872598584428716\n",
    "    \n",
    "    Confusion Matrix: \n",
    "    [[885 115]\n",
    "    [137 863]]\n",
    "\n",
    "    Classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.87      0.89      0.88      1000\n",
    "            1       0.88      0.86      0.87      1000\n",
    "\n",
    "    accuracy                            0.87      2000\n",
    "    macro avg       0.87      0.87      0.87      2000\n",
    "    weighted avg    0.87      0.87      0.87      2000\n",
    "\n",
    "#### Pretrained Language Model (PLM) Comparative Analysis & Discussion\n",
    "\n",
    "From the results above, we see that both [*Google AI*](https://blog.research.google/2018/11/open-sourcing-bert-state-of-art-pre.html)'s [***```BERT```***](https://arxiv.org/abs/1810.04805) & [*OpenAI*](https://openai.com/research/better-language-models)'s [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) ***PLMs*** faired very well in accuracy, of ~91.6% & 87.4% respectively.\n",
    "\n",
    "Between these two ***PLMs***, [***```BERT```***](https://arxiv.org/abs/1810.04805) & [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), we can see that from our performance results that there is/was a slight, low *difference of accuracy*, where [***```BERT```***](https://arxiv.org/abs/1810.04805) has a ***higher*** **accuracy** of *___~4.2%___* in *comparison to [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)*. \n",
    "\n",
    "Also, by looking at the respective Confusion Matrices for our two ***PLMs***, we can see that [***```BERT```***](https://arxiv.org/abs/1810.04805), of course, was able to classify sentiment more accurately than [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), but we also can observe that [***```BERT```***](https://arxiv.org/abs/1810.04805) seems to be *a little bit* more efficient in detecting *negative* sentimentality vs. [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) lean towards *positive* sentimentality - however, [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)'s decreased accuracy in detecting *negative* sentiment seems to be more significant in magnitude compared to the aforementioned, *negative* over *positive* sentimentality bias, of [***```BERT```***](https://arxiv.org/abs/1810.04805).\n",
    "\n",
    "Interestingly, a possible scenario where this supposed sentimentality bias in our text-classification task may present itself as to yield a different result, at least for accuracy & the F1 score, may lie in how the positive & negative sentiment composition during the processing of our training & testing dataset, particularly, at the time of randomized splitting.\n",
    "Specifically, if we had somehow chosen a random seed in our splitting code, that supposedly had much more positive or negative sentimentality in the composition of the reviews, we could have seen variations from our initial performance & results as described above.\n",
    "\n",
    "Ergo, between [*Google AI*](https://blog.research.google/2018/11/open-sourcing-bert-state-of-art-pre.html)'s [***```BERT```***](https://arxiv.org/abs/1810.04805) & [*OpenAI*](https://openai.com/research/better-language-models)'s [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) ***PLMs***, [***```BERT```***](https://arxiv.org/abs/1810.04805) seems to be a clearly more suitable model, at least for the text-classification task & model with our assignment's training & dataset, \"[*Multilingual Amazon Reviews Corpus*](https://arxiv.org/abs/2010.02573).\"\n",
    "\n",
    "\n",
    "#### Analysis of Current & Previous Text-Classification Models & Performance Results from *A1* & *A2*\n",
    "\n",
    "In comparison to the performance seen from *A2: ```Word2Vec``` & ```GloVe``` Embeddings*, both [*Google AI*](https://blog.research.google/2018/11/open-sourcing-bert-state-of-art-pre.html)'s [***```BERT```***](https://arxiv.org/abs/1810.04805) & [*OpenAI*](https://openai.com/research/better-language-models)'s [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) ***PLMs*** outperformed the accuracies & F1 scores from either ```Word2Vec``` or ```GloVe``` pretrained embeddings - where we recall the *A2* results as follows:\n",
    "\n",
    "*A2*: Version A, [```Word2Vec```](https://arxiv.org/abs/1301.3781) Model:\n",
    "\n",
    "    Accuracy:  ~86.3%,\n",
    "    F1 Score:  0.86105476673428\n",
    "\n",
    "    Confusion Matrix: \n",
    "    [[1754  246]\n",
    "    [ 302 1698]]\n",
    "\n",
    "    Classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.85      0.88      0.86      2000\n",
    "            1       0.87      0.85      0.86      2000\n",
    "\n",
    "    accuracy                            0.86      4000\n",
    "    macro avg       0.86      0.86      0.86      4000\n",
    "    weighted avg    0.86      0.86      0.86      4000\n",
    "\n",
    "*A2*: Version B, [```GloVe```](https://nlp.stanford.edu/projects/glove/) Model:\n",
    "\n",
    "    Accuracy: ~69.69%,\n",
    "    F1 Score: 0.7313829787234042\n",
    "\n",
    "    Confusion Matrix: \n",
    "    [[1138  862]\n",
    "    [ 350 1650]]\n",
    "\n",
    "    Classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "             0       0.76      0.57      0.65      2000\n",
    "             1       0.66      0.82      0.73      2000\n",
    "\n",
    "    accuracy                             0.70      4000\n",
    "    macro avg        0.71      0.70      0.69      4000\n",
    "    weighted avg     0.71      0.70      0.69      4000\n",
    "\n",
    "Although, we do see only a *slight* **~1.1% accuracy** improvement when we compare results from [```Word2Vec```](https://arxiv.org/abs/1301.3781) vs. [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf).\n",
    "\n",
    "However, this is a neat & expected observation, as we can recall that both [```Word2Vec```](https://arxiv.org/abs/1301.3781) & [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) are suitable or otherwise geared towards the generation of text, rather than the specificity of text-classification required for efficiency & performance with our Amazon review sentimentality prediction task.\n",
    "As expected, contrarywise, as the results from the implementation of [```GloVe```](https://nlp.stanford.edu/projects/glove/) pretrained embeddings & the [***```BERT```***](https://arxiv.org/abs/1810.04805) ***PLM*** show higher performance & accuracy compared to the [```Word2Vec```](https://arxiv.org/abs/1301.3781) pretrained embeddings & [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) ***PLM***, we could also make the observation of this supposed higher efficiency & efficacy by recalling that the [```GloVe```](https://nlp.stanford.edu/projects/glove/) pretrained embeddings were also designed & trained to create an unsupervised learning algorithm, with training that \"*performed on aggregated global word-word co-occurence statistics*\" ([Pennington, et al.](https://nlp.stanford.edu/projects/glove/), 2014), which, like the nature of [***```BERT```***](https://arxiv.org/abs/1810.04805)'s greater suitability towards text-classification tasks, could explain these higher accuracies & better performance as observed from our previous *A2* & most current, *A3* results.\n",
    "\n",
    "Recalling further, note that from our first assignment, *A1: Sentiment Analysis Text Classification*, we saw the following results from using *two (2) different algorithms* to implement the text-classification model, specifically, the *Logistic Regression* & the *Gaussian Näive Bayes* Algorithms:\n",
    "\n",
    "*From A1 Results:* Version A: *Gaussian Näive Bayes* Algorithm:\n",
    "\n",
    "    Accuracy: ~59.2%\n",
    "    F1 Score: 0.3664596273291925\n",
    "\n",
    "    CConfusion Matrix: \n",
    "    [[948  52]\n",
    "    [764 236]]\n",
    "\n",
    "    Classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.55      0.95      0.70      1000\n",
    "            1       0.82      0.24      0.37      1000\n",
    "\n",
    "    accuracy                            0.59      2000\n",
    "    macro avg       0.69      0.59      0.53      2000\n",
    "    weighted avg    0.69      0.59      0.53      2000\n",
    "\n",
    "*From A1 Results:* Version B: *Logistic Regression* Algorithm:\n",
    "\n",
    "    Accuracy:  92.7%\n",
    "    F1 Score:  0.9272908366533865\n",
    "\n",
    "    Confusion Matrix: \n",
    "    [[923  77]\n",
    "    [ 69 931]]\n",
    "\n",
    "    Classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.93      0.92      0.93      1000\n",
    "            1       0.92      0.93      0.93      1000\n",
    "\n",
    "    accuracy                            0.93      2000\n",
    "    macro avg       0.93      0.93      0.93      2000\n",
    "    weighted avg    0.93      0.93      0.93      2000\n",
    "\n",
    "This is interesting, as while it appears that using ***PLMs*** over ***pretrained embeddings*** yielded much significant improvements in both accuracy & performance, the results from using only the Logistic Regression Algorithm with no classifiers had yielded a *92.7% accuracy*, which still stands as one of the most accurate results from our three (3) assignments.\n",
    "\n",
    "However, this observation & other possible variations that can be observed, in performance & results, may be attributed to both the nature of text-tokenization, usage of classifiers, & particularly, that in the implementation of the first text-classification model of our *A1* assignment, I had trained the text-classification model cases with the ***full, eighty-thousand (80,000) rows of training data***, while in our current *A3* implementation, the training data for both the [***```BERT```***](https://arxiv.org/abs/1810.04805) & [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) ***PLMs*** were truncated randomly (with a shared random seed of ```69```) to a quarter-percent (25%) of the original number of rows, or twenty-thousand (20,000) rows of training data.\n",
    "\n",
    "#### Text-Classification Challenges & Limitations\n",
    "The initial & biggest challenge to implement these **pretrained language models** (***PLMs***) was the very significantly resource & time-intensive requirements for the computation, tokenization, & training of the two (2) ***PLMs*** used.\n",
    "\n",
    "With the current version of code & implementation, in average, it appeared that [***```BERT```***](https://arxiv.org/abs/1810.04805) took around *two-to-eight (2-to-8) minutes*, while [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) took longer, around *five-to-fifteen (5-to-15) minutes* long. Both, on average, usually took around *fifteen-to-thirty (15-to-30) minutes* to fully complete.\n",
    "\n",
    "However, in the earlier stages of coding & implementation, particularly before truncating the training & testing datasets to the currently-set percentage of twenty-five percent (25%) of the original amount of rows/lines of text, the average varied wildly, with completion durations taking *hours, upon hours* to complete or were stopped prior to completion due to time constraints.\n",
    "\n",
    "Furthermore, even post-truncation adjustments, the pretraining of the two ***PLMs***, in terms of computational power & resources, were very intensive, which had caused ***multiple***, **full computer crashes**, leading to a lot of processing delays & mental energy to continue implementing.\n",
    "\n",
    "#### Discussion for Future Performance & Efficacy Improvements\n",
    "With the issue of computational resources & the prevention of computer crashes, would be, of course, to finally sign-up for the student discounted *Google Cloud* subscription for using high-resource, cloud computing in *Google Colaboratory*, where NLP compatible, GPU-acceleration is available to expedite ***PLM***, model training, or otherwise for running intensive NLP code & tasks.\n",
    "As such, this specific idea for future performance & efficacy improvement will be implemented immediately following this current assignment, *A3*, where I will go ahead & attempt to set-up *Google Colaboratory* for use to hopefully, avoid the aforementioned resource & crash pitfalls for *A4* and our *Group Project*, heh.\n",
    "\n",
    "Furthermore, I should attempt to start early & try to implement for assignments in smaller code-blocks/pieces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References & Resources\n",
    "\n",
    "#### Libraries & Dependencies\n",
    "    numpy\n",
    "    pandas\n",
    "    torch\n",
    "    random\n",
    "\n",
    "[HuggingFace_hub](https://huggingface.co/docs/hub/models-libraries)\n",
    "\n",
    "[*Google AI*](https://blog.research.google/2018/11/open-sourcing-bert-state-of-art-pre.html)'s [*BERT*](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "[*OpenAI*](https://openai.com/research/better-language-models)'s [*GPT-2*](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "\n",
    "[*HuggingFace*](https://huggingface.co/docs/hub/models-libraries)'s [*transformers*](https://huggingface.co/docs/transformers/index)\n",
    "\n",
    "```from``` [```transformers```](https://huggingface.co/docs/transformers/index) ```import``` [```AutoModel```](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html)(s)\n",
    "\n",
    "```from``` [```transformers```](https://huggingface.co/docs/transformers/index) ```import``` [```AutoTokenizer```](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#autotokenizer)\n",
    "\n",
    "```from``` [```transformers```](https://huggingface.co/docs/transformers/index) ```import``` [```AutoTokenizer.from_pretrained```](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#transformers.AutoTokenizer.from_pretrained), [```AutoModel.from_pretrained```](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#transformers.AutoModelForPreTraining.from_pretrained)\n",
    "\n",
    "[sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "\n",
    "[sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "\n",
    "[sklearn.metrics.f1_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)\n",
    "\n",
    "[sklearn.metrics.accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n",
    "\n",
    "[sklearn.metrics.confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n",
    "\n",
    "[sklearn.metrics.classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)\n",
    "\n",
    "[nbconvert](https://nbconvert.readthedocs.io/en/latest/)\n",
    "\n",
    "\n",
    "#### References & Credits\n",
    "\n",
    "[*The Multilingual Amazon Reviews Corpus* by Phillip Keung, Yichao Lu, György Szarvas, & Noah A. Smith (October 6th, 2020)](https://arxiv.org/abs/2010.02573)\n",
    "\n",
    "[*BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin, Ming-Wei Chang, Kenton Lee, & Kristina Toutanova* (May 24th, 2019)](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "[*Language Models are Unsupervised Multitask Learners* by Alec Radford, Jeffrey Wu, Rewon Child David Luan, Dario Amodei, & Ilya Sutskever (*OpenAI*; February 14th, 2019)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "\n",
    "[*Comparing BERT, GPT-2, and GPT-3: A Look at the Pros and Cons of Popular Pre-Trained Language Models* by Em Fhal (January 17th, 2023)](https://medium.com/codepubcast/comparing-bert-gpt-2-and-gpt-3-a-look-at-the-pros-and-cons-of-popular-pre-trained-language-1e32c6f0af9b)\n",
    "\n",
    "[*GloVe: Global Vectors for Word Representation* by Jeffrey Pennington, Richard Socher, & Christopher D. Manning (2014)](https://nlp.stanford.edu/pubs/glove.pdf)\n",
    "\n",
    "[*Efficient Estimation of Word Representations in Vector Space* [*Word2Vec* Pre-trained Embeddings] by Tomas Mikolov, Kai Chen, Greg Corrado, & Jeffrey Dean (January 16th, 2013)](https://arxiv.org/abs/1301.3781)\n",
    "\n",
    "Credits to GitHub Copilot & ChatGPT for code implementation assistance.\n",
    "\n",
    "#### Special Thanks\n",
    "\n",
    "Thanks to fellow classmate *Will McIntosh* for their helpful tips & tricks for resource management particularly for ***PLMs*** in the current *A3* assignment, from the ```#nlp_f23``` class-channel on *pdx-cs* Slack, November 11th, 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extra Stuff\n",
    "\n",
    "### **A3 Results**: Raw Output from [*BERT*](https://arxiv.org/abs/1810.04805) & *OpenAI*'s [*GPT-2*](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) Pretrained Language Models (PLMs), using the Logistic Regression Algorithm\n",
    "    Here are le results [Logistic Regression, with comparative results between two (2) PLMs, BERT & GPT-2]...\n",
    "\n",
    "    Logistic Regression Algorithm, Version A: BERT Pretrained Language Model-based Text-Classification Performance, Metrics, & Results:\n",
    "    ...Accuracy was found to be,  91.55 %,\n",
    "    ...F1 Score was found to be:  0.9158785465405676 ,\n",
    "    ...with a Confusion Matrix: \n",
    "    [[911  89]\n",
    "    [ 80 920]] ,\n",
    "    ...& lastly, the classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.92      0.91      0.92      1000\n",
    "            1       0.91      0.92      0.92      1000\n",
    "\n",
    "        accuracy                           0.92      2000\n",
    "    macro avg       0.92      0.92      0.92      2000\n",
    "    weighted avg       0.92      0.92      0.92      2000\n",
    "\n",
    "    -----\n",
    "\n",
    "    Logistic Regression Algorithm, Version B: GPT-2 Pretrained Language Model-based Text-Classification Performance, Metrics, & Results:\n",
    "    ...Accuracy was found to be,  87.4 %,\n",
    "    ...F1 Score was found to be:  0.872598584428716 ,\n",
    "    ...with a Confusion Matrix: \n",
    "    [[885 115]\n",
    "    [137 863]] ,\n",
    "    ...& lastly, the classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.87      0.89      0.88      1000\n",
    "            1       0.88      0.86      0.87      1000\n",
    "\n",
    "        accuracy                           0.87      2000\n",
    "    macro avg       0.87      0.87      0.87      2000\n",
    "    weighted avg       0.87      0.87      0.87      2000\n",
    "\n",
    "-----\n",
    "\n",
    "### **A2 Results**: Raw Output from ```Word2Vec``` & GloVe Embedding Results, using the Logistic Regression Algorithm\n",
    "\n",
    "    Logistic Regression Algorithm, Version A: Word2Vec Pretrained Model-based Embeddings Performance, Metrics, & Results:\n",
    "    ...Accuracy was found to be,  86.3 %,\n",
    "    ...F1 Score was found to be:  0.86105476673428 ,\n",
    "    ...with a Confusion Matrix: \n",
    "    [[1754  246]\n",
    "    [ 302 1698]] ,\n",
    "    ...& lastly, the classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.85      0.88      0.86      2000\n",
    "            1       0.87      0.85      0.86      2000\n",
    "\n",
    "        accuracy                           0.86      4000\n",
    "    macro avg       0.86      0.86      0.86      4000\n",
    "    weighted avg       0.86      0.86      0.86      4000\n",
    "\n",
    "    -----\n",
    "\n",
    "    Logistic Regression Algorithm, Version B: GloVe Pretrained Model-based Embeddings Performance, Metrics, & Results:\n",
    "    ...Accuracy was found to be,  69.69999999999999 %,\n",
    "    ...F1 Score was found to be:  0.7313829787234042 ,\n",
    "    ...with a Confusion Matrix: \n",
    "    [[1138  862]\n",
    "    [ 350 1650]] ,\n",
    "    ...& lastly, the classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.76      0.57      0.65      2000\n",
    "            1       0.66      0.82      0.73      2000\n",
    "\n",
    "        accuracy                           0.70      4000\n",
    "    macro avg       0.71      0.70      0.69      4000\n",
    "    weighted avg       0.71      0.70      0.69      4000\n",
    "\n",
    "    -----\n",
    "\n",
    "#### ***From A1 Results for Reference:*** Initial Full 80k-Row Processing Results Raw Output\n",
    "\n",
    "    Algorithm #1, Version A: Gaussian Näive Bayes Performance, Metrics, & Results:\n",
    "    ...Accuracy was found to be,  59.199999999999996 %,\n",
    "    ...F1 Score was found to be:  0.3664596273291925 ,\n",
    "    ...with a Confusion Matrix: \n",
    "    [[948  52]\n",
    "    [764 236]] ,\n",
    "    ...& lastly, the classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.55      0.95      0.70      1000\n",
    "            1       0.82      0.24      0.37      1000\n",
    "\n",
    "        accuracy                           0.59      2000\n",
    "    macro avg       0.69      0.59      0.53      2000\n",
    "    weighted avg       0.69      0.59      0.53      2000\n",
    "\n",
    "    -----\n",
    "\n",
    "    Algorithm #2, Version A: Logistic Regression Performance, Metrics, & Results:\n",
    "    ...Accuracy was found to be,  92.7 %,\n",
    "    ...F1 Score was found to be:  0.9272908366533865 ,\n",
    "    ...with a Confusion Matrix: \n",
    "    [[923  77]\n",
    "    [ 69 931]] ,\n",
    "    ...& lastly, the classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.93      0.92      0.93      1000\n",
    "            1       0.92      0.93      0.93      1000\n",
    "\n",
    "        accuracy                           0.93      2000\n",
    "    macro avg       0.93      0.93      0.93      2000\n",
    "    weighted avg       0.93      0.93      0.93      2000\n",
    "\n",
    "    -----\n",
    "\n",
    "    Algorithm #1, Version B: Gaussian Näive Bayes Performance, Metrics, & Results:\n",
    "    ...Accuracy was found to be,  59.3 %,\n",
    "    ...F1 Score was found to be:  0.36899224806201547 ,\n",
    "    ...with a Confusion Matrix: \n",
    "    [[948  52]\n",
    "    [762 238]] ,\n",
    "    ...& lastly, the classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.55      0.95      0.70      1000\n",
    "            1       0.82      0.24      0.37      1000\n",
    "\n",
    "        accuracy                           0.59      2000\n",
    "    macro avg       0.69      0.59      0.53      2000\n",
    "    weighted avg       0.69      0.59      0.53      2000\n",
    "\n",
    "    -----\n",
    "\n",
    "    Algorithm #2, Version B: Logistic Regression Performance, Metrics, & Results:\n",
    "    ...Accuracy was found to be,  92.80000000000001 %,\n",
    "    ...F1 Score was found to be:  0.9281437125748503 ,\n",
    "    ...with a Confusion Matrix: \n",
    "    [[926  74]\n",
    "    [ 70 930]] ,\n",
    "    ...& lastly, the classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.93      0.93      0.93      1000\n",
    "            1       0.93      0.93      0.93      1000\n",
    "\n",
    "        accuracy                           0.93      2000\n",
    "    macro avg       0.93      0.93      0.93      2000\n",
    "    weighted avg       0.93      0.93      0.93      2000\n",
    "\n",
    "    -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook a3-Pretrained-Language-Models-dan-jang.ipynb to pdf\n",
      "[NbConvertApp] Writing 92330 bytes to notebook.tex\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] Running xelatex 3 times: ['xelatex', 'notebook.tex', '-quiet']\n",
      "[NbConvertApp] Running bibtex 1 time: ['bibtex', 'notebook']\n",
      "[NbConvertApp] WARNING | b had problems, most likely because there were no citations\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 3733661 bytes to a3-Pretrained-Language-Models-dan-jang.pdf\n"
     ]
    }
   ],
   "source": [
    "##### Juypter Notebook -> PDF Conversion thingy\n",
    "\n",
    "#!pip install nbconvert\n",
    "\n",
    "!jupyter nbconvert a3-Pretrained-Language-Models-dan-jang.ipynb --to pdf "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
