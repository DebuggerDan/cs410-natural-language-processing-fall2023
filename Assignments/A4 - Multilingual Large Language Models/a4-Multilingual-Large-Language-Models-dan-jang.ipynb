{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS410: Natural Language Processing, Fall 2023\n",
    "## A4: Multilingual Large Language Models (LLMs), Dan Jang - 11/27/2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description of Assignment\n",
    "\n",
    "##### Introduction\n",
    "As the training & testing datasets from our first & second assignments has become defunct recently ([*The Multilingual Amazon Reviews Corpus* by Phillip Keung, Yichao Lu, György Szarvas, & Noah A. Smith (October 6th, 2020)](https://arxiv.org/abs/2010.02573) [1]), in this assignment, *A4: Multilingual Large Language Models (LLMs)*, we will be using a new dataset, the \"[*Unified Multilingual Sentiment Analysis Benchmark*](https://huggingface.co/datasets/cardiffnlp/tweet_sentiment_multilingual)\" [2] - where we will be exploring & comparing the performance of two, specific ***Large Language Models*** (**LLMs**):\n",
    "\n",
    "1. [*Meta*](https://ai.meta.com/blog/large-language-model-llama-meta-ai/)'s [***```LLaMA 2```***](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) (*Large Language Model Meta AI 2*) [3], [4]\n",
    "\n",
    "&\n",
    "\n",
    "2. [*OpenAI*](https://platform.openai.com/docs/models/)'s [***```ChatGPT```***](https://openai.com/blog/chatgpt) (As of November 27th, 2023, ***```ChatGPT```*** is currently running the November 6th, 2023 Update model version of *GPT-3.5 Turbo* [***```gpt-3.5-turbo-1106```***](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)) [5]-[7]\n",
    "\n",
    "Like the previous two assignment, this assignment focuses on using specific NLP models - specifically, [*Meta*](https://ai.meta.com/blog/large-language-model-llama-meta-ai/)'s [***```LLaMA 2```***](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) [3], [4] and [*OpenAI*](https://platform.openai.com/docs/models/)'s [***```ChatGPT```***](https://openai.com/blog/chatgpt) ***Large Language Models*** (**LLMs**) [5], [6].\n",
    "\n",
    "As a different approach in comparison to our previous three (3) assignments, in this current assignment, *A4*, instead of implementing a full text-classification model for sentiment-prediction, we will instead be exploring the techniques of [*prompt engineering*](https://arxiv.org/abs/2310.04438) [8] in creating a text-classifier - in comparison to previous assignment, *A3*, where we had used the monolingual *Pretrained Language Models* (***PLMs***) of [***```BERT```***](https://arxiv.org/abs/1810.04805) [9] and [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) [10].\n",
    "\n",
    "##### Data Preparation\n",
    "In *A4*, we will be using a new dataset, [*Unified Multilingual Sentiment Analysis Benchmark*](https://huggingface.co/datasets/cardiffnlp/tweet_sentiment_multilingual) [2], which is a multilingual dataset with Tweets labeled with a ```sentiment``` value, where the classification labels are as follows:\n",
    "\n",
    "```0``` indicating *```negative```*,\n",
    "\n",
    "```1``` indicating *```neutral```*,\n",
    "\n",
    "...and a ```2``` indicating *```positive```*.\n",
    "\n",
    "The [*Unified Multilingual Sentiment Analysis Benchmark*](https://huggingface.co/datasets/cardiffnlp/tweet_sentiment_multilingual) [2] dataset contains sentiment-labeled Tweets in Arabic, English, French, German, Hindi, Italian, Portuguese, and Spanish.\n",
    "\n",
    "Instead of using several thousands of data-entries, for both a training and a testing dataset to train and test our text-classification model like we've explored in our previous assignments, of assignments *A1* through *A3* - in this assignment, *A4*, we will *instead* be utilizing a __much smaller subset of data__ from our [***new dataset***](https://huggingface.co/datasets/cardiffnlp/tweet_sentiment_multilingual) [2] & only utilizing a testing dataset based on a sampling of Tweet-entries across the different languages, in lieu of using both a testing and training dataset.\n",
    "\n",
    "Specifically, we will be creating a multilingual, Tweet-entry-based testing dataset, of around ~50 instances of the testing dataset for each language as aforementioned, and sentimentality-wise, we will only look at the Tweet-entries with either the *```positive```* or *```negative```* sentiment labels, and will be ignoring Tweet-entries with the *```neutral```* sentiment label.\n",
    "\n",
    "Although no training will be required, we will still need to carefully prepare a balanced test set across both classes, and throughout our various languages sampled from the [***new dataset***](https://huggingface.co/datasets/cardiffnlp/tweet_sentiment_multilingual) [2].\n",
    "\n",
    "##### Prompt Engineering\n",
    "\n",
    "Researcher Golam Md. Muktadir of the University of California, Santa Cruz, Computer Science and Engineering (muktadir@ucsc.edu) describes in good detail, the past-to-present history of *prompt engineering* in their relatively recent September 30th, 2023 paper, \"[*A Brief History of Prompt: Leveraging Language Models*](https://arxiv.org/abs/2310.04438)\" [8].\n",
    "\n",
    "Modern day *prompt engineering* can be described by the following description...\n",
    "\n",
    "\"...***researchers and practitioners [who] have explored various techniques to harness the full potential of language models, leveraging the power of prompts to guide, control, and tailor the output of these sophisticated AI systems.***...\"\n",
    "\n",
    "...where this *__very description__* of *prompt engineering* originates from a generated text borne of *prompt engineering* itself, as given by Researcher Muktadir in their paper's introduction, from these two prompts [8]:\n",
    "\n",
    "\"**Prompt #1**: *You are a scholar in machine learning and language models. I am writing a paper on the history of prompt engineering and generation. Can you give me a timeline for prompt engineering evolution? (We used this timeline to create prompts for each section later)*\"\n",
    "\n",
    "&\n",
    "\n",
    "\"**Prompt #2**: *Write the introduction of this paper. Emphasize that this paper focuses on how language prompts and queries have been used so far.*\"\n",
    "\n",
    "While the techniques of *prompt engineering*, seemingly only recently popularized, or rather, been made the spotlight mainstream subject of discussion of recent, in regard to the wide field of *Natural Language Processing* (***NLP***) - this popularization, of course, significantly due to the rise of [*OpenAI*](https://platform.openai.com/docs/models/)'s [***```ChatGPT```***](https://openai.com/blog/chatgpt) [6], [7] - however, nonetheless are a set of techniques a history long prior to this very novel advent of widely accessible platforms providing services based on *Natural Language Processing* (***NLP***) study & more specifically, through *Large Language Models* (***LLMs***) [8].\n",
    "\n",
    "As section-titled in their paper, Researcher Muktadir describes an early era of *prompt engineering* in \"*Prehistoric Prompting: Pre NN-Era*\", which describes among the most preliminary studies into what we know as *prompt engineering* today - as far back as the 1960s through the 1970s with \"*Early Natural Language Interfaces*\", the 1990s through the 2010s with \"*Advances in Natural Language Processing*\", e.g. *Neural Networks* (***NNs***) and *Machine Learning* (***ML***), to modern-day, featuring the rise of **Multilingual Large Language Models** (***MLLMs***) of those, we will exploring of two such ***MLLMs*** [8].\n",
    "\n",
    "\n",
    "##### Multilingual Large Language Models (Multilingual LLMs, or 'MLLMs')\n",
    "\n",
    "One way we can/will be accessing our two **Multilingual Large Language Models** (***MLLMs***), [*Meta*](https://ai.meta.com/blog/large-language-model-llama-meta-ai/)'s [***```LLaMA 2```***](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) [3], [4] and [*OpenAI*](https://platform.openai.com/docs/models/)'s [***```ChatGPT```***](https://openai.com/blog/chatgpt) **Multilingual Large Language Models** (***MLLMs***) [5], [6], would be through the graphical interface chat platform provided through [*HuggingFace* Chat](https://huggingface.co/chat) [11] (or through *HuggingFace*'s various NLP libraries, e.g. as used from our *A3* assignment, the [```transformers```](https://huggingface.co/docs/transformers/index)) library [12]).\n",
    "\n",
    "In this assignment, *A4*, we will be focusing on __answering the following questions__ through our exploration of the two aforementioned ***MLLMs*** - and the results and analysis from our exploration:\n",
    "\n",
    "1. \"*How do the two LLMs perform? Which one is better? Any possible explanation?*\"\n",
    "\n",
    "2. \"*Comparing the results across the six different languages, what do you observe? Any possible explanation?*\"\n",
    "\n",
    "3. \"*What challenges did you face?*\"\n",
    "\n",
    "##### Meta's Open-Source **LLaMA 2** (*Large Language Model Meta AI 2*) Multilingual Large Language Model (MLLM) [3], [4]\n",
    "The first ***MLLM*** we will be exploring will be the ***__open-source__***, [***```LLaMA 2```***](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) (*Large Language Model Meta AI 2*) [4] ***MLLM***, which was first co-released by [*Meta* and *Microsoft*](https://ai.meta.com/blog/llama-2/), back on June 17th, 2023 [14].\n",
    "\n",
    "This ***MLLM*** was first released & described as ***```LLaMA (Version 1)```*** in the paper, [\"*LLaMA: Open and Efficient Foundation Language Models*\"](https://arxiv.org/abs/2302.13971) [13] - which was first published earlier this year in February 27th, 2023, by the [*Meta AI*](https://ai.meta.com/blog/large-language-model-llama-meta-ai/) team [3] - authored by Researchers Hugo Touvron, and thirteen (13) other authors.\n",
    "\n",
    "This first version of ***```LLaMA```*** was described as \"*a collection of foundation language models ranging from 7B to 65B parameters*\", these models having been trained on \"*trillions of tokens*\", and importantly, claiming that *it is* and *was possible* to \"*train state-of-the-art models using publicly available datasets* **__exclusively__**\" - claiming, that the-then ***```LLaMA-13B```*** model outperformed the ***```GPT-3 (175B)```*** model on \"*most benchmarks*\" [13].\n",
    "\n",
    "The second iteration, ***```LLaMA 2```*** has been released as part of various ***fine-tuned LLMs***, e.g. *```LLaMA 2-Chat```*, *```Code LLaMA 2```*, etc. - as to represent the latest efforts by [*Meta* and *Microsoft*](https://ai.meta.com/blog/llama-2/) to provide their updated advancements of their open-source ***LLM*** - where, in their [**paper**](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/), they claim that the latest ***```LLaMA 2```*** ***LLM(s)*** are capable and *very* comparable in performance to those of other closed-source ***LLMs***, either in regard to a closed-source based set of techniques used to create the ***LLM(s)*** and/or a closed-source service that is provided as *SaaS* (***Software as a Service***), e.g. [*OpenAI*](https://platform.openai.com/docs/models/)'s\n",
    " [***```ChatGPT```***](https://openai.com/blog/chatgpt) [14], [4], [5], [6]. \n",
    "\n",
    "##### OpenAI's Proprietary **ChatGPT** (*Chat Generative Pretrained Transformer*) Multilingual Large Language Model (MLLM) [5]-[7]\n",
    "The most famous ***MLLM*** model as of recent mainstream attention, is the proprietary model [***```ChatGPT```***](https://openai.com/blog/chatgpt) (based on *Generative Pretrained Transformer 3.5 Turbo*, currently on the November 6th, 2023 update-iteration, [***```gpt-3.5-turbo-1106```***](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)) is based off advancements made by [*OpenAI*](https://platform.openai.com/docs/models/) on the closed-source 'release' of [***```GPT-3```***](https://arxiv.org/abs/2005.14165), and of course, follows the progress made from the second predecessor, [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) [6], [7], [5], [15], [10].\n",
    "\n",
    "Recalling from *A3*:\n",
    "In ***```GPT-2```***'s [technical paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), *OpenAI* researchers Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever describes the ***```GPT-2```*** model - a ***__unidirectional__*** model trained on a dataset of *8 million web pages* with **1.5 billion parameters** ([*OpenAI*](https://openai.com/research/better-language-models), [Radford et al.](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) [10].\n",
    "\n",
    "However, if we were to make a quick GPT lineage comparison between ***```gpt-3.5-turbo-1106```*** vs. ***```GPT-2```***, of course, the former, ***```gpt-3.5-turbo-1106```***, has up to ***175 billion parameters*** & the latter, at least the largest-parameter version of ***```GPT-2```***, has still only ***1.5 billion parameters*** - representing an extremely significant increase in parameter-power in ***```gpt-3.5-turbo-1106```*** over its long predecessor ***```GPT-2```*** model.\n",
    "\n",
    "##### Comparison of **LLaMA-2** vs. **ChatGPT** **Multilingual Large Language Models** (***MLLMs***) Architectures & Designs\n",
    "\n",
    "When comparing [**```LLaMA 2```**](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) vs. [**```ChatGPT```**](https://openai.com/blog/chatgpt) ([***```gpt-3.5-turbo-1106```***](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)), the first difference we can note is that, since [**```LLaMA 1```**](https://ai.meta.com/blog/large-language-model-llama-meta-ai/) was released & now with [**```LLaMA 2```**](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/), [**```LLaMA```**](https://arxiv.org/abs/2302.13971) has remained available as a series of ***open-source*** **Multilingual Large Language Models** (***MLLM***), as released & created by *Meta's AI Team* (& [*Microsoft* for version 2](https://ai.meta.com/blog/llama-2/)) - where anyone is able to request & receive access to freely download the model & its fine-tuned derivatives as per the nature open-source vs. the proprietary nature of [**```ChatGPT```**](https://openai.com/blog/chatgpt), which is only offered indirectly, as a **Software-as-a-Service** (***SaaS***) through the free version of *OpenAI*'s main *```ChatGPT```* *web-platform* ([chat.openai.com](https://chat.openai.com/)) or as a paid service through *OpenAI*'s [*API platform*](https://platform.openai.com/docs/models/) [4], [6], [7], [3], [13], [14], [6], [5].\n",
    "\n",
    "From September 1st, 2023, Enterprise AI Strategist Sunil Ramlochan of the *Prompt Engineering Institute* described in the summary of their article, [*How Does Llama-2 Compare to GPT-4/3.5 and Other AI Language Models*](https://promptengineering.org/how-does-llama-2-compare-to-gpt-and-other-ai-language-models/) that **```LLaMA 2```** \"*competes on accuracy despite lower complexity [vs. the complexity of GPT-3.5]*\" - where such strong accuracy could be through *Meta*'s various improvements from **LLaMA 1** to **LLaMA 2**, e.g. *Ghost Attention*, which may improve 'dialogue context tracking' [16].\n",
    "\n",
    "As cited in [16], Researcher Waleed Kadous of *anyscale* performed an analysis of our two **Multilingual Large Language Models** (***MLLMs***), in their supportively titled (of the claims made in [16]) article, \"[*Llama 2 is about as factually accurate as GPT-4 for summaries and is 30X cheaper*](https://www.anyscale.com/blog/llama-2-is-about-as-factually-accurate-as-gpt-4-for-summaries-and-is-30x-cheaper?ref=promptengineering.org)\" [17]:\n",
    "![Comparative Analysis of GPT-4, GPT-3.5-Turbo (ChatGPT), vs. LLaMA 2 by [17], as cited in [16]](GPT4-3.5-vs.-LLaMA-2-[16-17].png)\n",
    "\n",
    "\n",
    "##### Text Classification Model\n",
    "To build our text-classification model, we will __follow these steps__:\n",
    "\n",
    "1. Implementing & setting up our two (2) **Pretrained Language Models** (***PLMs***), [***```BERT```***](https://arxiv.org/abs/1810.04805) and [***```GPT-2```***](C:\\).\n",
    "\n",
    "2. Using the specific tokenizer used for each respective ***PLM*** to prepare the training & testing text data.\n",
    "\n",
    "3. Training of the text-classification model using the specifically-tokenized training dataset for each ***PLM***, based on \"sentiment_train.json.\"\n",
    "\n",
    "4. Evaluation of our text-classification model using the specifically-tokenized testing dataset for each ***PLM***, based on \"sentiment_test.json.\"\n",
    "\n",
    "##### Results & Analysis\n",
    "A detailed analysis of the model's performance by comparing the results from the output of our two algorithms, where we will __include the following__:\n",
    "\n",
    "1. *F1-score* or other relevant metrics.\n",
    "\n",
    "2. Any challenges or limitations of the text-classification model/task.\n",
    "\n",
    "***Additionally***, we will also try to provide a comparative analysis based on the results from our two previous assignments, our first assignment, *A1: Sentiment Analysis Text Classification* & from our second assignment, *A2: ```Word2Vec``` and ```GloVe``` Embeddings*.\n",
    "\n",
    "Specifically, we recall from our previous two assignments, that we first implemented text-classification models based on two suitable algorithms (*A1*), and in our second, implemented the usage of ```Word2Vec``` & ```GloVe``` pretrained embeddings to assist in our text-classification tasks (*A1*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries & Constants Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading / Loading the BERT Pretrained Language Model (PLM) through HuggingFace libraries...\n",
      "Loading the BERT Specific Tokenizer...\n",
      "...the BERT (Base, pretrained, uncased tokenization) Pretrained Language Model (PLM) has been downloaded / loaded!\n",
      "\n",
      "Downloading / Loading the GPT-2 Pretrained Language Model (PLM) through HuggingFace libraries...\n",
      "Loading the GPT-2 Specific Tokenizer...\n",
      "...the GPT-2 Pretrained Language Model (PLM) has been downloaded / loaded!\n",
      "\n",
      "Checking for GPT-2 tokenizer padding token...\n",
      "GPT-2 tokenizer has no padding token!\n",
      "...setting GPT-2 tokenizer padding token...\n",
      "...GPT-2 tokenizer padding token set!\n",
      "\n",
      "\n",
      "Model initialization all done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### CS410: Natural Language Processing, Fall 2023 - 11/13/2023\n",
    "##### A3: Pretrained Language Models (PLMs), Dan Jang - Initializations: Libraries, Models, Data, & Constants\n",
    "\n",
    "### 0.) Libraries\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import json\n",
    "import pandas\n",
    "#import huggingface_hub\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "## 1.0.) Constants, Variables, & Datasets\n",
    "\n",
    "percentness = float(100)\n",
    "# trainfile = str(trainfile)\n",
    "# testfile = str(testfile)\n",
    "traindata = []\n",
    "testdata = []\n",
    "\n",
    "# Loading the pretrained language models (PLMs), BERT & GPT-2 using HuggingFace libraries!\n",
    "## Bertie -> BERT PLM\n",
    "## Bumblebee -> GPT(ransformer)-2 PLM\n",
    "print(\"Downloading / Loading the BERT Pretrained Language Model (PLM) through HuggingFace libraries...\")\n",
    "bertie = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "print(\"Loading the BERT Specific Tokenizer...\")\n",
    "bertie_tokens = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(\"...the BERT (Base, pretrained, uncased tokenization) Pretrained Language Model (PLM) has been downloaded / loaded!\\n\")\n",
    "\n",
    "print(\"Downloading / Loading the GPT-2 Pretrained Language Model (PLM) through HuggingFace libraries...\")\n",
    "bumblebee = AutoModel.from_pretrained(\"gpt2\")\n",
    "print(\"Loading the GPT-2 Specific Tokenizer...\")\n",
    "bumblebee_tokens = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print(\"...the GPT-2 Pretrained Language Model (PLM) has been downloaded / loaded!\\n\")\n",
    "print(\"Checking for GPT-2 tokenizer padding token...\")\n",
    "if bumblebee_tokens.pad_token is None:\n",
    "    print(\"GPT-2 tokenizer has no padding token!\")\n",
    "    print(\"...setting GPT-2 tokenizer padding token...\")\n",
    "    bumblebee_tokens.pad_token = bumblebee_tokens.eos_token\n",
    "    print(\"...GPT-2 tokenizer padding token set!\")\n",
    "print(\"\\n\\nModel initialization all done!\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Implementation: *Text Classification, with data-processed using respective tokenizers from & with Two (2) Pretrained Language Models*, [***```BERT```***](https://arxiv.org/abs/1810.04805) and [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome, this is the main program for A3: Pretrained Language Models.\n",
      "Written by Dan J. for CS410: Natural Language Processing, Fall 2023.\n",
      "\n",
      "We will use two (2) pretrained language models (PLM), BERT & GPT-2.\n",
      "...to create a text-classifier to guess negative or positive sentimentiality based on various text-reviews of products.\n",
      "\n",
      "Loading the training & testing datasets...\n",
      "Successfully loaded the training & testing datasets!\n",
      "\n",
      "Is a GPU available: False\n",
      "Tokenizing the training & testing datasets for BERT...\n",
      "BERT Tokenization has been applied to its training & testing datasets!\n",
      "Tokenizing the training & testing datasets for GPT-2...\n",
      "GPT-2's tokenization has been applied to its training & testing datasets!\n",
      "-----\n",
      "\n",
      "Running text-classification model le training & testing datasets (with pretrained language models, BERT & GPT-2)...\n",
      "Running Logistic Regression algorithm, version A.) BERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tools\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..The data processing from our first PLM, BERT, is done!\n",
      "Running Logistic Regression algorithm, version B.) GPT-2...\n",
      "..The data processing from our second PLM, GPT-2, is done!\n",
      "...All Done!\n",
      "-----\n",
      "\n",
      "Here are le results [Logistic Regression, with comparative results between two (2) PLMs, BERT & GPT-2]...\n",
      "\n",
      "Logistic Regression Algorithm, Version A: BERT Pretrained Language Model-based Text-Classification Performance, Metrics, & Results:\n",
      "...Accuracy was found to be,  91.55 %,\n",
      "...F1 Score was found to be:  0.9158785465405676 ,\n",
      "...with a Confusion Matrix: \n",
      " [[911  89]\n",
      " [ 80 920]] ,\n",
      "...& lastly, the classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.92      1000\n",
      "           1       0.91      0.92      0.92      1000\n",
      "\n",
      "    accuracy                           0.92      2000\n",
      "   macro avg       0.92      0.92      0.92      2000\n",
      "weighted avg       0.92      0.92      0.92      2000\n",
      "\n",
      "-----\n",
      "\n",
      "Logistic Regression Algorithm, Version B: GPT-2 Pretrained Language Model-based Text-Classification Performance, Metrics, & Results:\n",
      "...Accuracy was found to be,  87.4 %,\n",
      "...F1 Score was found to be:  0.872598584428716 ,\n",
      "...with a Confusion Matrix: \n",
      " [[885 115]\n",
      " [137 863]] ,\n",
      "...& lastly, the classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.89      0.88      1000\n",
      "           1       0.88      0.86      0.87      1000\n",
      "\n",
      "    accuracy                           0.87      2000\n",
      "   macro avg       0.87      0.87      0.87      2000\n",
      "weighted avg       0.87      0.87      0.87      2000\n",
      "\n",
      "-----\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tools\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "##### CS410: Natural Language Processing, Fall 2023 - 11/13/2023\n",
    "##### A3: Pretrained Language Models (PLMs), Dan Jang - Main Implementation\n",
    "#### Objective: Exploring Natural Language Processing (NLP), by building a text-classifier\n",
    "#### for a text classification task, predicting whether a piece of text is \"positive\" or \"negative.\"\n",
    "#### ...focusing on two (2) pretrained language models (PLMs),\n",
    "#### ...BERT (Bidirectional Encoder Representations from Transformers) & OpenAI's GPT-2 (Generative Pretrained Transformer),\n",
    "#### ...and using the respective toenizers to each PLM to perform the text-classification task as aforementioned\n",
    "\n",
    "### 1.1.a) Logistic Regression algorithm using sklearn.linear_model.LogisticRegression\n",
    "### https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "### Returns four (4) thingys:\n",
    "# I.) accuracy_score,\n",
    "# II.) f1_score,\n",
    "# III.) confusion_matrix,\n",
    "# & IV.) classification_report.\n",
    "def plm(model, xtrain, ytrain, xtest, ytest):\n",
    "    \n",
    "    lreg = LogisticRegression()\n",
    "    \n",
    "    lreg.fit(xtrain, ytrain)\n",
    "    predictionresults = lreg.predict(xtest)\n",
    "    \n",
    "    return accuracy_score(ytest, predictionresults), f1_score(ytest, predictionresults), confusion_matrix(ytest, predictionresults), classification_report(ytest, predictionresults)\n",
    "\n",
    "### A3-Specific: Implementation functions for implementing the two pretrained language models, BERT & GPT-2.\n",
    "## Pretrained Language Model (PLM) Tokenizer Implementation Function\n",
    "def plmodel(words, model, tokenizer):\n",
    "    if tokenizer.pad_token is None:\n",
    "        raise ValueError(\"[Debug A3.1 - PLModel()]: Tokenizer has no padding token for the current model!\")\n",
    "    wordlist = tokenizer(words, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        results = model(**wordlist)\n",
    "    \n",
    "    return results.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    # wordlist = words.split()\n",
    "    # vecs = [model[w] for w in wordlist if w in model]\n",
    "    # if vecs:\n",
    "    #     return sum(vecs) / len(vecs)\n",
    "    # else:\n",
    "    #     return [0] * model.vector_size\n",
    "    \n",
    "# ## GPT(ransformer)-2 Model Implementation Function\n",
    "# def bumblebee(words, model):\n",
    "#     beds = [avgwordvec(w, model) for w in words]\n",
    "#     return beds\n",
    "\n",
    "def main(): #trainfile, testfile):\n",
    "    print(\"Welcome, this is the main program for A3: Pretrained Language Models.\")\n",
    "    print(\"Written by Dan J. for CS410: Natural Language Processing, Fall 2023.\")\n",
    "    print(\"\\nWe will use two (2) pretrained language models (PLM), BERT & GPT-2.\\n...to create a text-classifier to guess negative or positive sentimentiality based on various text-reviews of products.\")\n",
    "\n",
    "    # 1.0.I.A) Debug Statements #1a for dataset loading times:\n",
    "    print(\"\\nLoading the training & testing datasets...\")\n",
    "    # with open(trainfile, \"r\") as trainfile:\n",
    "    with open(\"sentiment_train.json\", \"r\") as trainfile:\n",
    "        #traindata = json.load(trainfile)\n",
    "        for row in trainfile:\n",
    "            traindata.append(json.loads(row))\n",
    "        \n",
    "    trainframe = pandas.DataFrame(traindata)\n",
    "        \n",
    "    # with open(testfile, \"r\") as testfile:\n",
    "    with open(\"sentiment_test.json\", \"r\") as testfile:\n",
    "        #testdata = json.load(testfile)\n",
    "        for row in testfile:\n",
    "            testdata.append(json.loads(row))\n",
    "        \n",
    "    testframe = pandas.DataFrame(testdata)\n",
    "\n",
    "    # 1.0.I.B) Debug Statements #1b for dataset loading times:\n",
    "    print(\"Successfully loaded the training & testing datasets!\\n\")\n",
    "    \n",
    "    ## 1.0.1.) Initial Preprocessing of the training & testing data\n",
    "    ## First, we isolate our two (2) columns, \"review_title\" & \"stars.\"\n",
    "    ## Second, we will convert values in the \"stars\" column so that 1 [negative] = 0 & 5 [positive] = 1.\n",
    "    ## This will allow us to make the negative or positive sentiment a binary value-based thingy.\n",
    "    trainframe = trainframe[['review_title', 'stars']]\n",
    "    trainframe['stars'] = trainframe['stars'].apply(lambda x: 1 if x == 5 else 0)\n",
    "    \n",
    "    testframe = testframe[['review_title', 'stars']]\n",
    "    testframe['stars'] = testframe['stars'].apply(lambda x: 1 if x == 5 else 0)\n",
    "    \n",
    "    ## A3-Specific: From our Slack channel (#nlp_f23), using tip to only use 25% of training dataset, evenly split\n",
    "    ## Credits to Classmate Will McIntosh from the Slack thread started by classmate Saurav Kumar Singh\n",
    "    ## & also, full credits to classmate Will McIntosh for the following code for GPU usage:\n",
    "    \n",
    "    #### Credits to Will McIntosh (11/11/2023):\n",
    "    # Testing\n",
    "    print(f\"Is a GPU available: {torch.cuda.is_available()}\")\n",
    "    #print(f\"Is this instance using a GPU?: {next(model.parameters()).is_cuda}\")\n",
    "    #### From Slack, #nlp_f23.\n",
    "\n",
    "    trainframe = trainframe.sample(frac=1, random_state=69)\n",
    "    trainframe = trainframe.iloc[:int(0.25 * len(trainframe))]\n",
    "    \n",
    "    # y2train = trainframe['stars']\n",
    "    # print(\"[A3 Debug Size-Print #3] y2train\", len(y2train))\n",
    "    \n",
    "    ytest = testframe['stars']\n",
    "    # print(\"[A3 Debug Size-Print #4] ytest\", len(ytest))\n",
    "    \n",
    "    ## Evenly split frames\n",
    "    x3train1, x3train2 = train_test_split(trainframe, test_size=0.5, random_state=69)\n",
    "    \n",
    "    y3train1 = x3train1['stars']\n",
    "    y3train2 = x3train2['stars']\n",
    "    \n",
    "    #print(\"[A3 Debug Size-Print #1] x3train1 & x3train2\", len(x3train1), len(x3train2))\n",
    "    ## A3-Specific: Applying BERT & GPT-2 PLM Specific Tokenization\n",
    "    print(\"Tokenizing the training & testing datasets for BERT...\")\n",
    "    x2train1 = x3train1['review_title'].apply(lambda x: plmodel(x, bertie, bertie_tokens))\n",
    "    #x2train1 = trainframe['review_title'].apply(lambda x: plmodel(x, bertie, bertie_tokens))\n",
    "    x2test1 = testframe['review_title'].apply(lambda x: plmodel(x, bertie, bertie_tokens))\n",
    "    print(\"BERT Tokenization has been applied to its training & testing datasets!\")\n",
    "    \n",
    "    #print(\"[A3 Debug Size-Print #2a] x2train1 & x2test1\", len(x2train1), len(x2test1))\n",
    "    \n",
    "    print(\"Tokenizing the training & testing datasets for GPT-2...\")\n",
    "    x2train2 = x3train2['review_title'].apply(lambda x: plmodel(x, bumblebee, bumblebee_tokens))\n",
    "    #x2train2 = trainframe['review_title'].apply(lambda x: plmodel(x, bumblebee, bumblebee_tokens))\n",
    "    x2test2 = testframe['review_title'].apply(lambda x: plmodel(x, bumblebee, bumblebee_tokens))\n",
    "    print(\"GPT-2's tokenization has been applied to its training & testing datasets!\")\n",
    "    \n",
    "    #print(\"[A3 Debug Size-Print #2b] x2train2 & x2test2\", len(x2train2), len(x2test2))\n",
    "\n",
    "    ### 1.0.2b) Run Text-Classification Algorithms & Print the Model Results\n",
    "    print(\"-----\\n\")\n",
    "    print(\"Running text-classification model le training & testing datasets (with pretrained language models, BERT & GPT-2)...\")\n",
    "\n",
    "    \n",
    "    print(\"Running Logistic Regression algorithm, version A.) BERT...\")\n",
    "    bed1accuracy, bed1f1, bed1cmatrix, bed1creport = plm(bertie, x2train1.tolist(), y3train1, x2test1.tolist(), ytest)\n",
    "    print(\"..The data processing from our first PLM, BERT, is done!\")\n",
    "    \n",
    "    print(\"Running Logistic Regression algorithm, version B.) GPT-2...\")\n",
    "    bed2accuracy, bed2f1, bed2cmatrix, bed2creport = plm(bumblebee, x2train2.tolist(), y3train2, x2test2.tolist(), ytest)\n",
    "    print(\"..The data processing from our second PLM, GPT-2, is done!\")\n",
    "    \n",
    "    print(\"...All Done!\")\n",
    "    print(\"-----\\n\")\n",
    "    \n",
    "    print(\"Here are le results [Logistic Regression, with comparative results between two (2) PLMs, BERT & GPT-2]...\\n\")\n",
    "    print(\"Logistic Regression Algorithm, Version A: BERT Pretrained Language Model-based Text-Classification Performance, Metrics, & Results:\")\n",
    "    print(\"...Accuracy was found to be, \", bed1accuracy * percentness, \"%,\")\n",
    "    print(\"...F1 Score was found to be: \", bed1f1, \",\")\n",
    "    print(\"...with a Confusion Matrix: \\n\", bed1cmatrix, \",\")\n",
    "    print(\"...& lastly, the classification Report: \\n\", bed1creport)\n",
    "    print(\"-----\\n\")\n",
    "    \n",
    "    print(\"Logistic Regression Algorithm, Version B: GPT-2 Pretrained Language Model-based Text-Classification Performance, Metrics, & Results:\")\n",
    "    print(\"...Accuracy was found to be, \", bed2accuracy * percentness, \"%,\")\n",
    "    print(\"...F1 Score was found to be: \", bed2f1, \",\")\n",
    "    print(\"...with a Confusion Matrix: \\n\", bed2cmatrix, \",\")\n",
    "    print(\"...& lastly, the classification Report: \\n\", bed2creport)\n",
    "    print(\"-----\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *A3: Pretrained Language Models (PLMs)*, Text-Classification Model Performance Analysis & Discussion\n",
    "\n",
    "#### Initial Data Results, Metrics, & Analysis\n",
    "\n",
    "Using the Logistic Regression Algorithm like previously for *A2*, this algorithm was used to implement our text-classification model, for the text-classification task in both *Version A* & *B*.\n",
    "\n",
    "As shown below, *Version A* featured [*Google AI*](https://blog.research.google/2018/11/open-sourcing-bert-state-of-art-pre.html)'s [***```BERT```***](https://arxiv.org/abs/1810.04805) model & *Version B* featured [*OpenAI*](https://openai.com/research/better-language-models)'s [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) model - where we saw the following results:\n",
    "\n",
    "##### Version A - ***```BERT```*** Pretrained Language Model (PLM) Results:\n",
    "\n",
    "    Accuracy:  ~91.6%\n",
    "    F1 Score:  0.9158785465405676\n",
    "\n",
    "    Confusion Matrix:\n",
    "    [[911  89]\n",
    "    [ 80 920]]\n",
    "\n",
    "    Classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.92      0.91      0.92      1000\n",
    "            1       0.91      0.92      0.92      1000\n",
    "\n",
    "    accuracy                            0.92      2000\n",
    "    macro avg       0.92      0.92      0.92      2000\n",
    "    weighted avg    0.92      0.92      0.92      2000\n",
    "\n",
    "##### Version B - ***```GPT-2```*** Pretrained Language Model (PLM) Results:\n",
    "    Accuracy:  87.4%\n",
    "    F1 Score:  0.872598584428716\n",
    "    \n",
    "    Confusion Matrix: \n",
    "    [[885 115]\n",
    "    [137 863]]\n",
    "\n",
    "    Classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.87      0.89      0.88      1000\n",
    "            1       0.88      0.86      0.87      1000\n",
    "\n",
    "    accuracy                            0.87      2000\n",
    "    macro avg       0.87      0.87      0.87      2000\n",
    "    weighted avg    0.87      0.87      0.87      2000\n",
    "\n",
    "#### Pretrained Language Model (PLM) Comparative Analysis & Discussion\n",
    "\n",
    "From the results above, we see that both [*Google AI*](https://blog.research.google/2018/11/open-sourcing-bert-state-of-art-pre.html)'s [***```BERT```***](https://arxiv.org/abs/1810.04805) & [*OpenAI*](https://openai.com/research/better-language-models)'s [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) ***PLMs*** faired very well in accuracy, of ~91.6% & 87.4% respectively.\n",
    "\n",
    "Between these two ***PLMs***, [***```BERT```***](https://arxiv.org/abs/1810.04805) & [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), we can see that from our performance results that there is/was a slight, low *difference of accuracy*, where [***```BERT```***](https://arxiv.org/abs/1810.04805) has a ***higher*** **accuracy** of *___~4.2%___* in *comparison to [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)*. \n",
    "\n",
    "Also, by looking at the respective Confusion Matrices for our two ***PLMs***, we can see that [***```BERT```***](https://arxiv.org/abs/1810.04805), of course, was able to classify sentiment more accurately than [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), but we also can observe that [***```BERT```***](https://arxiv.org/abs/1810.04805) seems to be *a little bit* more efficient in detecting *negative* sentimentality vs. [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) lean towards *positive* sentimentality - however, [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)'s decreased accuracy in detecting *negative* sentiment seems to be more significant in magnitude compared to the aforementioned, *negative* over *positive* sentimentality bias, of [***```BERT```***](https://arxiv.org/abs/1810.04805).\n",
    "\n",
    "Interestingly, a possible scenario where this supposed sentimentality bias in our text-classification task may present itself as to yield a different result, at least for accuracy & the F1 score, may lie in how the positive & negative sentiment composition during the processing of our training & testing dataset, particularly, at the time of randomized splitting.\n",
    "Specifically, if we had somehow chosen a random seed in our splitting code, that supposedly had much more positive or negative sentimentality in the composition of the reviews, we could have seen variations from our initial performance & results as described above.\n",
    "\n",
    "Ergo, between [*Google AI*](https://blog.research.google/2018/11/open-sourcing-bert-state-of-art-pre.html)'s [***```BERT```***](https://arxiv.org/abs/1810.04805) & [*OpenAI*](https://openai.com/research/better-language-models)'s [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) ***PLMs***, [***```BERT```***](https://arxiv.org/abs/1810.04805) seems to be a clearly more suitable model, at least for the text-classification task & model with our assignment's training & dataset, \"[*Multilingual Amazon Reviews Corpus*](https://arxiv.org/abs/2010.02573).\"\n",
    "\n",
    "\n",
    "#### Analysis of Current & Previous Text-Classification Models & Performance Results from *A1* & *A2*\n",
    "\n",
    "In comparison to the performance seen from *A2: ```Word2Vec``` & ```GloVe``` Embeddings*, both [*Google AI*](https://blog.research.google/2018/11/open-sourcing-bert-state-of-art-pre.html)'s [***```BERT```***](https://arxiv.org/abs/1810.04805) & [*OpenAI*](https://openai.com/research/better-language-models)'s [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) ***PLMs*** outperformed the accuracies & F1 scores from either ```Word2Vec``` or ```GloVe``` pretrained embeddings - where we recall the *A2* results as follows:\n",
    "\n",
    "*A2*: Version A, [```Word2Vec```](https://arxiv.org/abs/1301.3781) Model:\n",
    "\n",
    "    Accuracy:  ~86.3%,\n",
    "    F1 Score:  0.86105476673428\n",
    "\n",
    "    Confusion Matrix: \n",
    "    [[1754  246]\n",
    "    [ 302 1698]]\n",
    "\n",
    "    Classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.85      0.88      0.86      2000\n",
    "            1       0.87      0.85      0.86      2000\n",
    "\n",
    "    accuracy                            0.86      4000\n",
    "    macro avg       0.86      0.86      0.86      4000\n",
    "    weighted avg    0.86      0.86      0.86      4000\n",
    "\n",
    "*A2*: Version B, [```GloVe```](https://nlp.stanford.edu/projects/glove/) Model:\n",
    "\n",
    "    Accuracy: ~69.69%,\n",
    "    F1 Score: 0.7313829787234042\n",
    "\n",
    "    Confusion Matrix: \n",
    "    [[1138  862]\n",
    "    [ 350 1650]]\n",
    "\n",
    "    Classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "             0       0.76      0.57      0.65      2000\n",
    "             1       0.66      0.82      0.73      2000\n",
    "\n",
    "    accuracy                             0.70      4000\n",
    "    macro avg        0.71      0.70      0.69      4000\n",
    "    weighted avg     0.71      0.70      0.69      4000\n",
    "\n",
    "Although, we do see only a *slight* **~1.1% accuracy** improvement when we compare results from [```Word2Vec```](https://arxiv.org/abs/1301.3781) vs. [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf).\n",
    "\n",
    "However, this is a neat & expected observation, as we can recall that both [```Word2Vec```](https://arxiv.org/abs/1301.3781) & [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) are suitable or otherwise geared towards the generation of text, rather than the specificity of text-classification required for efficiency & performance with our Amazon review sentimentality prediction task.\n",
    "As expected, contrarywise, as the results from the implementation of [```GloVe```](https://nlp.stanford.edu/projects/glove/) pretrained embeddings & the [***```BERT```***](https://arxiv.org/abs/1810.04805) ***PLM*** show higher performance & accuracy compared to the [```Word2Vec```](https://arxiv.org/abs/1301.3781) pretrained embeddings & [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) ***PLM***, we could also make the observation of this supposed higher efficiency & efficacy by recalling that the [```GloVe```](https://nlp.stanford.edu/projects/glove/) pretrained embeddings were also designed & trained to create an unsupervised learning algorithm, with training that \"*performed on aggregated global word-word co-occurence statistics*\" ([Pennington, et al.](https://nlp.stanford.edu/projects/glove/), 2014), which, like the nature of [***```BERT```***](https://arxiv.org/abs/1810.04805)'s greater suitability towards text-classification tasks, could explain these higher accuracies & better performance as observed from our previous *A2* & most current, *A3* results.\n",
    "\n",
    "Recalling further, note that from our first assignment, *A1: Sentiment Analysis Text Classification*, we saw the following results from using *two (2) different algorithms* to implement the text-classification model, specifically, the *Logistic Regression* & the *Gaussian Näive Bayes* Algorithms:\n",
    "\n",
    "*From A1 Results:* Version A: *Gaussian Näive Bayes* Algorithm:\n",
    "\n",
    "    Accuracy: ~59.2%\n",
    "    F1 Score: 0.3664596273291925\n",
    "\n",
    "    CConfusion Matrix: \n",
    "    [[948  52]\n",
    "    [764 236]]\n",
    "\n",
    "    Classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.55      0.95      0.70      1000\n",
    "            1       0.82      0.24      0.37      1000\n",
    "\n",
    "    accuracy                            0.59      2000\n",
    "    macro avg       0.69      0.59      0.53      2000\n",
    "    weighted avg    0.69      0.59      0.53      2000\n",
    "\n",
    "*From A1 Results:* Version B: *Logistic Regression* Algorithm:\n",
    "\n",
    "    Accuracy:  92.7%\n",
    "    F1 Score:  0.9272908366533865\n",
    "\n",
    "    Confusion Matrix: \n",
    "    [[923  77]\n",
    "    [ 69 931]]\n",
    "\n",
    "    Classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.93      0.92      0.93      1000\n",
    "            1       0.92      0.93      0.93      1000\n",
    "\n",
    "    accuracy                            0.93      2000\n",
    "    macro avg       0.93      0.93      0.93      2000\n",
    "    weighted avg    0.93      0.93      0.93      2000\n",
    "\n",
    "This is interesting, as while it appears that using ***PLMs*** over ***pretrained embeddings*** yielded much significant improvements in both accuracy & performance, the results from using only the Logistic Regression Algorithm with no classifiers had yielded a *92.7% accuracy*, which still stands as one of the most accurate results from our three (3) assignments.\n",
    "\n",
    "However, this observation & other possible variations that can be observed, in performance & results, may be attributed to both the nature of text-tokenization, usage of classifiers, & particularly, that in the implementation of the first text-classification model of our *A1* assignment, I had trained the text-classification model cases with the ***full, eighty-thousand (80,000) rows of training data***, while in our current *A3* implementation, the training data for both the [***```BERT```***](https://arxiv.org/abs/1810.04805) & [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) ***PLMs*** were truncated randomly (with a shared random seed of ```69```) to a quarter-percent (25%) of the original number of rows, or twenty-thousand (20,000) rows of training data.\n",
    "\n",
    "#### Text-Classification Challenges & Limitations\n",
    "The initial & biggest challenge to implement these **pretrained language models** (***PLMs***) was the very significantly resource & time-intensive requirements for the computation, tokenization, & training of the two (2) ***PLMs*** used.\n",
    "\n",
    "With the current version of code & implementation, in average, it appeared that [***```BERT```***](https://arxiv.org/abs/1810.04805) took around *two-to-eight (2-to-8) minutes*, while [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) took longer, around *five-to-fifteen (5-to-15) minutes* long. Both, on average, usually took around *fifteen-to-thirty (15-to-30) minutes* to fully complete.\n",
    "\n",
    "However, in the earlier stages of coding & implementation, particularly before truncating the training & testing datasets to the currently-set percentage of twenty-five percent (25%) of the original amount of rows/lines of text, the average varied wildly, with completion durations taking *hours, upon hours* to complete or were stopped prior to completion due to time constraints.\n",
    "\n",
    "Furthermore, even post-truncation adjustments, the pretraining of the two ***PLMs***, in terms of computational power & resources, were very intensive, which had caused ***multiple***, **full computer crashes**, leading to a lot of processing delays & mental energy to continue implementing.\n",
    "\n",
    "#### Discussion for Future Performance & Efficacy Improvements\n",
    "With the issue of computational resources & the prevention of computer crashes, would be, of course, to finally sign-up for the student discounted *Google Cloud* subscription for using high-resource, cloud computing in *Google Colaboratory*, where NLP compatible, GPU-acceleration is available to expedite ***PLM***, model training, or otherwise for running intensive NLP code & tasks.\n",
    "As such, this specific idea for future performance & efficacy improvement will be implemented immediately following this current assignment, *A3*, where I will go ahead & attempt to set-up *Google Colaboratory* for use to hopefully, avoid the aforementioned resource & crash pitfalls for *A4* and our *Group Project*, heh.\n",
    "\n",
    "Furthermore, I should attempt to start early & try to implement for assignments in smaller code-blocks/pieces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References & Resources\n",
    "\n",
    "#### Libraries & Dependencies\n",
    "    numpy\n",
    "    pandas\n",
    "    torch\n",
    "    random\n",
    "\n",
    "[HuggingFace_hub](https://huggingface.co/docs/hub/models-libraries)\n",
    "\n",
    "[*Google AI*](https://blog.research.google/2018/11/open-sourcing-bert-state-of-art-pre.html)'s [*BERT*](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "[*OpenAI*](https://openai.com/research/better-language-models)'s [*GPT-2*](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "\n",
    "[*HuggingFace*](https://huggingface.co/docs/hub/models-libraries)'s [*transformers*](https://huggingface.co/docs/transformers/index)\n",
    "\n",
    "```from``` [```transformers```](https://huggingface.co/docs/transformers/index) ```import``` [```AutoModel```](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html)(s)\n",
    "\n",
    "```from``` [```transformers```](https://huggingface.co/docs/transformers/index) ```import``` [```AutoTokenizer```](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#autotokenizer)\n",
    "\n",
    "```from``` [```transformers```](https://huggingface.co/docs/transformers/index) ```import``` [```AutoTokenizer.from_pretrained```](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#transformers.AutoTokenizer.from_pretrained), [```AutoModel.from_pretrained```](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#transformers.AutoModelForPreTraining.from_pretrained)\n",
    "\n",
    "[sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "\n",
    "[sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "\n",
    "[sklearn.metrics.f1_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)\n",
    "\n",
    "[sklearn.metrics.accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n",
    "\n",
    "[sklearn.metrics.confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n",
    "\n",
    "[sklearn.metrics.classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)\n",
    "\n",
    "[nbconvert](https://nbconvert.readthedocs.io/en/latest/)\n",
    "\n",
    "\n",
    "#### References & Credits\n",
    "\n",
    "[1] [P. Keung, Y. Lu, G. Szarvas, and N. A. Smith, “The Multilingual Amazon Reviews Corpus.” arXiv, Oct. 06, 2020. doi: 10.48550/arXiv.2010.02573.](https://arxiv.org/abs/2010.02573)\n",
    "\n",
    "[2] [\"cardiffnlp/tweet_sentiment_multilingual · Datasets at Hugging Face.\" [Online]. Available: https://huggingface.co/datasets/cardiffnlp/tweet_sentiment_multilingual](https://huggingface.co/datasets/cardiffnlp/tweet_sentiment_multilingual)\"\n",
    "\n",
    "[3] [\"Introducing LLaMA: A foundational, 65-billion-parameter language model.\" [Online]. Available: https://ai.meta.com/blog/large-language-model-llama-meta-ai/](https://ai.meta.com/blog/large-language-model-llama-meta-ai/)\n",
    "\n",
    "[4] [\"Llama 2: Open Foundation and Fine-Tuned Chat Models | Research - AI at Meta.\" [Online]. Available: https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)\n",
    "\n",
    "[5] [\"OpenAI Platform.\" [Online]. Available: https://platform.openai.com](https://platform.openai.com/docs/models/)\n",
    "\n",
    "[6] [\"Introducing ChatGPT.\" [Online]. Available: https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt)\n",
    "\n",
    "[7] [\"New models and developer products announced at DevDay.\" [Online]. Available: https://openai.com/blog/new-models-and-developer-products-announced-at-devday](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)\n",
    "\n",
    "[8] [G. M. Muktadir, \"A Brief History of Prompt: Leveraging Language Models. (Through Advanced Prompting).\" arXiv, Nov. 28, 2023. doi: 10.48550/arXiv.2310.04438.](https://arxiv.org/abs/2310.04438)\n",
    "\n",
    "[9] [J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\" arXiv, May 24, 2019. doi: 10.48550/arXiv.1810.04805.](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "[10] [A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, \"Language Models are Unsupervised Multitask Learners\".](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "\n",
    "[11] [\"HuggingChat.\" [Online]. Available: https://huggingface.co/chat](https://huggingface.co/chat)\n",
    "\n",
    "[12] [\"🤗 Transformers.\" [Online]. Available: https://huggingface.co/docs/transformers/index](https://huggingface.co/docs/transformers/index)\n",
    "\n",
    "[13] [H. Touvron et al., \"LLaMA: Open and Efficient Foundation Language Models.\" arXiv, Feb. 27, 2023. doi: 10.48550/arXiv.2302.13971.](https://arxiv.org/abs/2302.13971)\n",
    "\n",
    "[14] [\"Meta and Microsoft Introduce the Next Generation of Llama.\" [Online]. Available: https://ai.meta.com/blog/llama-2/](https://ai.meta.com/blog/llama-2/)\n",
    "\n",
    "[15] [T. B. Brown et al., \"Language Models are Few-Shot Learners.\" arXiv, Jul. 22, 2020. doi: 10.48550/arXiv.2005.14165.](https://arxiv.org/abs/2005.14165)\n",
    "\n",
    "[16] [\"How Does Llama-2 Compare to GPT-4/3.5 and Other AI Language Models,\" Prompt Engineering Institute. [Online]. Available: https://promptengineering.org/how-does-llama-2-compare-to-gpt-and-other-ai-language-models/](https://promptengineering.org/how-does-llama-2-compare-to-gpt-and-other-ai-language-models/)\n",
    "\n",
    "[17] [\"Llama 2 vs. GPT-4: Nearly As Accurate and 30X Cheaper,\" Anyscale. [Online]. Available: https://www.anyscale.com/blog/llama-2-is-about-as-factually-accurate-as-gpt-4-for-summaries-and-is-30x-cheaper?ref=promptengineering.org](https://www.anyscale.com/blog/llama-2-is-about-as-factually-accurate-as-gpt-4-for-summaries-and-is-30x-cheaper?ref=promptengineering.org)\n",
    "\n",
    "\n",
    "Credits to GitHub Copilot & ChatGPT for code implementation assistance.\n",
    "\n",
    "#### Special Thanks\n",
    "\n",
    "Thanks to fellow classmate *Will McIntosh* for their helpful tips & tricks for resource management particularly for ***PLMs*** in the current *A3* assignment, from the ```#nlp_f23``` class-channel on *pdx-cs* Slack, November 11th, 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extra Stuff\n",
    "\n",
    "### **A3 Results**: Raw Output from [*BERT*](https://arxiv.org/abs/1810.04805) & *OpenAI*'s [*GPT-2*](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) Pretrained Language Models (PLMs), using the Logistic Regression Algorithm\n",
    "    Here are le results [Logistic Regression, with comparative results between two (2) PLMs, BERT & GPT-2]...\n",
    "\n",
    "    Logistic Regression Algorithm, Version A: BERT Pretrained Language Model-based Text-Classification Performance, Metrics, & Results:\n",
    "    ...Accuracy was found to be,  91.55 %,\n",
    "    ...F1 Score was found to be:  0.9158785465405676 ,\n",
    "    ...with a Confusion Matrix: \n",
    "    [[911  89]\n",
    "    [ 80 920]] ,\n",
    "    ...& lastly, the classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.92      0.91      0.92      1000\n",
    "            1       0.91      0.92      0.92      1000\n",
    "\n",
    "        accuracy                           0.92      2000\n",
    "    macro avg       0.92      0.92      0.92      2000\n",
    "    weighted avg       0.92      0.92      0.92      2000\n",
    "\n",
    "    -----\n",
    "\n",
    "    Logistic Regression Algorithm, Version B: GPT-2 Pretrained Language Model-based Text-Classification Performance, Metrics, & Results:\n",
    "    ...Accuracy was found to be,  87.4 %,\n",
    "    ...F1 Score was found to be:  0.872598584428716 ,\n",
    "    ...with a Confusion Matrix: \n",
    "    [[885 115]\n",
    "    [137 863]] ,\n",
    "    ...& lastly, the classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.87      0.89      0.88      1000\n",
    "            1       0.88      0.86      0.87      1000\n",
    "\n",
    "        accuracy                           0.87      2000\n",
    "    macro avg       0.87      0.87      0.87      2000\n",
    "    weighted avg       0.87      0.87      0.87      2000\n",
    "\n",
    "-----\n",
    "\n",
    "### **A2 Results**: Raw Output from ```Word2Vec``` & GloVe Embedding Results, using the Logistic Regression Algorithm\n",
    "\n",
    "    Logistic Regression Algorithm, Version A: Word2Vec Pretrained Model-based Embeddings Performance, Metrics, & Results:\n",
    "    ...Accuracy was found to be,  86.3 %,\n",
    "    ...F1 Score was found to be:  0.86105476673428 ,\n",
    "    ...with a Confusion Matrix: \n",
    "    [[1754  246]\n",
    "    [ 302 1698]] ,\n",
    "    ...& lastly, the classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.85      0.88      0.86      2000\n",
    "            1       0.87      0.85      0.86      2000\n",
    "\n",
    "        accuracy                           0.86      4000\n",
    "    macro avg       0.86      0.86      0.86      4000\n",
    "    weighted avg       0.86      0.86      0.86      4000\n",
    "\n",
    "    -----\n",
    "\n",
    "    Logistic Regression Algorithm, Version B: GloVe Pretrained Model-based Embeddings Performance, Metrics, & Results:\n",
    "    ...Accuracy was found to be,  69.69999999999999 %,\n",
    "    ...F1 Score was found to be:  0.7313829787234042 ,\n",
    "    ...with a Confusion Matrix: \n",
    "    [[1138  862]\n",
    "    [ 350 1650]] ,\n",
    "    ...& lastly, the classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.76      0.57      0.65      2000\n",
    "            1       0.66      0.82      0.73      2000\n",
    "\n",
    "        accuracy                           0.70      4000\n",
    "    macro avg       0.71      0.70      0.69      4000\n",
    "    weighted avg       0.71      0.70      0.69      4000\n",
    "\n",
    "    -----\n",
    "\n",
    "#### ***From A1 Results for Reference:*** Initial Full 80k-Row Processing Results Raw Output\n",
    "\n",
    "    Algorithm #1, Version A: Gaussian Näive Bayes Performance, Metrics, & Results:\n",
    "    ...Accuracy was found to be,  59.199999999999996 %,\n",
    "    ...F1 Score was found to be:  0.3664596273291925 ,\n",
    "    ...with a Confusion Matrix: \n",
    "    [[948  52]\n",
    "    [764 236]] ,\n",
    "    ...& lastly, the classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.55      0.95      0.70      1000\n",
    "            1       0.82      0.24      0.37      1000\n",
    "\n",
    "        accuracy                           0.59      2000\n",
    "    macro avg       0.69      0.59      0.53      2000\n",
    "    weighted avg       0.69      0.59      0.53      2000\n",
    "\n",
    "    -----\n",
    "\n",
    "    Algorithm #2, Version A: Logistic Regression Performance, Metrics, & Results:\n",
    "    ...Accuracy was found to be,  92.7 %,\n",
    "    ...F1 Score was found to be:  0.9272908366533865 ,\n",
    "    ...with a Confusion Matrix: \n",
    "    [[923  77]\n",
    "    [ 69 931]] ,\n",
    "    ...& lastly, the classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.93      0.92      0.93      1000\n",
    "            1       0.92      0.93      0.93      1000\n",
    "\n",
    "        accuracy                           0.93      2000\n",
    "    macro avg       0.93      0.93      0.93      2000\n",
    "    weighted avg       0.93      0.93      0.93      2000\n",
    "\n",
    "    -----\n",
    "\n",
    "    Algorithm #1, Version B: Gaussian Näive Bayes Performance, Metrics, & Results:\n",
    "    ...Accuracy was found to be,  59.3 %,\n",
    "    ...F1 Score was found to be:  0.36899224806201547 ,\n",
    "    ...with a Confusion Matrix: \n",
    "    [[948  52]\n",
    "    [762 238]] ,\n",
    "    ...& lastly, the classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.55      0.95      0.70      1000\n",
    "            1       0.82      0.24      0.37      1000\n",
    "\n",
    "        accuracy                           0.59      2000\n",
    "    macro avg       0.69      0.59      0.53      2000\n",
    "    weighted avg       0.69      0.59      0.53      2000\n",
    "\n",
    "    -----\n",
    "\n",
    "    Algorithm #2, Version B: Logistic Regression Performance, Metrics, & Results:\n",
    "    ...Accuracy was found to be,  92.80000000000001 %,\n",
    "    ...F1 Score was found to be:  0.9281437125748503 ,\n",
    "    ...with a Confusion Matrix: \n",
    "    [[926  74]\n",
    "    [ 70 930]] ,\n",
    "    ...& lastly, the classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.93      0.93      0.93      1000\n",
    "            1       0.93      0.93      0.93      1000\n",
    "\n",
    "        accuracy                           0.93      2000\n",
    "    macro avg       0.93      0.93      0.93      2000\n",
    "    weighted avg       0.93      0.93      0.93      2000\n",
    "\n",
    "    -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook a3-Pretrained-Language-Models-dan-jang.ipynb to pdf\n",
      "[NbConvertApp] Writing 92330 bytes to notebook.tex\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] Running xelatex 3 times: ['xelatex', 'notebook.tex', '-quiet']\n",
      "[NbConvertApp] Running bibtex 1 time: ['bibtex', 'notebook']\n",
      "[NbConvertApp] WARNING | b had problems, most likely because there were no citations\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 3733661 bytes to a3-Pretrained-Language-Models-dan-jang.pdf\n"
     ]
    }
   ],
   "source": [
    "##### Juypter Notebook -> PDF Conversion thingy\n",
    "\n",
    "#!pip install nbconvert\n",
    "\n",
    "!jupyter nbconvert a3-Pretrained-Language-Models-dan-jang.ipynb --to pdf "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
