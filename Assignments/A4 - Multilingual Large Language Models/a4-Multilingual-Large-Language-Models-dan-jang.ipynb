{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS410: Natural Language Processing, Fall 2023\n",
    "## A4: Multilingual Large Language Models (LLMs), Dan Jang - 11/27/2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Content Warning**: *Alike many \"natural language processing\" oriented or sources for text & data, the text & data used in this project may have been collected from public websites or sources, and may be unfiltered. Thus, some text & data may be disturbing, disagreeable, or otherwise objectionable.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description of Assignment\n",
    "\n",
    "##### Introduction\n",
    "As the training & testing datasets from our first & second assignments has become defunct recently ([*The Multilingual Amazon Reviews Corpus* by Phillip Keung, Yichao Lu, Gy√∂rgy Szarvas, & Noah A. Smith (October 6th, 2020)](https://arxiv.org/abs/2010.02573) [1]), in this assignment, *A4: Multilingual Large Language Models (LLMs)*, we will be using a new dataset, the \"[*Unified Multilingual Sentiment Analysis Benchmark*](https://huggingface.co/datasets/cardiffnlp/tweet_sentiment_multilingual)\" [2] - where we will be exploring & comparing the performance of two, specific ***Large Language Models*** (**LLMs**):\n",
    "\n",
    "1. [*Meta*](https://ai.meta.com/blog/large-language-model-llama-meta-ai/)'s [***```LLaMA 2```***](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) (*Large Language Model Meta AI 2*) [3], [4]\n",
    "\n",
    "&\n",
    "\n",
    "2. [*OpenAI*](https://platform.openai.com/docs/models/)'s [***```ChatGPT```***](https://openai.com/blog/chatgpt) (As of November 27th, 2023, ***```ChatGPT```*** is currently running the November 6th, 2023 Update model version of *GPT-3.5 Turbo* [***```gpt-3.5-turbo-1106```***](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)) [5]-[7]\n",
    "\n",
    "Like the previous two assignment, this assignment focuses on using specific NLP models - specifically, [*Meta*](https://ai.meta.com/blog/large-language-model-llama-meta-ai/)'s [***```LLaMA 2```***](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) [3], [4] and [*OpenAI*](https://platform.openai.com/docs/models/)'s [***```ChatGPT```***](https://openai.com/blog/chatgpt) ***Large Language Models*** (**LLMs**) [5], [6].\n",
    "\n",
    "As a different approach in comparison to our previous three (3) assignments, in this current assignment, *A4*, instead of implementing a full text-classification model for sentiment-prediction, we will instead be exploring the techniques of [*prompt engineering*](https://arxiv.org/abs/2310.04438) [8] in creating a text-classifier - in comparison to previous assignment, *A3*, where we had used the monolingual *Pretrained Language Models* (***PLMs***) of [***```BERT```***](https://arxiv.org/abs/1810.04805) [9] and [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) [10].\n",
    "\n",
    "##### Data Preparation\n",
    "In *A4*, we will be using a new dataset, [*Unified Multilingual Sentiment Analysis Benchmark*](https://huggingface.co/datasets/cardiffnlp/tweet_sentiment_multilingual) [2], which is a multilingual dataset with Tweets labeled with a ```sentiment``` value, where the classification labels are as follows:\n",
    "\n",
    "```0``` indicating *```negative```*,\n",
    "\n",
    "```1``` indicating *```neutral```*,\n",
    "\n",
    "...and a ```2``` indicating *```positive```*.\n",
    "\n",
    "The [*Unified Multilingual Sentiment Analysis Benchmark*](https://huggingface.co/datasets/cardiffnlp/tweet_sentiment_multilingual) [2] dataset contains sentiment-labeled Tweets in Arabic, English, French, German, Hindi, Italian, Portuguese, and Spanish.\n",
    "\n",
    "Instead of using several thousands of data-entries, for both a training and a testing dataset to train and test our text-classification model like we've explored in our previous assignments, of assignments *A1* through *A3* - in this assignment, *A4*, we will *instead* be utilizing a __much smaller subset of data__ from our [***new dataset***](https://huggingface.co/datasets/cardiffnlp/tweet_sentiment_multilingual) [2] & only utilizing a testing dataset based on a sampling of Tweet-entries across the different languages, in lieu of using both a testing and training dataset.\n",
    "\n",
    "Specifically, we will be creating a multilingual, Tweet-entry-based testing dataset, of a recommended amount of ~50 instances (the actual number used in my implementation will be specified below in my ***Text Classification Through Prompt Engineering*** section) of the testing dataset for each language as aforementioned, and sentimentality-wise, we will only look at the Tweet-entries with either the *```positive```* or *```negative```* sentiment labels, and will be ignoring Tweet-entries with the *```neutral```* sentiment label.\n",
    "\n",
    "Although no training will be required, we will still need to carefully prepare a balanced test set across both classes, and throughout our various languages sampled from the [***new dataset***](https://huggingface.co/datasets/cardiffnlp/tweet_sentiment_multilingual) [2].\n",
    "\n",
    "##### Prompt Engineering\n",
    "\n",
    "Researcher Golam Md. Muktadir of the University of California, Santa Cruz, Computer Science and Engineering (muktadir@ucsc.edu) describes in good detail, the past-to-present history of *prompt engineering* in their relatively recent September 30th, 2023 paper, \"[*A Brief History of Prompt: Leveraging Language Models*](https://arxiv.org/abs/2310.04438)\" [8].\n",
    "\n",
    "Modern day *prompt engineering* can be described by the following description...\n",
    "\n",
    "\"...***researchers and practitioners [who] have explored various techniques to harness the full potential of language models, leveraging the power of prompts to guide, control, and tailor the output of these sophisticated AI systems.***...\"\n",
    "\n",
    "...where this *__very description__* of *prompt engineering* originates from a generated text borne of *prompt engineering* itself, as given by Researcher Muktadir in their paper's introduction, from these two prompts [8]:\n",
    "\n",
    "\"**Prompt #1**: *You are a scholar in machine learning and language models. I am writing a paper on the history of prompt engineering and generation. Can you give me a timeline for prompt engineering evolution? (We used this timeline to create prompts for each section later)*\"\n",
    "\n",
    "&\n",
    "\n",
    "\"**Prompt #2**: *Write the introduction of this paper. Emphasize that this paper focuses on how language prompts and queries have been used so far.*\"\n",
    "\n",
    "While the techniques of *prompt engineering*, seemingly only recently popularized, or rather, been made the spotlight mainstream subject of discussion of recent, in regard to the wide field of *Natural Language Processing* (***NLP***) - this popularization, of course, significantly due to the rise of [*OpenAI*](https://platform.openai.com/docs/models/)'s [***```ChatGPT```***](https://openai.com/blog/chatgpt) [6], [7] - however, nonetheless are a set of techniques a history long prior to this very novel advent of widely accessible platforms providing services based on *Natural Language Processing* (***NLP***) study & more specifically, through *Large Language Models* (***LLMs***) [8].\n",
    "\n",
    "As section-titled in their paper, Researcher Muktadir describes an early era of *prompt engineering* in \"*Prehistoric Prompting: Pre NN-Era*\", which describes among the most preliminary studies into what we know as *prompt engineering* today - as far back as the 1960s through the 1970s with \"*Early Natural Language Interfaces*\", the 1990s through the 2010s with \"*Advances in Natural Language Processing*\", e.g. *Neural Networks* (***NNs***) and *Machine Learning* (***ML***), to modern-day, featuring the rise of **Multilingual Large Language Models** (***MLLMs***) of those, we will exploring of two such ***MLLMs*** [8].\n",
    "\n",
    "\n",
    "##### Multilingual Large Language Models (Multilingual LLMs, or 'MLLMs')\n",
    "\n",
    "One way we can/will be accessing our two **Multilingual Large Language Models** (***MLLMs***), [*Meta*](https://ai.meta.com/blog/large-language-model-llama-meta-ai/)'s [***```LLaMA 2```***](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) [3], [4] and [*OpenAI*](https://platform.openai.com/docs/models/)'s [***```ChatGPT```***](https://openai.com/blog/chatgpt) **Multilingual Large Language Models** (***MLLMs***) [5], [6], would be through the graphical interface chat platform provided through [*HuggingFace* Chat](https://huggingface.co/chat) [11].\n",
    "\n",
    "##### Meta's Open-Source **LLaMA 2** (*Large Language Model Meta AI 2*) Multilingual Large Language Model (MLLM) [3], [4]\n",
    "The first ***MLLM*** we will be exploring will be the ***__open-source__***, [***```LLaMA 2```***](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) (*Large Language Model Meta AI 2*) [4] ***MLLM***, which was first co-released by [*Meta* and *Microsoft*](https://ai.meta.com/blog/llama-2/), back on June 17th, 2023 [14].\n",
    "\n",
    "This ***MLLM*** was first released & described as ***```LLaMA (Version 1)```*** in the paper, [\"*LLaMA: Open and Efficient Foundation Language Models*\"](https://arxiv.org/abs/2302.13971) [13] - which was first published earlier this year in February 27th, 2023, by the [*Meta AI*](https://ai.meta.com/blog/large-language-model-llama-meta-ai/) team [3] - authored by Researchers Hugo Touvron, and thirteen (13) other authors.\n",
    "\n",
    "This first version of ***```LLaMA```*** was described as \"*a collection of foundation language models ranging from 7B to 65B parameters*\", these models having been trained on \"*trillions of tokens*\", and importantly, claiming that *it is* and *was possible* to \"*train state-of-the-art models using publicly available datasets* **__exclusively__**\" - claiming, that the-then ***```LLaMA-13B```*** model outperformed the ***```GPT-3 (175B)```*** model on \"*most benchmarks*\" [13].\n",
    "\n",
    "The second iteration, ***```LLaMA 2```*** has been released as part of various ***fine-tuned LLMs***, e.g. *```LLaMA 2-Chat```*, *```Code LLaMA 2```*, etc. - as to represent the latest efforts by [*Meta* and *Microsoft*](https://ai.meta.com/blog/llama-2/) to provide their updated advancements of their open-source ***LLM*** - where, in their [**paper**](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/), they claim that the latest ***```LLaMA 2```*** ***LLM(s)*** are capable and *very* comparable in performance to those of other closed-source ***LLMs***, either in regard to a closed-source based set of techniques used to create the ***LLM(s)*** and/or a closed-source service that is provided as *SaaS* (***Software as a Service***), e.g. [*OpenAI*](https://platform.openai.com/docs/models/)'s\n",
    " [***```ChatGPT```***](https://openai.com/blog/chatgpt) [14], [4], [5], [6]. \n",
    "\n",
    "##### OpenAI's Proprietary **ChatGPT** (*Chat Generative Pretrained Transformer*) Multilingual Large Language Model (MLLM) [5]-[7]\n",
    "The most famous ***MLLM*** model as of recent mainstream attention, is the proprietary model [***```ChatGPT```***](https://openai.com/blog/chatgpt) (based on *Generative Pretrained Transformer 3.5 Turbo*, currently on the November 6th, 2023 update-iteration, [***```gpt-3.5-turbo-1106```***](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)) is based off advancements made by [*OpenAI*](https://platform.openai.com/docs/models/) on the closed-source 'release' of [***```GPT-3```***](https://arxiv.org/abs/2005.14165), and of course, follows the progress made from the second predecessor, [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) [6], [7], [5], [15], [10].\n",
    "\n",
    "Recalling from *A3*:\n",
    "In ***```GPT-2```***'s [technical paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), *OpenAI* researchers Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever describes the ***```GPT-2```*** model - a ***__unidirectional__*** model trained on a dataset of *8 million web pages* with **1.5 billion parameters** ([*OpenAI*](https://openai.com/research/better-language-models), [Radford et al.](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) [10].\n",
    "\n",
    "However, if we were to make a quick GPT lineage comparison between ***```gpt-3.5-turbo-1106```*** vs. ***```GPT-2```***, of course, the former, ***```gpt-3.5-turbo-1106```***, has up to ***175 billion parameters*** & the latter, at least the largest-parameter version of ***```GPT-2```***, has still only ***1.5 billion parameters*** - representing an extremely significant increase in parameter-power in ***```gpt-3.5-turbo-1106```*** over its long predecessor ***```GPT-2```*** model.\n",
    "\n",
    "##### Comparison of **LLaMA-2** vs. **ChatGPT** **Multilingual Large Language Models** (***MLLMs***) Architectures & Designs\n",
    "\n",
    "When comparing [**```LLaMA 2```**](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) vs. [**```ChatGPT```**](https://openai.com/blog/chatgpt) ([***```gpt-3.5-turbo-1106```***](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)), the first difference we can note is that, since [**```LLaMA 1```**](https://ai.meta.com/blog/large-language-model-llama-meta-ai/) was released & now with [**```LLaMA 2```**](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/), [**```LLaMA```**](https://arxiv.org/abs/2302.13971) has remained available as a series of ***open-source*** **Multilingual Large Language Models** (***MLLM***), as released & created by *Meta's AI Team* (& [*Microsoft* for version 2](https://ai.meta.com/blog/llama-2/)) - where anyone is able to request & receive access to freely download the model & its fine-tuned derivatives as per the nature open-source vs. the proprietary nature of [**```ChatGPT```**](https://openai.com/blog/chatgpt), which is only offered indirectly, as a **Software-as-a-Service** (***SaaS***) through the free version of *OpenAI*'s main *```ChatGPT```* *web-platform* ([chat.openai.com](https://chat.openai.com/)) or as a paid service through *OpenAI*'s [*API platform*](https://platform.openai.com/docs/models/) [4], [6], [7], [3], [13], [14], [6], [5].\n",
    "\n",
    "From September 1st, 2023, Enterprise AI Strategist Sunil Ramlochan of the *Prompt Engineering Institute* described in the summary of their article, [*How Does Llama-2 Compare to GPT-4/3.5 and Other AI Language Models*](https://promptengineering.org/how-does-llama-2-compare-to-gpt-and-other-ai-language-models/) that [**```LLaMA 2```**](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) \"*competes on accuracy despite lower complexity* [vs. the complexity of GPT-3.5]\" [16] - where such strong accuracy could be through *Meta*'s various improvements from [**```LLaMA 1```**](https://ai.meta.com/blog/large-language-model-llama-meta-ai/) to [**```LLaMA 2```**](https://ai.meta.com/blog/llama-2/), e.g. *Ghost Attention*, which may improve \"*dialogue context tracking*\" [16], [4], [14].\n",
    "\n",
    "As cited in [16], Researcher Waleed Kadous of *anyscale* performed an analysis of our two **Multilingual Large Language Models** (***MLLMs***), in their supportively titled (of the claims made in [16]) article, \"[*Llama 2 is about as factually accurate as GPT-4 for summaries and is 30X cheaper*](https://www.anyscale.com/blog/llama-2-is-about-as-factually-accurate-as-gpt-4-for-summaries-and-is-30x-cheaper?ref=promptengineering.org)\" [17]:\n",
    "\n",
    "![Comparative Analysis of GPT-4, GPT-3.5-Turbo (ChatGPT), vs. LLaMA 2, from [17] - as cited in [16]](GPT35-4-vs-LLaMa-2.png)\n",
    "\n",
    "\n",
    "##### Text Classification Through Prompt Engineering\n",
    "To perform our text-classification task on these, we will __follow these steps__:\n",
    "\n",
    "1. Prepare a balanced test set of a specific, constant chosen-size of test Tweet-instances for each of the eight different languages from our new dataset, [*Unified Multilingual Sentiment Analysis Benchmark*](https://huggingface.co/datasets/cardiffnlp/tweet_sentiment_multilingual) [2]. Specifically, we need to pick either 'positive' or 'negative' classes, ergo, ignoring Tweet-instances with the 'neutral' label.\n",
    "In my implementation, I will be using a test set of \n",
    "\n",
    "2. Using Prompt Engineering, we will have each of the ***Multilingual LLMs*** attempt to predict the sentiment, neither 'positive' nor 'negative', of each of the Tweet-instances in our test set, for each language.\n",
    "\n",
    "##### Results & Analysis\n",
    "We will, of course, be comparing the two ***Multilingual LLMs*** by the metrics such as the *F1-score* or other relevant metrics like from our previous assignments we've explored this lovely Fall term.\n",
    "\n",
    "Specific to this assignment, *A4*, we will be focusing on __answering the following questions__ through our exploration of the two aforementioned ***MLLMs*** - and the results and analysis from our exploration:\n",
    "\n",
    "1. \"*How do the two LLMs perform? Which one is better? Any possible explanation?*\"\n",
    "\n",
    "2. \"*Comparing the results across the six different languages, what do you observe? Any possible explanation?*\"\n",
    "\n",
    "3. \"*What challenges did you face?*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries & Constants Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CS410: Natural Language Processing, Fall 2023 - 11/27/2023\n",
    "##### A4: Multilingual Large Language Models (LLMs), Dan Jang - Initializations: Libraries, Models, Data, & Constants\n",
    "\n",
    "### 0.) Libraries\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import json as jn\n",
    "import pandas\n",
    "import os\n",
    "#import huggingface_hub\n",
    "import numpy as np\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A4 - The \"Multilingual And Normalization Generator from Original Set (M.A.N.G.O.S.)\" Function\n",
    "#### The M.A.N.G.O.S. function was borne out of a desire to remain sane whilst having properly Python-parsed testing instances for each of our eight (8) language-based datasets, heh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multilingual And Normalization Generator from Original Set (M.A.N.G.O.S.) Function is now loading...\n",
      "parsed_arabic has been successfully parsed...\n",
      "parsed_english has been successfully parsed...\n",
      "parsed_french has been successfully parsed...\n",
      "parsed_german has been successfully parsed...\n",
      "parsed_hindi has been successfully parsed...\n",
      "parsed_italian has been successfully parsed...\n",
      "parsed_portuguese has been successfully parsed...\n",
      "parsed_spanish has been successfully parsed...\n",
      "Multilingual And Normalization Generator from Original Set (M.A.N.G.O.S.) Function has successfully parsed all the tweet-datasets!\n"
     ]
    }
   ],
   "source": [
    "##### CS410: Natural Language Processing, Fall 2023 - 11/30/2023\n",
    "##### A4: Multilingual Large Language Models (LLMs), Dan Jang - \"Multilingual And Normalization Generator from Original Set (M.A.N.G.O.S.)\"\n",
    "##### Just a cute name for a function that will parse .jsonl files properly, as for some reason, Visual Studio Code does not have any sorting extensions compatible with .jsonl files directly - at least for the raw formats of our new dataset, hehe.\n",
    "\n",
    "### A4: 1.) Multilingual And Normalization Generator from Original Set (M.A.N.G.O.S.) Function\n",
    "\n",
    "## A4: 1a.) Path & files to pre-processed testing instance-sets\n",
    "datapath = 'data/tweet_sentiment_multilingual/data/processed/processedv2'\n",
    "# tweetsets = [\n",
    "#     'processed_arabic.jsonl',\n",
    "#     'processed_english.jsonl',\n",
    "#     'processed_french.jsonl',\n",
    "#     'processed_german.jsonl',\n",
    "#     'processed_hindi.jsonl',\n",
    "#     'processed_italian.jsonl',\n",
    "#     'processed_portuguese.jsonl',\n",
    "#     'processed_spanish.jsonl'\n",
    "# ]\n",
    "langlist = [\n",
    "    'arabic',\n",
    "    'english',\n",
    "    'french',\n",
    "    'german',\n",
    "    'hindi',\n",
    "    'italian',\n",
    "    'portuguese',\n",
    "    'spanish'\n",
    "]\n",
    "\n",
    "print(\"Multilingual And Normalization Generator from Original Set (M.A.N.G.O.S.) Function is now loading...\")\n",
    "\n",
    "# A4: 1b.) Main data array for holding the mangos (parsed .jsonl files -> Python data arrays)\n",
    "mangos = {lang: [] for lang in langlist}\n",
    "\n",
    "# A4: 1c.) The main M.A.N.G.O. parser loop\n",
    "for lang in langlist:\n",
    "    lepath = os.path.join(datapath, f'processed_{lang}.jsonl')\n",
    "    \n",
    "    with open(lepath, 'r', encoding='utf-8') as mango:\n",
    "        # peel = line; mango = file; e.g., for each line in current processed_lang.jsonl file, etc.\n",
    "        for peel in mango:\n",
    "            tweet = jn.loads(peel)\n",
    "            if tweet[\"label\"] == \"1\":\n",
    "                print(\"uh oh, we got a mango with no taste! (a neutral tweet detected, whoopsie\")\n",
    "            else:\n",
    "                mangos[lang].append({\n",
    "                    'tweet': tweet[\"text\"],\n",
    "                    'actual-sentiment': tweet[\"label\"],\n",
    "                    'chatgpt-sentiment': None,\n",
    "                    'llama2-sentiment': None,\n",
    "                })\n",
    "\n",
    "# A4: 1d.) Box all the freshly peeled mangos into a nice & tidy, new Python file for implementation\n",
    "with open('tweety_mangos.py', 'w', encoding='utf-8') as candiedmangos:\n",
    "    for lang, tweet in mangos.items():\n",
    "        instancename = f'parsed_{lang}'\n",
    "        #outfile.write(f'{list_name} = {json.dumps(data, ensure_ascii=False, indent=4)}\\n\\n')\n",
    "        candiedmangos.write(f'{instancename} = {jn.dumps(tweet, ensure_ascii=False, indent=4)}\\n\\n')\n",
    "        print(f'{instancename} has been successfully parsed...')\n",
    "        \n",
    "print(\"Multilingual And Normalization Generator from Original Set (M.A.N.G.O.S.) Function has successfully parsed all the tweet-datasets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Engineering, Section I: ChatGPT Examples\n",
    "\n",
    "To perform the sentiment analysis, a series of similar, *engineered prompts* like shown below were used to get the Tweet-testing instance's text data sentiment analyzed by *```ChatGPT```*.\n",
    "\n",
    "Specifically, the prompt below was used to complete parts of each list of our parsed language testing-datasets, where the resultant output from prompts engineered were designed to produce a specifically formatted output, where we wanted *```ChatGPT```* to both perform sentiment analysis, and additionally, fill-in the sentiment rating within our parsed Python list format.\n",
    "\n",
    "Since *```ChatGPT```* does seem to have a tangible - but, at least a reasonably wide - *per-request* generation-output limit on its output messages (& where you have to click **```Continue Generating```**) - for each parsed language data list, e.g. ```parsed_arabic```, multiple prompts were used & resultant generated outputs combined cumulatively - **until**, *and* **for all**, Tweet-instances in each of our eight (8) language-based datasets were successully analyzed by *```ChatGPT```* & values for sentiment prediction rating were given *```ChatGPT```*.\n",
    "\n",
    "The *engineered prompt* resembled the following:\n",
    "\n",
    "**Prompt #1** - Me: \"...\n",
    "\n",
    "    I need you to perform sentiment prediction as a text classification model.\n",
    "\n",
    "    Fill in the \"chatgpt_sentiment\" values for each item in parsed_arabic, based your own analysis of the  sentiment of the text-data contained within each item's \"tweet\" key.\n",
    "    Specifically, fill in either a value of \"0\" for negative sentiment or a value of \"2\" for positive sentiment.\n",
    "\n",
    "    Here is the data for parsed_arabic:\n",
    "    ```py\n",
    "    parsed_arabic = [\n",
    "        {\n",
    "            \"tweet\": \"ŸÜŸàÿßŸÑ ÿßŸÑÿ≤ÿ∫ÿ®Ÿä (ÿßŸÑÿ¥ÿßÿ® ÿÆÿßŸÑÿØ ŸÑŸäÿ≥ ÿπÿßŸÑŸÖŸä) ŸáŸáŸáŸáŸáŸáŸáŸá ÿ£ÿ™ŸÅÿ±ÿ¨Ÿä ÿπŸÑŸâ Ÿáÿß ÿßŸÑŸÅŸäÿØŸäŸà Ÿäÿß ŸÖÿ®ÿ™ÿØÿ¶ÿ© http v√≠a @user\",\n",
    "            \"chatgpt-sentiment\": None,\n",
    "            \"llama2-sentiment\": None\n",
    "        },\n",
    "        {\n",
    "            \"tweet\": \"ŸÜŸàÿßŸÑ ÿßŸÑÿ≤ÿ∫ÿ®Ÿä ŸÑÿ∑ŸäŸÅŸá ÿßŸÑŸÅŸÜÿßŸÜŸá ÿßŸÑŸàÿ≠ŸäÿØŸá ÿßŸÑŸÑŸä ŸÉŸÑ ÿßŸÑŸÅŸäÿØŸäŸà ŸÉŸÑŸäÿ®ÿßÿ™ ÿ™ÿ®ÿπŸáÿß ŸÖÿßÿ™ÿ≥ÿ®ÿ® ÿ™ŸÑŸàÿ´ ÿ®ÿµÿ±Ÿä ŸàŸÑÿß ÿ≥ŸÖÿπŸä ŸÑŸà ÿµŸàÿ™Ÿáÿß ÿßŸÇŸÑ ŸÖŸÜ ÿπÿßÿØŸä\",\n",
    "            \"chatgpt-sentiment\": None,\n",
    "            \"llama2-sentiment\": None\n",
    "        },\n",
    "        {\n",
    "            \"tweet\": \"ŸÑŸÖÿß ŸÇÿßŸÑÿ™ ŸÜŸàÿßŸÑ ÿßŸÑÿ≤ÿ∫ÿ®Ÿä ŸÑÿßÿ®ŸÇŸÑŸáÿß ŸáÿßŸÑŸÑŸÇÿ® ŸÅÿ±ÿ≠Ÿàÿß ŸÅÿßŸÜÿ≤Ÿáÿß üòÇüòÇüòÇŸÉÿßŸÜ ŸÑÿßÿ≤ŸÖ ŸäÿßÿÆÿØŸàŸáÿß ÿßŸáÿßŸÜÿ© ŸÖÿ¥ ÿ´ŸÜÿßÿ° http\",\n",
    "            \"chatgpt-sentiment\": None,\n",
    "            \"llama2-sentiment\": None\n",
    "        },\n",
    "        #... so on yes, until tweet/item[23]\n",
    "        {\n",
    "            \"tweet\": \"ŸÜŸàÿßŸÑ ÿßŸÑÿ≤ÿ∫ÿ®Ÿä ŸÖŸÜ ÿßŸÑÿ≠ÿ¨ÿßÿ™ ÿßŸÑŸä Ÿáÿ™ŸÅÿ∂ŸÑ ÿ¨ŸÖŸäŸÑŸá ÿπŸÑŸä ÿ∑ŸàŸÑ...üòçüòçüòçüòç http\",\n",
    "            \"chatgpt-sentiment\": None,\n",
    "            \"llama2-sentiment\": None\n",
    "        }\n",
    "    ]\n",
    "    ```\n",
    "...\"\n",
    "\n",
    "**Response** - [*```ChatGPT```*](https://chat.openai.com): \"...\n",
    "\n",
    "    Sure, I can help you with sentiment prediction. Please note that sentiment analysis can be subjective, and the following predictions are based on my interpretation of the given Arabic text.\n",
    "\n",
    "    parsed_arabic = [\n",
    "        {\n",
    "            \"tweet\": \"ŸÜŸàÿßŸÑ ÿßŸÑÿ≤ÿ∫ÿ®Ÿä (ÿßŸÑÿ¥ÿßÿ® ÿÆÿßŸÑÿØ ŸÑŸäÿ≥ ÿπÿßŸÑŸÖŸä) ŸáŸáŸáŸáŸáŸáŸáŸá ÿ£ÿ™ŸÅÿ±ÿ¨Ÿä ÿπŸÑŸâ Ÿáÿß ÿßŸÑŸÅŸäÿØŸäŸà Ÿäÿß ŸÖÿ®ÿ™ÿØÿ¶ÿ© http v√≠a @user\",\n",
    "            \"chatgpt-sentiment\": 2,\n",
    "            \"llama2-sentiment\": None\n",
    "        },\n",
    "        {\n",
    "            \"tweet\": \"ŸÜŸàÿßŸÑ ÿßŸÑÿ≤ÿ∫ÿ®Ÿä ŸÑÿ∑ŸäŸÅŸá ÿßŸÑŸÅŸÜÿßŸÜŸá ÿßŸÑŸàÿ≠ŸäÿØŸá ÿßŸÑŸÑŸä ŸÉŸÑ ÿßŸÑŸÅŸäÿØŸäŸà ŸÉŸÑŸäÿ®ÿßÿ™ ÿ™ÿ®ÿπŸáÿß ŸÖÿßÿ™ÿ≥ÿ®ÿ® ÿ™ŸÑŸàÿ´ ÿ®ÿµÿ±Ÿä ŸàŸÑÿß ÿ≥ŸÖÿπŸä ŸÑŸà ÿµŸàÿ™Ÿáÿß ÿßŸÇŸÑ ŸÖŸÜ ÿπÿßÿØŸä\",\n",
    "            \"chatgpt-sentiment\": 2,\n",
    "            \"llama2-sentiment\": None\n",
    "        },\n",
    "        {\n",
    "            \"tweet\": \"ŸÑŸÖÿß ŸÇÿßŸÑÿ™ ŸÜŸàÿßŸÑ ÿßŸÑÿ≤ÿ∫ÿ®Ÿä ŸÑÿßÿ®ŸÇŸÑŸáÿß ŸáÿßŸÑŸÑŸÇÿ® ŸÅÿ±ÿ≠Ÿàÿß ŸÅÿßŸÜÿ≤Ÿáÿß üòÇüòÇüòÇŸÉÿßŸÜ ŸÑÿßÿ≤ŸÖ ŸäÿßÿÆÿØŸàŸáÿß ÿßŸáÿßŸÜÿ© ŸÖÿ¥ ÿ´ŸÜÿßÿ° http\",\n",
    "            \"chatgpt-sentiment\": 2,\n",
    "            \"llama2-sentiment\": None\n",
    "        },\n",
    "        {\n",
    "            \"tweet\": \"@user ÿ™ÿ∞ŸÉÿ±ŸÜŸä ÿ®ÿ£ÿ∫ŸÜŸäÿ© ŸÜŸàÿßŸÑ ÿßŸÑÿ≤ÿ∫ÿ®Ÿä \\\"ÿπŸäŸÜŸäŸÉ ŸÉÿØÿßÿ®ŸäŸÜ\\\"\",\n",
    "            \"chatgpt-sentiment\": 2,\n",
    "            \"llama2-sentiment\": None\n",
    "        },\n",
    "        {\n",
    "            \"tweet\": \"ÿ®ŸÑÿß ÿ™ÿ¥ŸÅŸäÿ±- ÿ£ŸÖŸÑ ÿ≠ŸÖÿßÿØŸä ÿ®ÿ™ŸÜÿ™ŸÇÿØ ÿßŸÑŸÜÿ¨ŸÖÿ© ŸÜŸàÿßŸÑ ÿßŸÑÿ≤ÿ∫ÿ®Ÿä:\\\" ŸÜÿ™Ÿä... http\",\n",
    "            \"chatgpt-sentiment\": 0,\n",
    "            \"llama2-sentiment\": None\n",
    "        },\n",
    "        {\n",
    "            \"tweet\": \"ŸÅŸÜÿßŸÜÿ© ŸÑÿ®ŸÜÿßŸÜŸäÿ© ŸÉÿ®Ÿäÿ±ÿ©  ÿµŸàÿ™Ÿáÿß ÿ•ÿ≥ÿ∑Ÿàÿ±Ÿä ÿü! #ŸÖÿßÿ¨ÿØÿ©_ÿßŸÑÿ±ŸàŸÖŸä   @user #ŸÜÿ¨ŸàŸâ_ŸÉÿ±ŸÖ  @user #ŸÜŸàÿßŸÑ_ÿßŸÑÿ≤ÿ∫ÿ®Ÿä  @user #ÿ¨ŸàŸÑŸäÿß_ÿ®ÿ∑ÿ±ÿ≥\",\n",
    "            \"chatgpt-sentiment\": 2,\n",
    "            \"llama2-sentiment\": None\n",
    "        },\n",
    "        {\n",
    "            \"tweet\": \"#ŸÑÿ®ŸÜÿßŸÜŸä_ŸäŸÇŸäŸÖ_ÿØŸàÿ±Ÿá_ŸÖŸÉŸäÿßÿ¨_ÿ®ÿßŸÑÿ±Ÿäÿßÿ∂ŸÖÿßŸÜŸÉÿ®ŸÜŸâ ÿ∫Ÿäÿ± ÿ®ÿ±ÿßÿ∑ŸÖ ŸÜŸàÿßŸÑ ÿßŸÑÿ≤ÿ∫ÿ®ŸäŸàŸÜŸáŸàÿØ ÿßŸÑŸäÿ≥ÿßŸàŸÖÿ§ÿÆÿ±ÿ© ŸáŸäŸÅÿßÿ° ŸàŸáÿ®ŸäŸÇŸÑŸÜÿßŸÑŸÉŸÖ ŸÉŸÑŸá ŸÜŸÅÿÆ ŸàŸáŸàÿßÿ®ÿ≥üòâüòâ Ÿäÿßÿ≤ŸäŸÜ ŸÜŸÅÿÆŸáŸÖ ŸäŸÑÿ∑ÿ¥\",\n",
    "            \"chatgpt-sentiment\": 0,\n",
    "            \"llama2-sentiment\": None\n",
    "        },\n",
    "        #... so on yes, until tweet/item[23], or usually, a few of the last tweets/items would be incomplete due to the free-version rate-limiting, ergo, I'd then click continue generating\n",
    "...\"\n",
    "\n",
    "*Although*, it might be more long-term beneficial to simply create a set of functions, such that just generates a 24-element, one-line JSON line that contains all twenty-four (24) Tweet-instances per given language out of our eight (8) language testing-sets, I used the above method for the sake being able to more readily/visually verify that *```ChatGPT```* (or for *```LLaMA 2```*) is, hopefully not, generating mismatched sentiments or nonsensical sentiments.\n",
    "\n",
    "E.g. if some very obviously polarized Tweet or multiple super obviously one-sided Tweet-items, where the sentiment should be super on-the-nose, e.g. ```\"tweet\": \"Trying to have a conversation with my dad about vegetarianism is the most pointless infuriating thing ever #caveman \"```, being set to \"2\" as somehow a positive sentiment; if such occurs, I'd be able to hopefully spot out errors.\n",
    "\n",
    "However, besides that aspect, the aforementioned is a thought of possible long-term streamlining the process in regards to this specific task of Tweet sentiment analysis through *prompt engineering*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Engineering, Section II: LLaMA 2 Examples\n",
    "\n",
    "*```LLaMA 2```*, interestingly, was actually easier to work with & get more in-depth results from - yet, this clear advantage was massively overshadowed by an unfortunate aspect that the specific Chat-based instance of *```LLaMA 2```* to be used in this assignment kept on freezing, becoming unresponsive even with only a moderate to small input given-data in various kinds of engineered prompts, and has been completely down, service-wise, for the most of my attempts to use / check if it was back up, which heavily delayed my progress for my *A4* assignment - although, still, I apologize for my own delays nonetheless.\n",
    "\n",
    "As a compromise, due to [*```HuggingChat```*](https://huggingface.co/chat/) being unavailable for multiple hours at the time, I instead explored a specific version of *```LLaMA-2```*, the [*```LLaMA-2-7b-chat```*](https://huggingface.co/meta-llama/Llama-2-7b-chat) model [19], which, was also hosted on & one of [*````HuggingFace```*](https://huggingface.co/spaces/huggingface-projects/llama-2-7b-chat)'s [```huggingface-projects```](https://huggingface.co/huggingface-projects) [20], but of course, the [*```LLaMA-2-7b-chat```*] specific version, quantitatively has *ten-fold* less parameters than the model used by [*```HuggingChat```*](https://huggingface.co/chat), which is/was [*```meta-llama/Llama-2-70b-chat-hf```*](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf) [21] had offered & the lack of file-upload functionality is slightly a bummer.\n",
    "\n",
    "In that thought, initially, my *engineered prompts*  for *```LLaMA 2```* were much grander & had more input-text per prompt, e.g.:\n",
    "\n",
    "**Prompt #2a** - Me: \"...\n",
    "\n",
    "    Could you please perform sentiment analysis for each item in the following Python language testing data-lists, where there are eight different language-data lists, each containing 24 elements.\n",
    "\n",
    "    In each of these elements, there is a \"tweet\" key with a value correlating to a random Tweet message in the language of that given language testing data-lists. Using the text data of \"tweet\" in each of the elements, provide & set a sentiment rating, for each element based on its \"tweet\" text-data, in the \"llama2-sentiment\" key.\n",
    "\n",
    "    Specifically, provide & set the \"llama2-sentiment\" key-value to either a \"0\" for negative sentiment or a \"2\" for positive sentiment:\n",
    "    \n",
    "    ```py # pretend this represents an uploaded file of the parsed Python data lists, full_tweety_mangos.py\n",
    "    parsed_arabic = [\n",
    "        ...\n",
    "    ]\n",
    "\n",
    "    parsed_english = [\n",
    "        ...\n",
    "    ]\n",
    "\n",
    "    #... etc.\n",
    "    ```\n",
    "...\"\n",
    "\n",
    "However, using the alternative, [*```LLaMA-2-7b-chat```*](https://huggingface.co/meta-llama/Llama-2-7b-chat) model-chat [19], my *prompt engineering* trended back in resembling to the *piece-by-piece* sentiment analysis gathering strategy used for *```ChatGPT```* - however, I still used the same prompt as above, just that I was not able to do more than one whole language-testing instance-set of 24 Tweets at a time:\n",
    "\n",
    "**Prompt #2b** - Me: \"...\n",
    "\n",
    "    Could you please perform sentiment analysis for each item in the following Python language testing data-lists, where there are eight different language-data lists, each containing 24 elements.\n",
    "\n",
    "    In each of these elements, there is a \"tweet\" key with a value correlating to a random Tweet message in the language of that given language testing data-lists. Using the text data of \"tweet\" in each of the elements, provide & set a sentiment rating, for each element based on its \"tweet\" text-data, in the \"llama2-sentiment\" key.\n",
    "\n",
    "    Specifically, provide & set the \"llama2-sentiment\" key-value to either a \"0\" for negative sentiment or a \"2\" for positive sentiment:\n",
    "    \n",
    "    ```py\n",
    "    parsed_arabic = [\n",
    "        ...\n",
    "    ]\n",
    "\n",
    "![Engineered Prompt #2b for LLaMA-2 7B Chat [20]](PromptEngSectionII-Prompt2b.png)\n",
    "\n",
    "...\"\n",
    "\n",
    "**Response #2b** - *```LLaMA-2-7b-chat```*: \"...\n",
    "\n",
    "    Sure, I can assist you in assigning sentiment scores to each tweet in the provided list using the llama2-sentiment key. Here are the results:\n",
    "![Response from LLaMA-2 7B Chat [20]](PromptEngSectionII-Response2b.png)\n",
    "\n",
    "...\"\n",
    "\n",
    "*However*, I found another alternative for accessing [*```meta-llama/Llama-2-70b-chat-hf```*](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf) [21] whilst [*```HuggingChat```*](https://huggingface.co/chat/) [11] remained down (at 503 service error code) - which was through [llama2.ai](https://www.llama2.ai/) [22] which luckily - supposedly - also runs the ```70B```-illion parameter version of *```LLaMA 2```* - ...but that also broke during usage.\n",
    "\n",
    "So, I just was stuck with using [*```LLaMA-2-7b-chat```*](https://huggingface.co/meta-llama/Llama-2-7b-chat) model-chat [19]. Just in case, I am downloading the LLaMA 2 70B model directly in the case that even this ```7B``` parameter model-chat goes down as well.\n",
    "\n",
    "With the [*```LLaMA-2-7b-chat```*](https://huggingface.co/meta-llama/Llama-2-7b-chat) model-chat [19], I engineered a pretty consistent-output prompt that seemed to be effective for the piece-by-piece approach as aforementioned:\n",
    "\n",
    "**Prompt #3a** - Me: \"...\n",
    "\n",
    "    Please fill in the \"llama2-sentiment\" key in the following elements, based on your sentiment analysis of each element's text-data from the \"tweet\" element - specifically, fill in with either a value of \"0\" for a negative sentiment or a value of \"2\" for a positive sentiment:\n",
    "    ```py\n",
    "\n",
    "    ```\n",
    "    \n",
    "...\"\n",
    "\n",
    "**Example Response #3** - [*```LLaMA-2-7b-chat```*](https://huggingface.co/meta-llama/Llama-2-7b-chat): \"...\n",
    "\n",
    "![Example Response from LLaMA-2 7B Chat [20]](PromptEngSectionII-ExampleResponse3.png)\n",
    "\n",
    "\n",
    "Sometimes, [*```LLaMA-2-7b-chat```*](https://huggingface.co/meta-llama/Llama-2-7b-chat) seemed to be struggling to choose between a positive or negative sentiment, as it had given 'neutral' scores at times, which is not valid. Ergo, I tweaked my original **Prompt #3a** as follows:\n",
    "\n",
    "**Prompt #3b** - Me: \"...\n",
    "\n",
    "    Please fill in the \"llama2-sentiment\" key in the following elements, based on your sentiment analysis of each element's text-data from the \"tweet\" element - specifically, you must only fill in either a value of \"0\" for a negative sentiment or a value of \"2\" for a positive sentiment - there cannot be any neutral sentiment scores given:\n",
    "    ```py\n",
    "\n",
    "    ```\n",
    "    \n",
    "...\"\n",
    "\n",
    "As an additional *prompt engineering* technique, I used a 'cleaned' version of my language testing-instance sets, such that there are no pre-existing sentiment scores from *```ChatGPT```*, *```LLaMA 2```*, or the actual sentiment scores that may possibly influence the sentiment analysis outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ DELETE AFTER\n",
    "\n",
    "Please fill in the \"llama2-sentiment\" key in the following elements, based on your sentiment analysis of each element's text-data from the \"tweet\" element - specifically, fill in with either a value of \"0\" for a negative sentiment or a value of \"2\" for a positive sentiment:\n",
    "```py\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Please fill in the \"llama2-sentiment\" key in the following elements, based on your sentiment analysis of each element's text-data from the \"tweet\" element - specifically, you must only fill in either a value of \"0\" for a negative sentiment or a value of \"2\" for a positive sentiment - there cannot be any neutral sentiment scores given:\n",
    "```py\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatted JSON->Python Data Loading from Post-Classified Sentiment Ratings from ChatGPT & LLaMA 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic = [\n",
    "    {\n",
    "        'prompt': \"\\u0646\\u0648\\u0627\\u0644 \\u0627\\u0644\\u0632\\u063a\\u0628\\u064a (\\u0627\\u0644\\u0634\\u0627\\u0628 \\u062e\\u0627\\u0644\\u062f \\u0644\\u064a\\u0633 \\u0639\\u0627\\u0644\\u0645\\u064a) \\u0647\\u0647\\u0647\\u0647\\u0647\\u0647\\u0647 \\u0623\\u062a\\u0641\\u0631\\u062c\\u064a \\u0639\\u0644\\u0649 \\u0647\\u0627 \\u0627\\u0644\\u0641\\u064a\\u062f\\u064a\\u0648 \\u064a\\u0627 \\u0645\\u0628\\u062a\\u062f\\u0626\\u0629 http v\\u00eda @user\",\n",
    "        'actual_rating': \"0\",\n",
    "        'predicted_rating_chatgpt': 0,  # My predicted sentiment rating (0 = negative)\n",
    "        'predicted_rating_llama': None,\n",
    "    },\n",
    "    {\n",
    "        'prompt': \"\\u0646\\u0648\\u0627\\u0644 \\u0627\\u0644\\u0632\\u063a\\u0628\\u064a \\u0644\\u0637\\u064a\\u0641\\u0647 \\u0627\\u0644\\u0641\\u0646\\u0627\\u0646\\u0647 \\u0627\\u0644\\u0648\\u062d\\u064a\\u062f\\u0647 \\u0627\\u0644\\u0644\\u064a \\u0643\\u0644 \\u0627\\u0644\\u0641\\u064a\\u062f\\u064a\\u0648 \\u0643\\u0644\\u064a\\u0628\\u0627\\u062a \\u062a\\u0628\\u0639\\u0647\\u0627 \\u0645\\u0627\\u062a\\u0633\\u0628\\u0628 \\u062a\\u0644\\u0648\\u062b \\u0628\\u0635\\u0631\\u064a \\u0648\\u0644\\u0627 \\u0633\\u0645\\u0639\\u064a \\u0644\\u0648 \\u0635\\u0648\\u062a\\u0647\\u0627 \\u0627\\u0642\\u0644 \\u0645\\u0646 \\u0639\\u0627\\u062f\\u064a\",\n",
    "        'actual_rating': \"2\",\n",
    "        'predicted_rating_chatgpt': 2,  # My predicted sentiment rating (2 = positive)\n",
    "        'predicted_rating_llama': None,\n",
    "    },\n",
    "    {\n",
    "        'prompt': \"\\u0644\\u0645\\u0627 \\u0642\\u0627\\u0644\\u062a \\u0646\\u0648\\u0627\\u0644 \\u0627\\u0644\\u0632\\u063a\\u0628\\u064a \\u0644\\u0627\\u0628\\u0642\\u0644\\u0647\\u0627 \\u0647\\u0627\\u0644\\u0644\\u0642\\u0628 \\u0641\\u0631\\u062d\\u0648\\u0627 \\u0641\\u0627\\u0646\\u0632\\u0647\\u0627 \\ud83d\\ude02\\ud83d\\ude02\\ud83d\\ude02\\u0643\\u0627\\u0646 \\u0644\\u0627\\u0632\\u0645 \\u064a\\u0627\\u062e\\u062f\\u0648\\u0647\\u0627 \\u0627\\u0647\\u0627\\u0646\\u0629 \\u0645\\u0634 \\u062b\\u0646\\u0627\\u0621 http\",\n",
    "        'actual_rating': \"0\",\n",
    "        'predicted_rating_chatgpt': 0,  # My predicted sentiment rating (0 = negative)\n",
    "        'predicted_rating_llama': None,\n",
    "    },\n",
    "    {\n",
    "        'prompt': \"@user \\u062a\\u0630\\u0643\\u0631\\u0646\\u064a \\u0628\\u0623\\u063a\\u0646\\u064a\\u0629 \\u0646\\u0648\\u0627\\u0644 \\u0627\\u0644\\u0632\\u063a\\u0628\\u064a \\\"\\u0639\\u064a\\u0646\\u064a\\u0643 \\u0643\\u062f\\u0627\\u0628\\u064a\\u0646\\\"\",\n",
    "        'actual_rating': \"2\",\n",
    "        'predicted_rating_chatgpt': 2,  # My predicted sentiment rating (2 = positive)\n",
    "        'predicted_rating_llama': None,\n",
    "    },\n",
    "    {\n",
    "        'prompt': \"\\u0628\\u0644\\u0627 \\u062a\\u0634\\u0641\\u064a\\u0631- \\u0623\\u0645\\u0644 \\u062d\\u0645\\u0627\\u062f\\u064a \\u0628\\u062a\\u0646\\u062a\\u0642\\u062f \\u0627\\u0644\\u0646\\u062c\\u0645\\u0629 \\u0646\\u0648\\u0627\\u0644 \\u0627\\u0644\\u0632\\u063a\\u0628\\u064a:\\\" \\u0646\\u062a\\u064a... http\",\n",
    "        'actual_rating': \"0\",\n",
    "        'predicted_rating_chatgpt': 0,  # My predicted sentiment rating (0 = negative)\n",
    "        'predicted_rating_llama': None,\n",
    "    },\n",
    "    {\n",
    "        'prompt': \"\\u0641\\u0646\\u0627\\u0646\\u0629 \\u0644\\u0628\\u0646\\u0627\\u0646\\u064a\\u0629 \\u0643\\u0628\\u064a\\u0631\\u0629  \\u0635\\u0648\\u062a\\u0647\\u0627 \\u0625\\u0633\\u0637\\u0648\\u0631\\u064a \\u061f! #\\u0645\\u0627\\u062c\\u062f\\u0629_\\u0627\\u0644\\u0631\\u0648\\u0645\\u064a   @user #\\u0646\\u062c\\u0648\\u0649_\\u0643\\u0631\\u0645  @user #\\u0646\\u0648\\u0627\\u0644_\\u0627\\u0644\\u0632\\u063a\\u0628\\u064a  @user #\\u062c\\u0648\\u0644\\u064a\\u0627_\\u0628\\u0637\\u0631\\u0633\",\n",
    "        'actual_rating': \"2\",\n",
    "        'predicted_rating_chatgpt': 2,  # My predicted sentiment rating (2 = positive)\n",
    "        'predicted_rating_llama': None,\n",
    "    },\n",
    "    {\n",
    "        'prompt': \"#\\u0644\\u0628\\u0646\\u0627\\u0646\\u064a_\\u064a\\u\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A4 - Main Implementation: Metrics & Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome, this is the main program for A3: Pretrained Language Models.\n",
      "Written by Dan J. for CS410: Natural Language Processing, Fall 2023.\n",
      "\n",
      "We will use two (2) pretrained language models (PLM), BERT & GPT-2.\n",
      "...to create a text-classifier to guess negative or positive sentimentiality based on various text-reviews of products.\n",
      "\n",
      "Loading the training & testing datasets...\n",
      "Successfully loaded the training & testing datasets!\n",
      "\n",
      "Is a GPU available: False\n",
      "Tokenizing the training & testing datasets for BERT...\n",
      "BERT Tokenization has been applied to its training & testing datasets!\n",
      "Tokenizing the training & testing datasets for GPT-2...\n",
      "GPT-2's tokenization has been applied to its training & testing datasets!\n",
      "-----\n",
      "\n",
      "Running text-classification model le training & testing datasets (with pretrained language models, BERT & GPT-2)...\n",
      "Running Logistic Regression algorithm, version A.) BERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tools\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..The data processing from our first PLM, BERT, is done!\n",
      "Running Logistic Regression algorithm, version B.) GPT-2...\n",
      "..The data processing from our second PLM, GPT-2, is done!\n",
      "...All Done!\n",
      "-----\n",
      "\n",
      "Here are le results [Logistic Regression, with comparative results between two (2) PLMs, BERT & GPT-2]...\n",
      "\n",
      "Logistic Regression Algorithm, Version A: BERT Pretrained Language Model-based Text-Classification Performance, Metrics, & Results:\n",
      "...Accuracy was found to be,  91.55 %,\n",
      "...F1 Score was found to be:  0.9158785465405676 ,\n",
      "...with a Confusion Matrix: \n",
      " [[911  89]\n",
      " [ 80 920]] ,\n",
      "...& lastly, the classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.92      1000\n",
      "           1       0.91      0.92      0.92      1000\n",
      "\n",
      "    accuracy                           0.92      2000\n",
      "   macro avg       0.92      0.92      0.92      2000\n",
      "weighted avg       0.92      0.92      0.92      2000\n",
      "\n",
      "-----\n",
      "\n",
      "Logistic Regression Algorithm, Version B: GPT-2 Pretrained Language Model-based Text-Classification Performance, Metrics, & Results:\n",
      "...Accuracy was found to be,  87.4 %,\n",
      "...F1 Score was found to be:  0.872598584428716 ,\n",
      "...with a Confusion Matrix: \n",
      " [[885 115]\n",
      " [137 863]] ,\n",
      "...& lastly, the classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.89      0.88      1000\n",
      "           1       0.88      0.86      0.87      1000\n",
      "\n",
      "    accuracy                           0.87      2000\n",
      "   macro avg       0.87      0.87      0.87      2000\n",
      "weighted avg       0.87      0.87      0.87      2000\n",
      "\n",
      "-----\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tools\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "##### CS410: Natural Language Processing, Fall 2023 - 11/27/2023\n",
    "##### A4: Multilingual Large Language Models (LLMs)\n",
    "#### Objective: Exploring Natural Language Processing (NLP),\n",
    "#### for a text classification task, predicting whether a piece of text is \"positive\" or \"negative.\"\n",
    "#### ...focusing on two (2) Multilingual Large Language Models (MLLMs),\n",
    "#### ...Meta's LLaMA 2 & OpenAI's ChatGPT (gpt-3.5-turbo-1106),\n",
    "#### ...and using prompt engineering to perform the classification task on eight (8) language-based, Tweet-testing-datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *A4: Multilingual Large Language Models (MLLMs)*, Results and Analysis: Meta's LLaMA-2 vs. OpenAI's ChatGPT!\n",
    "\n",
    "#### Initial Data Results, Metrics, & Analysis\n",
    "\n",
    "Using the Logistic Regression Algorithm like previously for *A2*, this algorithm was used to implement our text-classification model, for the text-classification task in both *Version A* & *B*.\n",
    "\n",
    "As shown below, *Version A* featured [*Google AI*](https://blog.research.google/2018/11/open-sourcing-bert-state-of-art-pre.html)'s [***```BERT```***](https://arxiv.org/abs/1810.04805) model & *Version B* featured [*OpenAI*](https://openai.com/research/better-language-models)'s [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) model - where we saw the following results:\n",
    "\n",
    "##### Version A - ***```BERT```*** Pretrained Language Model (PLM) Results:\n",
    "\n",
    "    Accuracy:  ~91.6%\n",
    "    F1 Score:  0.9158785465405676\n",
    "\n",
    "    Confusion Matrix:\n",
    "    [[911  89]\n",
    "    [ 80 920]]\n",
    "\n",
    "    Classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.92      0.91      0.92      1000\n",
    "            1       0.91      0.92      0.92      1000\n",
    "\n",
    "    accuracy                            0.92      2000\n",
    "    macro avg       0.92      0.92      0.92      2000\n",
    "    weighted avg    0.92      0.92      0.92      2000\n",
    "\n",
    "##### Version B - ***```GPT-2```*** Pretrained Language Model (PLM) Results:\n",
    "    Accuracy:  87.4%\n",
    "    F1 Score:  0.872598584428716\n",
    "    \n",
    "    Confusion Matrix: \n",
    "    [[885 115]\n",
    "    [137 863]]\n",
    "\n",
    "    Classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.87      0.89      0.88      1000\n",
    "            1       0.88      0.86      0.87      1000\n",
    "\n",
    "    accuracy                            0.87      2000\n",
    "    macro avg       0.87      0.87      0.87      2000\n",
    "    weighted avg    0.87      0.87      0.87      2000\n",
    "\n",
    "#### Pretrained Language Model (PLM) Comparative Analysis & Discussion\n",
    "\n",
    "From the results above, we see that both [*Google AI*](https://blog.research.google/2018/11/open-sourcing-bert-state-of-art-pre.html)'s [***```BERT```***](https://arxiv.org/abs/1810.04805) & [*OpenAI*](https://openai.com/research/better-language-models)'s [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) ***PLMs*** faired very well in accuracy, of ~91.6% & 87.4% respectively.\n",
    "\n",
    "Between these two ***PLMs***, [***```BERT```***](https://arxiv.org/abs/1810.04805) & [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), we can see that from our performance results that there is/was a slight, low *difference of accuracy*, where [***```BERT```***](https://arxiv.org/abs/1810.04805) has a ***higher*** **accuracy** of *___~4.2%___* in *comparison to [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)*. \n",
    "\n",
    "Also, by looking at the respective Confusion Matrices for our two ***PLMs***, we can see that [***```BERT```***](https://arxiv.org/abs/1810.04805), of course, was able to classify sentiment more accurately than [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), but we also can observe that [***```BERT```***](https://arxiv.org/abs/1810.04805) seems to be *a little bit* more efficient in detecting *negative* sentimentality vs. [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) lean towards *positive* sentimentality - however, [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)'s decreased accuracy in detecting *negative* sentiment seems to be more significant in magnitude compared to the aforementioned, *negative* over *positive* sentimentality bias, of [***```BERT```***](https://arxiv.org/abs/1810.04805).\n",
    "\n",
    "Interestingly, a possible scenario where this supposed sentimentality bias in our text-classification task may present itself as to yield a different result, at least for accuracy & the F1 score, may lie in how the positive & negative sentiment composition during the processing of our training & testing dataset, particularly, at the time of randomized splitting.\n",
    "Specifically, if we had somehow chosen a random seed in our splitting code, that supposedly had much more positive or negative sentimentality in the composition of the reviews, we could have seen variations from our initial performance & results as described above.\n",
    "\n",
    "Ergo, between [*Google AI*](https://blog.research.google/2018/11/open-sourcing-bert-state-of-art-pre.html)'s [***```BERT```***](https://arxiv.org/abs/1810.04805) & [*OpenAI*](https://openai.com/research/better-language-models)'s [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) ***PLMs***, [***```BERT```***](https://arxiv.org/abs/1810.04805) seems to be a clearly more suitable model, at least for the text-classification task & model with our assignment's training & dataset, \"[*Multilingual Amazon Reviews Corpus*](https://arxiv.org/abs/2010.02573).\"\n",
    "\n",
    "\n",
    "#### Analysis of Current & Previous Text-Classification Models & Performance Results from *A1* & *A2*\n",
    "\n",
    "In comparison to the performance seen from *A2: ```Word2Vec``` & ```GloVe``` Embeddings*, both [*Google AI*](https://blog.research.google/2018/11/open-sourcing-bert-state-of-art-pre.html)'s [***```BERT```***](https://arxiv.org/abs/1810.04805) & [*OpenAI*](https://openai.com/research/better-language-models)'s [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) ***PLMs*** outperformed the accuracies & F1 scores from either ```Word2Vec``` or ```GloVe``` pretrained embeddings - where we recall the *A2* results as follows:\n",
    "\n",
    "*A2*: Version A, [```Word2Vec```](https://arxiv.org/abs/1301.3781) Model:\n",
    "\n",
    "    Accuracy:  ~86.3%,\n",
    "    F1 Score:  0.86105476673428\n",
    "\n",
    "    Confusion Matrix: \n",
    "    [[1754  246]\n",
    "    [ 302 1698]]\n",
    "\n",
    "    Classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.85      0.88      0.86      2000\n",
    "            1       0.87      0.85      0.86      2000\n",
    "\n",
    "    accuracy                            0.86      4000\n",
    "    macro avg       0.86      0.86      0.86      4000\n",
    "    weighted avg    0.86      0.86      0.86      4000\n",
    "\n",
    "*A2*: Version B, [```GloVe```](https://nlp.stanford.edu/projects/glove/) Model:\n",
    "\n",
    "    Accuracy: ~69.69%,\n",
    "    F1 Score: 0.7313829787234042\n",
    "\n",
    "    Confusion Matrix: \n",
    "    [[1138  862]\n",
    "    [ 350 1650]]\n",
    "\n",
    "    Classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "             0       0.76      0.57      0.65      2000\n",
    "             1       0.66      0.82      0.73      2000\n",
    "\n",
    "    accuracy                             0.70      4000\n",
    "    macro avg        0.71      0.70      0.69      4000\n",
    "    weighted avg     0.71      0.70      0.69      4000\n",
    "\n",
    "Although, we do see only a *slight* **~1.1% accuracy** improvement when we compare results from [```Word2Vec```](https://arxiv.org/abs/1301.3781) vs. [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf).\n",
    "\n",
    "However, this is a neat & expected observation, as we can recall that both [```Word2Vec```](https://arxiv.org/abs/1301.3781) & [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) are suitable or otherwise geared towards the generation of text, rather than the specificity of text-classification required for efficiency & performance with our Amazon review sentimentality prediction task.\n",
    "As expected, contrarywise, as the results from the implementation of [```GloVe```](https://nlp.stanford.edu/projects/glove/) pretrained embeddings & the [***```BERT```***](https://arxiv.org/abs/1810.04805) ***PLM*** show higher performance & accuracy compared to the [```Word2Vec```](https://arxiv.org/abs/1301.3781) pretrained embeddings & [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) ***PLM***, we could also make the observation of this supposed higher efficiency & efficacy by recalling that the [```GloVe```](https://nlp.stanford.edu/projects/glove/) pretrained embeddings were also designed & trained to create an unsupervised learning algorithm, with training that \"*performed on aggregated global word-word co-occurence statistics*\" ([Pennington, et al.](https://nlp.stanford.edu/projects/glove/), 2014), which, like the nature of [***```BERT```***](https://arxiv.org/abs/1810.04805)'s greater suitability towards text-classification tasks, could explain these higher accuracies & better performance as observed from our previous *A2* & most current, *A3* results.\n",
    "\n",
    "Recalling further, note that from our first assignment, *A1: Sentiment Analysis Text Classification*, we saw the following results from using *two (2) different algorithms* to implement the text-classification model, specifically, the *Logistic Regression* & the *Gaussian N√§ive Bayes* Algorithms:\n",
    "\n",
    "*From A1 Results:* Version A: *Gaussian N√§ive Bayes* Algorithm:\n",
    "\n",
    "    Accuracy: ~59.2%\n",
    "    F1 Score: 0.3664596273291925\n",
    "\n",
    "    CConfusion Matrix: \n",
    "    [[948  52]\n",
    "    [764 236]]\n",
    "\n",
    "    Classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.55      0.95      0.70      1000\n",
    "            1       0.82      0.24      0.37      1000\n",
    "\n",
    "    accuracy                            0.59      2000\n",
    "    macro avg       0.69      0.59      0.53      2000\n",
    "    weighted avg    0.69      0.59      0.53      2000\n",
    "\n",
    "*From A1 Results:* Version B: *Logistic Regression* Algorithm:\n",
    "\n",
    "    Accuracy:  92.7%\n",
    "    F1 Score:  0.9272908366533865\n",
    "\n",
    "    Confusion Matrix: \n",
    "    [[923  77]\n",
    "    [ 69 931]]\n",
    "\n",
    "    Classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.93      0.92      0.93      1000\n",
    "            1       0.92      0.93      0.93      1000\n",
    "\n",
    "    accuracy                            0.93      2000\n",
    "    macro avg       0.93      0.93      0.93      2000\n",
    "    weighted avg    0.93      0.93      0.93      2000\n",
    "\n",
    "This is interesting, as while it appears that using ***PLMs*** over ***pretrained embeddings*** yielded much significant improvements in both accuracy & performance, the results from using only the Logistic Regression Algorithm with no classifiers had yielded a *92.7% accuracy*, which still stands as one of the most accurate results from our three (3) assignments.\n",
    "\n",
    "However, this observation & other possible variations that can be observed, in performance & results, may be attributed to both the nature of text-tokenization, usage of classifiers, & particularly, that in the implementation of the first text-classification model of our *A1* assignment, I had trained the text-classification model cases with the ***full, eighty-thousand (80,000) rows of training data***, while in our current *A3* implementation, the training data for both the [***```BERT```***](https://arxiv.org/abs/1810.04805) & [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) ***PLMs*** were truncated randomly (with a shared random seed of ```69```) to a quarter-percent (25%) of the original number of rows, or twenty-thousand (20,000) rows of training data.\n",
    "\n",
    "#### Text-Classification Challenges & Limitations\n",
    "The initial & biggest challenge to implement these **pretrained language models** (***PLMs***) was the very significantly resource & time-intensive requirements for the computation, tokenization, & training of the two (2) ***PLMs*** used.\n",
    "\n",
    "With the current version of code & implementation, in average, it appeared that [***```BERT```***](https://arxiv.org/abs/1810.04805) took around *two-to-eight (2-to-8) minutes*, while [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) took longer, around *five-to-fifteen (5-to-15) minutes* long. Both, on average, usually took around *fifteen-to-thirty (15-to-30) minutes* to fully complete.\n",
    "\n",
    "However, in the earlier stages of coding & implementation, particularly before truncating the training & testing datasets to the currently-set percentage of twenty-five percent (25%) of the original amount of rows/lines of text, the average varied wildly, with completion durations taking *hours, upon hours* to complete or were stopped prior to completion due to time constraints.\n",
    "\n",
    "Furthermore, even post-truncation adjustments, the pretraining of the two ***PLMs***, in terms of computational power & resources, were very intensive, which had caused ***multiple***, **full computer crashes**, leading to a lot of processing delays & mental energy to continue implementing.\n",
    "\n",
    "#### Discussion for Future Performance & Efficacy Improvements\n",
    "With the issue of computational resources & the prevention of computer crashes, would be, of course, to finally sign-up for the student discounted *Google Cloud* subscription for using high-resource, cloud computing in *Google Colaboratory*, where NLP compatible, GPU-acceleration is available to expedite ***PLM***, model training, or otherwise for running intensive NLP code & tasks.\n",
    "As such, this specific idea for future performance & efficacy improvement will be implemented immediately following this current assignment, *A3*, where I will go ahead & attempt to set-up *Google Colaboratory* for use to hopefully, avoid the aforementioned resource & crash pitfalls for *A4* and our *Group Project*, heh.\n",
    "\n",
    "Furthermore, I should attempt to start early & try to implement for assignments in smaller code-blocks/pieces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References & Resources\n",
    "\n",
    "#### Libraries, Dependencies, & Credits\n",
    "    numpy\n",
    "    pandas\n",
    "    torch\n",
    "    random\n",
    "\n",
    "[HuggingFace_hub](https://huggingface.co/docs/hub/models-libraries)\n",
    "\n",
    "[*Google AI*](https://blog.research.google/2018/11/open-sourcing-bert-state-of-art-pre.html)'s [*BERT*](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "[*OpenAI*](https://openai.com/research/better-language-models)'s [*GPT-2*](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "\n",
    "[*HuggingFace*](https://huggingface.co/docs/hub/models-libraries)'s [*transformers*](https://huggingface.co/docs/transformers/index)\n",
    "\n",
    "```from``` [```transformers```](https://huggingface.co/docs/transformers/index) ```import``` [```AutoModel```](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html)(s)\n",
    "\n",
    "```from``` [```transformers```](https://huggingface.co/docs/transformers/index) ```import``` [```AutoTokenizer```](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#autotokenizer)\n",
    "\n",
    "```from``` [```transformers```](https://huggingface.co/docs/transformers/index) ```import``` [```AutoTokenizer.from_pretrained```](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#transformers.AutoTokenizer.from_pretrained), [```AutoModel.from_pretrained```](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#transformers.AutoModelForPreTraining.from_pretrained)\n",
    "\n",
    "[sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "\n",
    "[sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "\n",
    "[sklearn.metrics.f1_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)\n",
    "\n",
    "[sklearn.metrics.accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n",
    "\n",
    "[sklearn.metrics.confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n",
    "\n",
    "[sklearn.metrics.classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)\n",
    "\n",
    "[nbconvert](https://nbconvert.readthedocs.io/en/latest/)\n",
    "\n",
    "Credits to GitHub Copilot & ChatGPT for code implementation assistance.\n",
    "\n",
    "\n",
    "#### References\n",
    "\n",
    "[1] [P. Keung, Y. Lu, G. Szarvas, and N. A. Smith, ‚ÄúThe Multilingual Amazon Reviews Corpus.‚Äù arXiv, Oct. 06, 2020. doi: 10.48550/arXiv.2010.02573.](https://arxiv.org/abs/2010.02573)\n",
    "\n",
    "[2] [\"cardiffnlp/tweet_sentiment_multilingual ¬∑ Datasets at Hugging Face.\" [Online]. Available: https://huggingface.co/datasets/cardiffnlp/tweet_sentiment_multilingual](https://huggingface.co/datasets/cardiffnlp/tweet_sentiment_multilingual)\n",
    "\n",
    "[3] [\"Introducing LLaMA: A foundational, 65-billion-parameter language model.\" [Online]. Available: https://ai.meta.com/blog/large-language-model-llama-meta-ai/](https://ai.meta.com/blog/large-language-model-llama-meta-ai/)\n",
    "\n",
    "[4] [\"Llama 2: Open Foundation and Fine-Tuned Chat Models | Research - AI at Meta.\" [Online]. Available: https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)\n",
    "\n",
    "[5] [\"OpenAI Platform.\" [Online]. Available: https://platform.openai.com](https://platform.openai.com/docs/models/)\n",
    "\n",
    "[6] [\"Introducing ChatGPT.\" [Online]. Available: https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt)\n",
    "\n",
    "[7] [\"New models and developer products announced at DevDay.\" [Online]. Available: https://openai.com/blog/new-models-and-developer-products-announced-at-devday](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)\n",
    "\n",
    "[8] [G. M. Muktadir, \"A Brief History of Prompt: Leveraging Language Models. (Through Advanced Prompting).\" arXiv, Nov. 28, 2023. doi: 10.48550/arXiv.2310.04438.](https://arxiv.org/abs/2310.04438)\n",
    "\n",
    "[9] [J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\" arXiv, May 24, 2019. doi: 10.48550/arXiv.1810.04805.](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "[10] [A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, \"Language Models are Unsupervised Multitask Learners\".](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "\n",
    "[11] [\"HuggingChat.\" [Online]. Available: https://huggingface.co/chat](https://huggingface.co/chat)\n",
    "\n",
    "[12] [\"ü§ó Transformers.\" [Online]. Available: https://huggingface.co/docs/transformers/index](https://huggingface.co/docs/transformers/index)\n",
    "\n",
    "[13] [H. Touvron et al., \"LLaMA: Open and Efficient Foundation Language Models.\" arXiv, Feb. 27, 2023. doi: 10.48550/arXiv.2302.13971.](https://arxiv.org/abs/2302.13971)\n",
    "\n",
    "[14] [\"Meta and Microsoft Introduce the Next Generation of Llama.\" [Online]. Available: https://ai.meta.com/blog/llama-2/](https://ai.meta.com/blog/llama-2/)\n",
    "\n",
    "[15] [T. B. Brown et al., \"Language Models are Few-Shot Learners.\" arXiv, Jul. 22, 2020. doi: 10.48550/arXiv.2005.14165.](https://arxiv.org/abs/2005.14165)\n",
    "\n",
    "[16] [\"How Does Llama-2 Compare to GPT-4/3.5 and Other AI Language Models,\" Prompt Engineering Institute. [Online]. Available: https://promptengineering.org/how-does-llama-2-compare-to-gpt-and-other-ai-language-models/](https://promptengineering.org/how-does-llama-2-compare-to-gpt-and-other-ai-language-models/)\n",
    "\n",
    "[17] [\"Llama 2 vs. GPT-4: Nearly As Accurate and 30X Cheaper,\" Anyscale. [Online]. Available: https://www.anyscale.com/blog/llama-2-is-about-as-factually-accurate-as-gpt-4-for-summaries-and-is-30x-cheaper?ref=promptengineering.org](https://www.anyscale.com/blog/llama-2-is-about-as-factually-accurate-as-gpt-4-for-summaries-and-is-30x-cheaper?ref=promptengineering.org)\n",
    "\n",
    "[18] [A. Arshad, \"How to Use Llama 2 from Hugging Face to Solve Real-World Problems,\" Medium. [Online]. Available: https://medium.com/@anasarshad22/how-to-use-llama-2-from-hugging-face-to-solve-real-world-problems-f0216e32ef](https://medium.com/@anasarshad22/how-to-use-llama-2-from-hugging-face-to-solve-real-world-problems-f0216e32ef62)\n",
    "\n",
    "[19] [\"meta-llama/Llama-2-7b-chat ¬∑ Hugging Face.\" [Online]. Available: https://huggingface.co/meta-llama/Llama-2-7b-chat](https://huggingface.co/meta-llama/Llama-2-7b-chat)\n",
    "\n",
    "[20] [\"Llama 2 7B Chat - a Hugging Face Space by huggingface-projects.\" [Online]. Available: https://huggingface.co/spaces/huggingface-projects/llama-2-7b-chat](https://huggingface.co/spaces/huggingface-projects/llama-2-7b-chat)\n",
    "\n",
    "[21] [\"meta-llama/Llama-2-70b-chat-hf ¬∑ Hugging Face.\" [Online]. Available: https://huggingface.co/meta-llama/Llama-2-70b-chat-hf](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf)\n",
    "\n",
    "[22] [\"Chat with Llama 2.\" [Online]. Available: https://www.llama2.ai/](https://www.llama2.ai/)\n",
    "\n",
    "\n",
    "#### Special Thanks\n",
    "\n",
    "A huge shoutout to our awesome & wonderful TA Bahareh G. for their patience in dealing with my delays & for having to read my often super long written analysis / my messy code!\n",
    "\n",
    "Thanks & credits to my fellow classmate Ahmad A. for tips & suggestions on making a Python-specific data array for a more intuitive method to calculate metrics in the Jupyter Notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Juypter Notebook -> PDF Conversion thingy\n",
    "\n",
    "#!pip install nbconvert\n",
    "\n",
    "!jupyter nbconvert a4-Multilingual-Large-Language-Models-dan-jang --to pdf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra Stuff, pt. 1.\n",
    "\n",
    "##### **The Eight (8) Raw A4 Pre-Processed Testing Language Tweet-Instance-Sets from the New Dataset [2]**\n",
    "\n",
    "##### Dataset Source: [2] [\"cardiffnlp/tweet_sentiment_multilingual ¬∑ Datasets at Hugging Face.\" [Online]. Available: https://huggingface.co/datasets/cardiffnlp/tweet_sentiment_multilingual](https://huggingface.co/datasets/cardiffnlp/tweet_sentiment_multilingual)\n",
    "\n",
    "#### **Content Warning**: *Alike many \"natural language processing\" oriented or sources for text & data, the text & data used in this project may have been collected from public websites or sources, and may be unfiltered. Thus, some text & data may be disturbing, disagreeable, or otherwise objectionable.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.) processed_arabic.jsonl\n",
    "```json\n",
    "{\"text\": \"\\u0646\\u0648\\u0627\\u0644 \\u0627\\u0644\\u0632\\u063a\\u0628\\u064a (\\u0627\\u0644\\u0634\\u0627\\u0628 \\u062e\\u0627\\u0644\\u062f \\u0644\\u064a\\u0633 \\u0639\\u0627\\u0644\\u0645\\u064a) \\u0647\\u0647\\u0647\\u0647\\u0647\\u0647\\u0647\\u0647 \\u0623\\u062a\\u0641\\u0631\\u062c\\u064a \\u0639\\u0644\\u0649 \\u0647\\u0627 \\u0627\\u0644\\u0641\\u064a\\u062f\\u064a\\u0648 \\u064a\\u0627 \\u0645\\u0628\\u062a\\u062f\\u0626\\u0629 http v\\u00eda @user\", \"label\": \"0\"}\n",
    "{\"text\": \"\\u0646\\u0648\\u0627\\u0644 \\u0627\\u0644\\u0632\\u063a\\u0628\\u064a \\u0644\\u0637\\u064a\\u0641\\u0647 \\u0627\\u0644\\u0641\\u0646\\u0627\\u0646\\u0647 \\u0627\\u0644\\u0648\\u062d\\u064a\\u062f\\u0647 \\u0627\\u0644\\u0644\\u064a \\u0643\\u0644 \\u0627\\u0644\\u0641\\u064a\\u062f\\u064a\\u0648 \\u0643\\u0644\\u064a\\u0628\\u0627\\u062a \\u062a\\u0628\\u0639\\u0647\\u0627 \\u0645\\u0627\\u062a\\u0633\\u0628\\u0628 \\u062a\\u0644\\u0648\\u062b \\u0628\\u0635\\u0631\\u064a \\u0648\\u0644\\u0627 \\u0633\\u0645\\u0639\\u064a \\u0644\\u0648 \\u0635\\u0648\\u062a\\u0647\\u0627 \\u0627\\u0642\\u0644 \\u0645\\u0646 \\u0639\\u0627\\u062f\\u064a\", \"label\": \"2\"}\n",
    "{\"text\": \"\\u0644\\u0645\\u0627 \\u0642\\u0627\\u0644\\u062a \\u0646\\u0648\\u0627\\u0644 \\u0627\\u0644\\u0632\\u063a\\u0628\\u064a \\u0644\\u0627\\u0628\\u0642\\u0644\\u0647\\u0627 \\u0647\\u0627\\u0644\\u0644\\u0642\\u0628 \\u0641\\u0631\\u062d\\u0648\\u0627 \\u0641\\u0627\\u0646\\u0632\\u0647\\u0627 \\ud83d\\ude02\\ud83d\\ude02\\ud83d\\ude02\\u0643\\u0627\\u0646 \\u0644\\u0627\\u0632\\u0645 \\u064a\\u0627\\u062e\\u062f\\u0648\\u0647\\u0627 \\u0627\\u0647\\u0627\\u0646\\u0629 \\u0645\\u0634 \\u062b\\u0646\\u0627\\u0621 http\", \"label\": \"0\"}\n",
    "{\"text\": \"@user \\u062a\\u0630\\u0643\\u0631\\u0646\\u064a \\u0628\\u0623\\u063a\\u0646\\u064a\\u0629 \\u0646\\u0648\\u0627\\u0644 \\u0627\\u0644\\u0632\\u063a\\u0628\\u064a \\\"\\u0639\\u064a\\u0646\\u064a\\u0643 \\u0643\\u062f\\u0627\\u0628\\u064a\\u0646\\\"\", \"label\": \"2\"}\n",
    "{\"text\": \"\\u0628\\u0644\\u0627 \\u062a\\u0634\\u0641\\u064a\\u0631- \\u0623\\u0645\\u0644 \\u062d\\u0645\\u0627\\u062f\\u064a \\u0628\\u062a\\u0646\\u062a\\u0642\\u062f \\u0627\\u0644\\u0646\\u062c\\u0645\\u0629 \\u0646\\u0648\\u0627\\u0644 \\u0627\\u0644\\u0632\\u063a\\u0628\\u064a:\\\" \\u0646\\u062a\\u064a... http\", \"label\": \"0\"}\n",
    "{\"text\": \"\\u0641\\u0646\\u0627\\u0646\\u0629 \\u0644\\u0628\\u0646\\u0627\\u0646\\u064a\\u0629 \\u0643\\u0628\\u064a\\u0631\\u0629  \\u0635\\u0648\\u062a\\u0647\\u0627 \\u0625\\u0633\\u0637\\u0648\\u0631\\u064a \\u061f! #\\u0645\\u0627\\u062c\\u062f\\u0629_\\u0627\\u0644\\u0631\\u0648\\u0645\\u064a   @user #\\u0646\\u062c\\u0648\\u0649_\\u0643\\u0631\\u0645  @user #\\u0646\\u0648\\u0627\\u0644_\\u0627\\u0644\\u0632\\u063a\\u0628\\u064a  @user #\\u062c\\u0648\\u0644\\u064a\\u0627_\\u0628\\u0637\\u0631\\u0633\", \"label\": \"2\"}\n",
    "{\"text\": \"#\\u0644\\u0628\\u0646\\u0627\\u0646\\u064a_\\u064a\\u0642\\u064a\\u0645_\\u062f\\u0648\\u0631\\u0647_\\u0645\\u0643\\u064a\\u0627\\u062c_\\u0628\\u0627\\u0644\\u0631\\u064a\\u0627\\u0636\\u0645\\u0627\\u0646\\u0643\\u0628\\u0646\\u0649 \\u063a\\u064a\\u0631 \\u0628\\u0631\\u0627\\u0637\\u0645 \\u0646\\u0648\\u0627\\u0644 \\u0627\\u0644\\u0632\\u063a\\u0628\\u064a\\u0648\\u0646\\u0647\\u0648\\u062f \\u0627\\u0644\\u064a\\u0633\\u0627\\u0648\\u0645\\u0624\\u062e\\u0631\\u0629 \\u0647\\u064a\\u0641\\u0627\\u0621 \\u0648\\u0647\\u0628\\u064a\\u0642\\u0644\\u0646\\u0627\\u0644\\u0643\\u0645 \\u0643\\u0644\\u0647 \\u0646\\u0641\\u062e \\u0648\\u0647\\u0648\\u0627\\u0628\\u0633\\ud83d\\ude09\\ud83d\\ude09 \\u064a\\u0627\\u0632\\u064a\\u0646 \\u0646\\u0641\\u062e\\u0647\\u0645 \\u064a\\u0644\\u0637\\u0634\", \"label\": \"0\"}\n",
    "{\"text\": \"\\u0633\\u064a\\u0644\\u0641\\u064a \\u0644\\u0644\\u0641\\u0646\\u0627\\u0646\\u0629 \\ud83d\\udc47\\u0627\\u0644\\u0630\\u0647\\u0628\\u064a\\u0629 #\\u0646\\u0648\\u0627\\u0644_\\u0627\\u0644\\u0632\\u063a\\u0628\\u064a @user \\u0628\\u0627\\u0637\\u0644\\u0627\\u0644\\u0629 \\u0631\\u0642\\u064a\\u0642\\u0629 \\u062c\\u0645\\u064a\\u0644\\u0629 \\u0635\\u0628\\u0627\\u062d \\u0627\\u0644\\u064a\\u0648\\u0645 \\ud83d\\udc8b\\u2764\\ud83c\\udf39\\u0635\\u0628\\u0627\\u062d \\u0627\\u0644\\u062e\\u064a\\u0631 \\ud83d\\ude0d http\", \"label\": \"2\"}\n",
    "{\"text\": \"\\u0645\\u0646\\u0627\\u0641\\u0633\\u0629 \\u0643\\u0628\\u064a\\u0631\\u0629 \\u0628\\u064a\\u0646 #\\u0646\\u0648\\u0627\\u0644_\\u0627\\u0644\\u0632\\u063a\\u0628\\u064a \\u0648 #\\u0623\\u0645\\u0644_\\u062d\\u062c\\u0627\\u0632\\u064a  \\u0648\\u0645\\u0634\\u0627\\u0643\\u0644 \\u0648\\u063a\\u064a\\u0631\\u0629 \\u0645\\u062a\\u0628\\u0627\\u062f\\u0644\\u0629 \\u0645\\u0646 \\u0647\\u064a \\u0646\\u062c\\u0645\\u0629 \\u0634\\u0631\\u0643\\u0629 #\\u0644\\u0627\\u064a\\u0641_\\u0633\\u062a\\u0627\\u064a\\u0644\\u0632_\\u0627\\u0633\\u062a\\u0648\\u062f\\u064a\\u0648\\u0632#\\u062a\\u0648\\u0642\\u0639\\u0627\\u062a2017\", \"label\": \"0\"}\n",
    "{\"text\": \"\\u0631\\u0648\\u0626\\u064a\\u062a\\u0643 #\\u062a\\u0631\\u064a\\u062d #\\u0627\\u0644\\u0646\\u0638\\u0631 #\\u0633\\u0645\\u0627\\u0639\\u0643 #\\u064a\\u0631\\u064a\\u062d #\\u0627\\u0644\\u0627\\u0639\\u0635\\u0627\\u0628 #\\u0643\\u0644\\u0645\\u0627\\u062a\\u0643 #\\u0628\\u0644\\u0633\\u0645 #\\u0627\\u0644\\u062c\\u0631\\u0648\\u062d \\u064a #\\u0646\\u062c\\u0645\\u062a\\u064a #\\u0627\\u0644\\u0630\\u0647\\u0628\\u064a\\u0629 #\\u0646\\u0648\\u0627\\u0644_\\u0627\\u0644\\u0632\\u063a\\u0628\\u064a #\\u0628\\u062d\\u0628\\u0643 @user\\u2026 http\", \"label\": \"2\"}\n",
    "{\"text\": \"#star_news@Cheb_Khaled_ \\u0631\\u062f \\u0627\\u0644\\u0634\\u0627\\u0628 \\u062e\\u0627\\u0644\\u062f \\u0639\\u0644\\u0649 \\u0646\\u0648\\u0627\\u0644 \\u0627\\u0644\\u0632\\u063a\\u0628\\u064a \\u0645\\u0636\\u062d\\u0643 \\u0647\\u0647\\u0647\\u0647\\u0647\\u0647\\u0647\\u0647\\u0647\\u0647\\u0647  \\u0646\\u0648\\u0627\\u0644 \\u0644\\u0627\\u0632\\u0645 \\u0645\\u0627 \\u062a\\u062a\\u0643\\u0644\\u0645\\u064a \\u0639\\u0644\\u0649 \\u0627\\u0633\\u064a\\u0627\\u062f\\u0643http\", \"label\": \"0\"}\n",
    "{\"text\": \"\\u0639\\u0645 \\u0628\\u062d\\u0643\\u064a \\u0645\\u0639 \\u062d\\u0627\\u0644\\u064a #\\u0646\\u0648\\u0627\\u0644_\\u0627\\u0644\\u0632\\u063a\\u0628\\u064a http\", \"label\": \"2\"}\n",
    "{\"text\": \"\\u0644\\u0648 \\u0627\\u0634\\u0648\\u0641\\u0647 \\u0642\\u062f\\u0627\\u0645\\u064a \\u0630\\u0628\\u062d\\u062a\\u0647 \\u0628\\u0633 \\u0637\\u0628\\u0639\\u0627 \\u0645\\u0648 \\u0642\\u0628\\u0644 \\u0627\\u0644\\u062a\\u062d\\u0642\\u064a\\u0642  \\ud83d\\ude05\\ud83d\\ude05 @user #nadinenassibnjeim #nnn #star #actress\\u2026 http\", \"label\": \"0\"}\n",
    "{\"text\": \"@user - \\u0622\\u0644\\u0639\\u0622\\u0642\\u0644 \\u0622\\u0644\\u0622\\u06af\\u062b\\u0631 \\u062c\\u0646\\u0648\\u0646\\u0622 \\u0628\\u0622\\u0644\\u0630\\u0647\\u0628\\u06cc\\u0629\\u0629 #\\u0646\\u0648\\u0622\\u0644_\\u0622\\u0644\\u0632\\u063a\\u0628\\u064a \\u2764\\ud83d\\ude0d\\ud83d\\udc97\\ud83d\\udc97 http\", \"label\": \"2\"}\n",
    "{\"text\": \"@user \\u0647\\u0646\\u0627\\u0643 \\u0644\\u0639\\u0628\\u0629 \\u0648\\u062a\\u0628\\u0627\\u062f\\u0644 \\u0627\\u062f\\u0648\\u0627\\u0631 \\u0628\\u064a\\u0646 \\u0645\\u064a\\u0634\\u0627\\u0644 \\u0639\\u0648\\u0646 \\u0648\\u062d\\u0632\\u0628 \\u0627\\u0644\\u0644\\u0647 \\u0644\\u062e\\u062f\\u0627\\u0639 \\u0627\\u0644\\u0633\\u0639\\u0648\\u062f\\u064a\\u0647 \\u062d\\u062a\\u0649 \\u064a\\u062d\\u0635\\u0644\\u0648\\u0627 \\u0639\\u0644\\u0649 \\u0627\\u0644\\u0647\\u0628\\u0629 \\u0627\\u0644\\u0645\\u0627\\u0644\\u064a\\u0647 \\u0641\\u062d\\u0632\\u0628 \\u0627\\u0644\\u0644\\u0647 \\u0641\\u064a \\u0636\\u0627\\u0626\\u0642\\u0647 \\u0645\\u0627\\u0644\\u064a\\u0647 .\", \"label\": \"0\"}\n",
    "{\"text\": \"@user - \\ufe91\\ufef4\\ufee6 \\u06af\\u064f\\u0644 \\\" \\ufee7\\u0650\\ufed4\\ufeb2 \\ufed3\\ufef4\\u0651\\ufee8\\ufef2 \\\" \\u0635\\u064f\\u0648\\u062a\\u06af\\u06af\\\"  #\\u0646\\u0648\\u0622\\u0644_\\u0622\\u0644\\u0632\\u063a\\u0628\\u064a #\\u0622\\u063a\\u0644\\u0649_\\u0622\\u0644\\u062d\\u0628\\u0622\\u064a\\u0628 \\ud83d\\udc99\\ud83d\\udc97\\ud83d\\udc97\\ud83d\\ude0d\\u2764 http\", \"label\": \"2\"}\n",
    "{\"text\": \"-\\u0628\\u064a\\u0627\\u0631 \\u0631\\u0628\\u0651\\u0627\\u0637: \\u0644\\u0627 \\u0647\\u0644\\u0651\\u0642 \\u0628\\u062f\\u064a \\u0627\\u0633\\u0626\\u0644\\u0643\\u060c \\u0645\\u064a\\u0646 \\u0635\\u062f\\u064a\\u0642\\u0643 \\u0623\\u0643\\u062a\\u0631 \\\" \\u0627\\u0644\\u0631\\u0651\\u0626\\u064a\\u0633 \\u0645\\u064a\\u0634\\u0627\\u0644 \\u0639\\u0648\\u0646 \\u0623\\u0648 \\u0633\\u0644\\u064a\\u0645\\u0627\\u0646 \\u0641\\u0631\\u0646\\u062c\\u064a\\u0629\\u061f\\\"\\u0645\\u0634 \\u0639\\u0644\\u064a\\u0646\\u0627 \\u0647\\u0627\\u0644\\u062d\\u0631\\u0643\\u0627\\u062a. \\u0625\\u0646\\u0651\\u0648 \\u0627\\u0639\\u062a\\u0631\\u0641 \\u0628\\u063a\\u0644\\u0637\\u062a\\u0643 #\\u0628\\u0644\\u0627_\\u0647\\u0628\\u0644Part 2\", \"label\": \"0\"}\n",
    "{\"text\": \"@user - \\u0639\\u0644\\u0622\\u062c \\ufedf\\u064f\\ufee0\\ufee4\\ufeb0\\u0622\\ufe9d \\u0622\\ufedf\\ufee4\\ufeae \\ufed3\\ufef2 \\u0635\\u064f\\u0640\\u0648\\u062a\\u06af\\u06af  \\ud83d\\udc9b\\ud83d\\udc97\\ud83d\\udc99\\u2764\\ud83d\\udc99#\\u0646\\u0648\\u0622\\u0644_\\u0622\\u0644\\u0632\\u063a\\u0628\\u064a #\\u06af\\u064f\\u0644_\\u06cc\\u0648\\u0645_\\u062c\\u0645\\u0639\\u0629\\u0629 \\ud83d\\udc97\\ud83d\\udc9b http\", \"label\": \"2\"}\n",
    "{\"text\": \"\\u0648\\u062f\\u0627\\u062f \\u062c\\u0627\\u0628\\u0631: \\u0645\\u064a\\u0634\\u0627\\u0644 \\u0639\\u0648\\u0646 \\u0644\\u0646 \\u064a\\u0643\\u0645\\u0644 \\u0648\\u0644\\u0627\\u064a\\u062a\\u0647 #\\u0644\\u0644\\u0646\\u0634\\u0631\", \"label\": \"0\"}\n",
    "{\"text\": \"I liked a @user video from @user http \\u0646\\u0648\\u0627\\u0644 \\u0627\\u0644\\u0632\\u063a\\u0628\\u064a \\u0648 \\u0648\\u0627\\u0626\\u0644 \\u0643\\u0641\\u0648\\u0631\\u064a \\u0645\\u064a\\u0646 \\u062d\\u0628\\u064a\\u0628\\u064a\", \"label\": \"2\"}\n",
    "{\"text\": \"\\u0648\\u0645\\u0646 \\u063a\\u064a\\u0631 #\\u0627\\u0644\\u0631\\u0626\\u064a\\u0633_\\u0645\\u064a\\u0634\\u0627\\u0644_\\u0639\\u0648\\u0646 \\u064a\\u0633\\u062a\\u0623\\u0647\\u0644 \\u0630\\u0644\\u0643 \\u0641\\u064a \\u0648\\u062c\\u0647 \\u0627\\u0644\\u0641\\u0633\\u0627\\u062f \\u0648\\u0627\\u0644\\u062d\\u0631\\u0627\\u0645\\u064a\\u064a  #\\u062c\\u0647\\u0632\\u0648\\u0627_\\u0633\\u0648\\u0627\\u0639\\u062f\\u0643\\u0645 http\", \"label\": \"0\"}\n",
    "{\"text\": \"\\u0646\\u0648\\u0627\\u0644 \\u0627\\u0644\\u0632\\u063a\\u0628\\u064a  \\u0635\\u0648\\u062a \\u0627\\u0644\\u0647\\u062f\\u0648\\u0621 \\u2764\\ufe0f\\u2764\\ufe0f\", \"label\": \"2\"}\n",
    "{\"text\": \"\\u0631\\u0623\\u0649 \\u0627\\u0644\\u0631\\u0626\\u064a\\u0633 \\u0645\\u064a\\u0634\\u0627\\u0644 \\u0639\\u0648\\u0646 \\u0623\\u0646 (\\u0627\\u0644\\u0631\\u0626\\u064a\\u0633 \\u0627\\u0644\\u0633\\u0648\\u0631\\u064a) \\u0628\\u0634\\u0627\\u0631 \\u0627\\u0644\\u0623\\u0633\\u062f \\u0644\\u0648 \\u062e\\u0633\\u0631 \\u0627\\u0644\\u062d\\u0631\\u0628\\u060c \\u0644\\u062a\\u062d\\u0648\\u0644\\u062a \\u0633\\u0648\\u0631\\u064a\\u0629 \\u0625\\u0644\\u0649 \\u0644\\u064a\\u0628\\u064a\\u0627 \\u062b\\u0627\\u0646\\u064a\\u0629\\\"\\u060c\", \"label\": \"0\"}\n",
    "{\"text\": \"\\u0646\\u0648\\u0627\\u0644 \\u0627\\u0644\\u0632\\u063a\\u0628\\u064a \\u0645\\u0646 \\u0627\\u0644\\u062d\\u062c\\u0627\\u062a \\u0627\\u0644\\u064a \\u0647\\u062a\\u0641\\u0636\\u0644 \\u062c\\u0645\\u064a\\u0644\\u0647 \\u0639\\u0644\\u064a \\u0637\\u0648\\u0644...\\ud83d\\ude0d\\ud83d\\ude0d\\ud83d\\ude0d\\ud83d\\ude0d http\", \"label\": \"2\"}\n",
    "```\n",
    "\n",
    "#### # 2.) processed_english.jsonl (Pre-Processed by manually filtering out any jsonl objects with a \"1\" value for the \"label\" key, which would be the 'neutral' rating)\n",
    "```json\n",
    "{\"text\": \"Trying to have a conversation with my dad about vegetarianism is the most pointless infuriating thing ever #caveman \", \"label\": \"0\"}\n",
    "{\"text\": \"@user You are a stand up guy and a Gentleman Vice President Pence \", \"label\": \"2\"}\n",
    "{\"text\": \"@user @user @user Looks like Flynn isn't too pleased with me, he blocked me. You blocked by Flynn too @user \", \"label\": \"0\"}\n",
    "{\"text\": \"i'm not even catholic, but pope francis is my dude. like i just need him to hug me and tell me everything is okay. \", \"label\": \"2\"}\n",
    "{\"text\": \"@user for al the crying you do about how middle America is left out-they have twice as much voting power \", \"label\": \"0\"}\n",
    "{\"text\": \"Samsung to Bring Android 7.0 Nougat to Galaxy S6, S6 edge, Note 5, and Tab S2 - Softpedia News \", \"label\": \"2\"}\n",
    "{\"text\": \"Israel deems comatose Gaza man who needs treatment in West Bank  a security threat. #Palestine  via @user \", \"label\": \"0\"}\n",
    "{\"text\": \"I will go so far to say s1 of westworld isn't just good, it's brilliant. A story within a story within a story about storytelling \", \"label\": \"2\"}\n",
    "{\"text\": \"Iraqi Forces set to storm 3 areas of #Mosul, #AlQahira, #alMasarif &  #alAmn#MosulOps#mosuloffensive#iraq#ISIS\\u2026 \", \"label\": \"0\"}\n",
    "{\"text\": \"#NationalFastFoodDay Would love to live there. Chick-fil-A \\ud83d\\ude0d \", \"label\": \"2\"}\n",
    "{\"text\": \"Electoral College must reject Trump #rejecttrump #notmypresident #takingbackdemocracy #blacklivesmatter #nevertrump \", \"label\": \"0\"}\n",
    "{\"text\": \"The Reputation Doctor weighs in on Tony Romo #NFL @user joins @user on #TheMorningRush LISTEN: \", \"label\": \"2\"}\n",
    "{\"text\": \"Stop #fracking #Cuadrilla persecuting land defenders through the legal system #BankruptCuadrilla #ECOCIDE \", \"label\": \"0\"}\n",
    "{\"text\": \"So proud of way @user & #trumpTransitionteam are molding strong leadership group for #America #TeamTrump #MakeAmericaGreatAgain \", \"label\": \"2\"}\n",
    "{\"text\": \"@user Everyone's thinking far too short-termed. When all fossil fuels run out with no substitutes, then the crisis will come. \", \"label\": \"0\"}\n",
    "{\"text\": \"@user @user - #ScreamQueens so lucky to get to work with TK. \", \"label\": \"2\"}\n",
    "{\"text\": \"@user @user @user Why is it that today society is casually comfortable about being pubicly distasteful?#deplorables \", \"label\": \"0\"}\n",
    "{\"text\": \"This is a big deal and a smart move by Microsoft: \", \"label\": \"2\"}\n",
    "{\"text\": \"Ben Carson for Housing & Urban Development?? \\ud83d\\ude10 I just can't \\ud83d\\ude12 \", \"label\": \"0\"}\n",
    "{\"text\": \"Digesting while watching #ScreamQueens s1 Thanksgiving & Black Friday eps. SO SO GOOD!! \", \"label\": \"2\"}\n",
    "{\"text\": \"Israel's New Racism: Persecution of African Migrants in Holy Land #gaza #palestine #israel #BDS \", \"label\": \"0\"}\n",
    "{\"text\": \"I've been listening to Leonard Cohen nonstop for the past month. Only to find out he passed literally weeks ago. Wow my heart is so overwhel \", \"label\": \"2\"}\n",
    "{\"text\": \"@user She just didn't get them in areas were she needed them. Lots of voter suppression going on. Hacking & tampering\\ud83d\\udc99 \", \"label\": \"0\"}\n",
    "{\"text\": \"Listen to #NBAwards Winner @user interview on @user \", \"label\": \"2\"}\n",
    "```\n",
    "\n",
    "#### # 3.) processed_french.jsonl (Pre-Processed by manually filtering out any jsonl objects with a \"1\" value for the \"label\" key, which would be the 'neutral' rating)\n",
    "```json\n",
    "{\"text\": \"Royal: le pr\\u00e9sident n'aime pas les pauvres? \\\"c'est n'importe quoi\\\" http \\u2026\", \"label\": \"0\"}\n",
    "{\"text\": \"#press #seo: Nager vert avec les piscines \\u00e9cologiques http #piscine\\u00e9cologique\", \"label\": \"2\"}\n",
    "{\"text\": \"#Biodiversit\\u00e9 #Gard : Des L\\u00e9zards ocell\\u00e9s d\\u00e9truits sans autorisation \\u00e0 Saint-Gilles ! http\", \"label\": \"0\"}\n",
    "{\"text\": \"Etat des lieux et analyse des m\\u00e9canismes de soutien public aux \\u00e9nergies renouvelables en France \\u25ba\\u25ba http\", \"label\": \"2\"}\n",
    "{\"text\": \"Les esp\\u00e8ces disparaissent 1.000 fois plus depuis que l'Homme existe | @user http\", \"label\": \"0\"}\n",
    "{\"text\": \"Une entreprise allemande propose une solution \\u00e9co-efficace de ferme urbaine http #NRJ(Canada) #RegionsofFrance\", \"label\": \"2\"}\n",
    "{\"text\": \"R\\u00e9chauffement : l\\u2019ONU pr\\u00e9dit un v\\u00e9ritable \\u00abenfer climatique\\u00bb d\\u2019ici 2050 http http\", \"label\": \"0\"}\n",
    "{\"text\": \"@user @user Ah Oui, les \\u00e9oliennes, pourrait aussi les \\u00e9loigner .. ouf Merci...J'en veux une dans ma cour...\", \"label\": \"2\"}\n",
    "{\"text\": \"Info: \\u00c9oliennes - Le r\\u00e9sultat de l\\u2019enqu\\u00eate publique est tomb\\u00e9 pour Esnes et Montz\\u00e9ville : avis d\\u00e9favorable\", \"label\": \"0\"}\n",
    "{\"text\": \"R\\u00e9chauffement climatique: il est grand temps d'agir - http\", \"label\": \"2\"}\n",
    "{\"text\": \"#GCO : les \\u00e9cologistes strasbourgeois sont \\\"stup\\u00e9faits\\\" - http\", \"label\": \"0\"}\n",
    "{\"text\": \"J'aime ma bio parce que je mange bio et je travail dans la biologie et je roule en biocarburant.\", \"label\": \"2\"}\n",
    "{\"text\": \"Pol\\u00e9mique autour de l\\u2019abattage massif de bouquetins en Haute-Savoie | Eco(lo) http\", \"label\": \"0\"}\n",
    "{\"text\": \"Echange autour du th\\u00e8me de l'agriculture raisonn\\u00e9e \\u00e0 la R\\u00e9union http\", \"label\": \"2\"}\n",
    "{\"text\": \"Un r\\u00e9seau de journalistes pour lutter contre les n\\u00e9gationnistes du changement climatique. http\", \"label\": \"0\"}\n",
    "{\"text\": \"@user \\\"Illimit\\u00e9es\\\", cela fera sourire les environnementalistes. Et le th\\u00e8me du d\\u00e9clin am\\u00e9ricain est r\\u00e9curent depuis les ann\\u00e9es 1970...\", \"label\": \"2\"}\n",
    "{\"text\": \"L\\u2019exportation de l\\u2019\\u00e9nergie \\u00e9olienne, une source de profits boud\\u00e9e par Hydro-Qu\\u00e9bec  | Michel Lagac\\u00e9... http\", \"label\": \"0\"}\n",
    "{\"text\": \"@user ah \\u00e7a c'est s\\u00fbr. J'ai fais du volont\\u00e9riat en \\u00e9cologie.\", \"label\": \"2\"}\n",
    "{\"text\": \"L\\u2019exportation de l\\u2019\\u00e9nergie \\u00e9olienne, une source de profits boud\\u00e9e par Hydro-Qu\\u00e9bec \\u00a0|\\u00a0Michel\\u00a0Lagac\\u00e9 http\", \"label\": \"0\"}\n",
    "{\"text\": \"En France aussi, la police / gendarmerie est au service d'une minorit\\u00e9... http #Sivens #\\u00c9cologie http\", \"label\": \"2\"}\n",
    "{\"text\": \"La lutte contre la mosa\\u00efque du manioc aux Comores: La s\\u00e9lection vari\\u00e9tale pour lutter contre la mosa\\u00efque du manioc http\", \"label\": \"0\"}\n",
    "{\"text\": \"Il nous sort \\\"J'vais pas allumer la lumi\\u00e8re parce que je veux pas faire marcher des centrales \\u00e9lectriques.\\\" Ah.\", \"label\": \"2\"}\n",
    "{\"text\": \"Cuba: Mesures contre les effets du changement http #Cuba\", \"label\": \"0\"}\n",
    "{\"text\": \"@user @user @user Je mets au d\\u00e9fis ce d\\u00e9put\\u00e9 bobo \\u00e9cologiste d'aller au chantier de Saint-Nazaire #chiche\", \"label\": \"2\"}\n",
    "```\n",
    "\n",
    "#### # 4.) processed_german.jsonl (Pre-Processed by manually filtering out any jsonl objects with a \"1\" value for the \"label\" key, which would be the 'neutral' rating)\n",
    "```json\n",
    "{\"text\": \"@user korrekt! Verstehe sowas nicht...\", \"label\": \"0\"}\n",
    "{\"text\": \"Der Dubbletimepart von Julien war ja mal sowas von genial! :D (@user http\", \"label\": \"2\"}\n",
    "{\"text\": \"#Instachallenge #Day16 #what #i #am #reading #Fratzensammler #Horror #Wattpad http\", \"label\": \"0\"}\n",
    "{\"text\": \"@user ah. Hatte nur bis radolfzell geschaut wegen ticket. Aber so isses fast normal ;)\", \"label\": \"2\"}\n",
    "{\"text\": \"@user \\u00dcberall lauert Gefahr. Unverhofft. #Achtung\", \"label\": \"0\"}\n",
    "{\"text\": \"Sonntag \\u2714 Fitness \\u2714 Nichts tun\\u2714\", \"label\": \"2\"}\n",
    "{\"text\": \"@user da werden wir uns wohl nicht einig werden, bef\\u00fcrchte ich!\", \"label\": \"0\"}\n",
    "{\"text\": \"@user Toll. Sehr l\\u00f6blich. Sieht echt klasse aus.\", \"label\": \"2\"}\n",
    "{\"text\": \"\\\"Absurde Unwahrheiten\\\" -Hoene\\u00df stocksauer auf den \\\"Stern\\\" http #Absurde #Nachrichten #N24\", \"label\": \"0\"}\n",
    "{\"text\": \"RT @user: Linda, perfeita, guapa, Wundersch\\u00f6n, Bella, Beautiful !! http\", \"label\": \"2\"}\n",
    "{\"text\": \"Dank @user spiele ich jetzt ohne Schei\\u00df Candy Crush Saga ... es regt mich ma\\u00dflos auf aber aufh\\u00f6ren kann ich auch ned -.-\", \"label\": \"0\"}\n",
    "{\"text\": \"Andreas Sp\\u00e4ck aus der Crowd sucht ein originelles Hochzeitgeschenk... ;-) Wer kann helfen? Merci ! Chuy You... http\", \"label\": \"2\"}\n",
    "{\"text\": \"Nicht nur gg B\\u00fcrger, sondern auch gg eigene Mitarbeiter mobben die Beh\\u00f6rden. Diese F\\u00e4lle bearbeitet http\", \"label\": \"0\"}\n",
    "{\"text\": \"@user OMG Titanium! Unbelievable Jahmene\", \"label\": \"2\"}\n",
    "{\"text\": \"Unn\\u00f6tiger schei\\u00df. Ernsthaft.\", \"label\": \"0\"}\n",
    "{\"text\": \"Bei http l\\ufffduft zur zeit EminemSing For The Moment also schnell einschalten :)\", \"label\": \"2\"}\n",
    "{\"text\": \"RT @user: liebe @user, dein Duschgel schmeckt nicht #tranzparenztweet\", \"label\": \"0\"}\n",
    "{\"text\": \"@user Aw, na gut, dann schlaf fein, klein Delalein :)\", \"label\": \"2\"}\n",
    "{\"text\": \"@user kann man viel gelesen haben davon, nona, aber grunds\\u00e4tzlich so nat\\u00fcrlich l\\u00e4cherlich, die Liste! ff\", \"label\": \"0\"}\n",
    "{\"text\": \"@user kale chips= life\", \"label\": \"2\"}\n",
    "{\"text\": \"Arschl\\u00f6cher!!! MOBIL http\", \"label\": \"0\"}\n",
    "{\"text\": \"RT @user: Das beste kommt zum Schluss? Ils\\u00f6: \\\"Manchmal kommt der Beste auch erst in den letzten 45min.\\\"\\u2026 http\", \"label\": \"2\"}\n",
    "{\"text\": \"RT @user: Wach liegen, Gedankenkarussell, unbeantwortete Fragen, Bangen, Hoffen, Zweifeln, Stolz, Verblendung, Leere, Einsamkeit\\u2026\", \"label\": \"0\"}\n",
    "{\"text\": \"@user Mich haben se nach vorne geschickt, konnt ne stille stehen ^^\", \"label\": \"2\"}\n",
    "```\n",
    "\n",
    "#### # 5.) processed_hindi.jsonl (Pre-Processed by manually filtering out any jsonl objects with a \"1\" value for the \"label\" key, which would be the 'neutral' rating)\n",
    "```json\n",
    "{\"text\": \"CONGRESS na ye party kabhi bani hoti na india ka partition hota nd na hi humari country itni khokhli hoti   @ \", \"label\": \"0\"}\n",
    "{\"text\": \"ha ha ha ha ha ha ha .  .  .  .  . :d ye mast tha .  .  .  . cute aur comic .  .  .  . :)\", \"label\": \"2\"}\n",
    "{\"text\": \"kejariwal tum apana soch ka dayra badho ab is misunderstanding se kam nahi chalane wala hai tumhara\", \"label\": \"0\"}\n",
    "{\"text\": \"jai hind\", \"label\": \"2\"}\n",
    "{\"text\": \"result me konsa bc univrsty tope mari ... jo itna wait kr rha h \", \"label\": \"0\"}\n",
    "{\"text\": \"rt @mukhijanidhi: hai teri hasti aisi jo dekhe wo mar mite #msgyouthicon #msgrevolution\", \"label\": \"2\"}\n",
    "{\"text\": \"Dhyan rakh ?? \", \"label\": \"0\"}\n",
    "{\"text\": \"aisa school ho to me kabhi ghar hi na jau .  .  . \", \"label\": \"2\"}\n",
    "{\"text\": \"pratiyogita darpan ki spelling mistake hai :)\", \"label\": \"0\"}\n",
    "{\"text\": \"i'll never forget that first girl i was crazy about in 5th grade .  i still got her valentine day card in my secret safe . \", \"label\": \"2\"}\n",
    "{\"text\": \"BC porn site ki tarah advertise karo tum bas ..... !!!! male hoe .. \", \"label\": \"0\"}\n",
    "{\"text\": \"hahaha sai m .  .  ek baar class se nikal jate the to pura school naap ke wapas ate the\", \"label\": \"2\"}\n",
    "{\"text\": \"post of the night: anonymous (id: excv6jkr) 11/02/12(fri)00:52:39 no . 434329650 this thread is like watching a monkey fuck a coconut\", \"label\": \"0\"}\n",
    "{\"text\": \"Jaanma main bol rahi hu ki,tum mere twits dekho :/\", \"label\": \"2\"}\n",
    "{\"text\": \"Did \", \"label\": \"0\"}\n",
    "{\"text\": \"rt psharma2525: rt komalinsaan: gurmeetramrahim #lovetoseemsg #msgincinemas schi agr duniya ye movi dekh le to yaha swrg bn skta h\", \"label\": \"2\"}\n",
    "{\"text\": \"Woh  bhi \", \"label\": \"0\"}\n",
    "{\"text\": \"rare ,  300kg meteorite discovered in poland ,  biggest in eastern europe and may provide clues about earth's core: http/URL\", \"label\": \"2\"}\n",
    "{\"text\": \"band karo ye atyachaar. #indvsuae\", \"label\": \"0\"}\n",
    "{\"text\": \"durdarshan wah wah  .  .  .  . \", \"label\": \"2\"}\n",
    "{\"text\": \"Ruk \", \"label\": \"0\"}\n",
    "{\"text\": \"Salman bai ap kese hen ma b ap ka fen hun\", \"label\": \"2\"}\n",
    "{\"text\": \"aur jab koi ek excuse maar deta tha toh dusra kehta saale ye maine socha hua tha tune kyun bol diya\", \"label\": \"0\"}\n",
    "{\"text\": \"kiranji ko delhi ka cm ummidwar ghosit karna modiji ki doodarshita wa samghdaari ko darshata hai . \", \"label\": \"2\"}\n",
    "```\n",
    "\n",
    "#### # 6.) processed_italian.jsonl (Pre-Processed by manually filtering out any jsonl objects with a \"1\" value for the \"label\" key, which would be the 'neutral' rating)\n",
    "```json\n",
    "{\"text\": \"@user @user Ma Ferrero? il compagno Ferrero? ma il suo partito esiste ancora? allora stiamo proprio frecati !!!\", \"label\": \"0\"}\n",
    "{\"text\": \"Non vedi l'ora che venga qui almeno lo sentir\\u00f2 pi\\u00f9 spesso e potr\\u00f2 finalmente stare con lui alla faccia di chi mi ha friendzonato.\", \"label\": \"2\"}\n",
    "{\"text\": \"@user @user solo che poi arriva @user e decide sempre tutto lui\", \"label\": \"0\"}\n",
    "{\"text\": \"@user Tutta la pasticceria! \\ud83d\\udc8f Grazie amore mio! \\u2764\", \"label\": \"2\"}\n",
    "{\"text\": \"Quanto mi stai cazzo troia\", \"label\": \"0\"}\n",
    "{\"text\": \"Perch\\u00e9 alla fine siamo una famiglia e la famiglia non va mai abbandonata\\ud83d\\udc95 #WeWillMeetAgain1D http\", \"label\": \"2\"}\n",
    "{\"text\": \"@user @user @user @user @user calcio \\u00e8 la quintessenza della noia. Sport frenetici come basket o volley no\", \"label\": \"0\"}\n",
    "{\"text\": \"@user Ciao!Mia sorella ha un canale YouTube che si chiama corinnelife97potresti visitarlo e iscriverti se ti piace?Grazie\\ud83d\\udc9a\", \"label\": \"2\"}\n",
    "{\"text\": \"cazzo sono tutti sti tweet in tl di \\\\\\\"innocente\\\\\\\" \\\\\\\"colpevole\\\\\\\" ao\", \"label\": \"0\"}\n",
    "{\"text\": \"La bellezza della mie Steve Madden Fringly \\ud83d\\udc97 http\", \"label\": \"2\"}\n",
    "{\"text\": \"@user per\\u00f2 ha detto che \\u00e8 collegato positivamente alla pedofilia quindi ci vede qualcosa di buono\", \"label\": \"0\"}\n",
    "{\"text\": \"La pi\\u00f9 grande prova d'amore \\u00e8 sempre stata togliersi dai coglioni.\", \"label\": \"2\"}\n",
    "{\"text\": \"@user @user Quando invece riaprii i miei occhi mi accorsi che non era un sogno...Era tutto nitido e reale!Grigio e freddo...\", \"label\": \"0\"}\n",
    "{\"text\": \"\\u2013 tomlinsik;Profilo molto carino in generale abbinato molto bene.\", \"label\": \"2\"}\n",
    "{\"text\": \"Io faccio di tutto e lei ha ancora la faccia di ignorarmi perch\\u00e9 sono cos\\u00ec stupida? L'avrei dovuta mandare a fanculo da tempo.\", \"label\": \"0\"}\n",
    "{\"text\": \"@user @user @user @user @user @user prima ora e dopo \\ud83d\\ude0d\\ud83d\\ude0d\\ud83d\\ude1a\", \"label\": \"2\"}\n",
    "{\"text\": \"@user quanto ti odio da uno a dieci\", \"label\": \"0\"}\n",
    "{\"text\": \"@user @user @user ti vogliamo bene anche noi Harry\", \"label\": \"2\"}\n",
    "{\"text\": \"Io mi lamento della gente che scrive ancora \\\\\\\"freddy mercury\\\\\\\" ma anche quella che scrive \\\\\\\"jhonny cash\\\\\\\" non scherza\", \"label\": \"0\"}\n",
    "{\"text\": \"Oggi cominciamo cos\\u00ec! Con i nostri saluti per voi di venerd\\u00ec scorso dopo \\\\\\\"Notre Dame de Paris\\\\\\\". http\", \"label\": \"2\"}\n",
    "{\"text\": \"Dal prossimo anno Audi non sar\\u00e0 pi\\u00f9 uno sponsor del Milan il testimone passer\\u00e0 alla Piaggio...e via con gli Apetti!!\", \"label\": \"0\"}\n",
    "{\"text\": \"IL MIO DIARIO - Pagine d'estate quando ti innamori piano piano dietro uno schermo. Grazie Greta. @user \\u2764\\ufe0f http\", \"label\": \"2\"}\n",
    "{\"text\": \"@user @user il fatto che una persona che vada in discoteca implichi che questa si droghi \\u00e8 una fesseria...\", \"label\": \"0\"}\n",
    "{\"text\": \"@user che la forza sia con noi\", \"label\": \"2\"}\n",
    "```\n",
    "\n",
    "#### 7.) processed_portuguese.jsonl (Pre-Processed by manually filtering out any jsonl objects with a \"1\" value for the \"label\" key, which would be the 'neutral' rating)\n",
    "```json\n",
    "{\"text\": \"todos os meus favoritos na prova de elimina\\u00e7\\u00e3o #MasterChefBR\", \"label\": \"0\"}\n",
    "{\"text\": \"Augusto Cury \\u00e9 o cara\\ud83d\\ude09 entrevista myito boa do #ConversaComBial\", \"label\": \"2\"}\n",
    "{\"text\": \"Mano Vitor, t\\u00e1 muito feio! Decep\\u00e7\\u00e3o. #MasterChefBR\", \"label\": \"0\"}\n",
    "{\"text\": \"Morrendo de amores pelo @user no programa do @user. #TheNoite\", \"label\": \"2\"}\n",
    "{\"text\": \"MIRIAN CHOORANDO PELA PROVA PASSADA #MasterChefBR\", \"label\": \"0\"}\n",
    "{\"text\": \"a cada epis\\u00f3dio q passa Fabrizio continua um gato ne \\ud83d\\ude0f #MasterChefBR\", \"label\": \"2\"}\n",
    "{\"text\": \"Traz o @user e tira a Mirian Pvr    #MasterChefBR\", \"label\": \"0\"}\n",
    "{\"text\": \"A maior e melhor cantora que voc\\u00ea respeita @user no @user. #SandyLeah #Encontro #SandynoEncontro\", \"label\": \"2\"}\n",
    "{\"text\": \"Imagina que insuport\\u00e1vel ter de dar de comer pra uma gente que calcula CADA CALORIA que come? Jesus... #MasterChefBR\", \"label\": \"0\"}\n",
    "{\"text\": \"Baixa caloria? Frango desfiado e salada. T\\u00e1 \\u00f3timo  #MasterChefBR\", \"label\": \"2\"}\n",
    "{\"text\": \"A\\u00ed que agonia essa Marian, meu Deus do c\\u00e9u, mulher!! #MasterChefBR\", \"label\": \"0\"}\n",
    "{\"text\": \"\\\"Eu adoro fazer piquenique no TEMPRO\\u201d  haha te amo yuko #MasterChefBR\", \"label\": \"2\"}\n",
    "{\"text\": \"\\\"uma mulher mais nova com um homem mais velho \\u00e9 normal\\\" normal dizer que ela ta com ele por dinheiro n\\u00e9? #Encontro\", \"label\": \"0\"}\n",
    "{\"text\": \"Que amor esse @user no #Encontro \\u2764\\u2764\\u2764\", \"label\": \"2\"}\n",
    "{\"text\": \"Vanessa da Mata acabou com a fofurice do parto da mala da Bela Gil kkkkkk #AltasHoras\", \"label\": \"0\"}\n",
    "{\"text\": \"f\\u00e1tima bernardes maravilhosa no #VideoShowAoVivo\", \"label\": \"2\"}\n",
    "{\"text\": \"Al\\u00f4 NET, voces t\\u00e3o fora do ar?! Arrumem por favor que eu quero assistir #MasterChefBR hojeeee\", \"label\": \"0\"}\n",
    "{\"text\": \"#MasterChefBR Hummm deu vontade de comer uma sardinha com p\\u00e3ozinho kkk partiu abrir uma latinha! \\ud83d\\ude01\", \"label\": \"2\"}\n",
    "{\"text\": \"Pq o programa n\\u00e3o ta ao vivo? @user  #MaisVoc\\u00ea\", \"label\": \"0\"}\n",
    "{\"text\": \"Mr Catra dando um show no The Noite ! Falando da pol\\u00edtica no pa\\u00eds \\ud83d\\udc4f #TheNoite @user @user\", \"label\": \"2\"}\n",
    "{\"text\": \"Se algu\\u00e9m me fala que \\u00e9 termog\\u00eanico, eu devolvo o prato na hora #MasterChefBR\", \"label\": \"0\"}\n",
    "{\"text\": \"Ai eu me derreto todinha com a Paola\\ud83d\\ude2d\\ud83d\\ude0d #MasterChefBR\", \"label\": \"2\"}\n",
    "{\"text\": \"#AltasHoras assistindo o programa com uma cx de len\\u00e7os ao lado! Haja\\u2764\\ud83d\\ude22\", \"label\": \"0\"}\n",
    "{\"text\": \"Legal est\\u00e1 prova tomara que tenha mais #DomingoLegal\", \"label\": \"2\"}\n",
    "```\n",
    "\n",
    "#### 8.) processed_spanish.jsonl (Pre-Processed by manually filtering out any jsonl objects with a \"1\" value for the \"label\" key, which would be the 'neutral' rating)\n",
    "```json\n",
    "{\"text\": \"@user jajajaja dale, hacete la boluda vos jajaja igual a vos nunca se te puede tomar en serio te mando un abrazo desde Per\\u00fa!\", \"label\": \"0\"}\n",
    "{\"text\": \"@user MAAAAE RAJADO! Pero lo bueno es q uno se va independizando!y logrando metas\", \"label\": \"2\"}\n",
    "{\"text\": \"Bueno hoy fui a almorzar a Nanay con otras 3 dras xq la capacitaci\\u00f3n mal organizada no nos di\\u00f3 almuerzo y encima nos mandan a comer 2pm\", \"label\": \"0\"}\n",
    "{\"text\": \"@user \\u00a1Hola Tom\\u00e1s! \\u00bfHab\\u00e9is visto los nuevos #dinos de #TierraMagna? Es normal que haya colas antes de que comience el espect\\u00e1culo\", \"label\": \"2\"}\n",
    "{\"text\": \"@user la hijueputa tela se me sali\\u00f3. yo quer\\u00eda volver a quedar acostada.\", \"label\": \"0\"}\n",
    "{\"text\": \"@user pues no est\\u00e1 nada mal\", \"label\": \"2\"}\n",
    "{\"text\": \"@user quiz\\u00e1 para profesionales no sea mucho,pero hay no remunerados principalmente femenino para quienes es un sue\\u00f1o, pasa en mi pa\\u00eds\", \"label\": \"0\"}\n",
    "{\"text\": \"Hora de seguir so\\u00f1ando muy bonito\", \"label\": \"2\"}\n",
    "{\"text\": \"Me estoy muriendo. Ojal\\u00e1 mi jefa me haga trabajar viernes y s\\u00e1bado en la ma\\u00f1ana. Porque cerrar va a ser fatal\", \"label\": \"0\"}\n",
    "{\"text\": \"La felicidad tiene un nombr #Tailandia  Si es tu PRIMERA VEZ, apunta estos #consejos!   http http\", \"label\": \"2\"}\n",
    "{\"text\": \"@user @user justamente ah\\u00ed es tu equivocada suposici\\u00f3n, qxq estudi\\u00f3 en 1 colegio ficho ser\\u00e1 pulcro? La religi\\u00f3n le importa mda\", \"label\": \"0\"}\n",
    "{\"text\": \"@user Alpha suerte y ten un feliz a\\u00f1o 2017\", \"label\": \"2\"}\n",
    "{\"text\": \"Que viene Nonpa y fijo es para mayores. Ya lloro.\", \"label\": \"0\"}\n",
    "{\"text\": \"@user pues s\\u00ed, s\\u00e9 lo que se siente. Pero pi\\u00e9nselo como el \\u00faltimo esfuerzo, que ya ahorita llegan las vacaciones\", \"label\": \"2\"}\n",
    "{\"text\": \"@user @user @user @user HAHAHAHHA MMM NOS VAMOS EN 2 semanas y no tenemos ni hotel Pa la primera noche \\ud83d\\ude02\\ud83d\\ude02\\ud83d\\ude02\", \"label\": \"0\"}\n",
    "{\"text\": \"@user Que bonito,y yo tengo una sorpresa para Ti ,que te gustara much\\u00edsimo!\", \"label\": \"2\"}\n",
    "{\"text\": \"No me sorprende que 2ne1 se haya separado, man. Pero a\\u00fan as\\u00ed me dio en el cora bc es un grupo legendario y empec\\u00e9 mi era kpop con ellas\", \"label\": \"0\"}\n",
    "{\"text\": \"@user pero yo te kiero\", \"label\": \"2\"}\n",
    "{\"text\": \"@user tu te planteas comprar gr\\u00e1fica. Yo si lo planteo tengo que comprar todo, mi pc es un venerable anciano\", \"label\": \"0\"}\n",
    "{\"text\": \"Cambiar \\\"El mejor sistema\\u2122\\\" y la educaci\\u00f3n m\\u00e9dica en Espa\\u00f1a ser\\u00e1 complicado, pero con gente como vosotros tengo fe  @user @user\", \"label\": \"2\"}\n",
    "{\"text\": \"@user @user fallecido hace meses. Hay q contrastar.\", \"label\": \"0\"}\n",
    "{\"text\": \"Que haya una planta de tamarindo afuera de la que posiblemente va a ser la casa en la que vivas de vez en cuando, es una buena se\\u00f1al\", \"label\": \"2\"}\n",
    "{\"text\": \"no me deis confianza que luego os digo las cosas muy claras y os jode\", \"label\": \"0\"}\n",
    "{\"text\": \"@user ay, gracias. Lo malo es que ese es solo para primer viaje. Pero mil gracias igualmente\", \"label\": \"2\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra Stuff, pt. 2.\n",
    "\n",
    "##### **Commented-Out Code from A3** (BERT & GPT-2 Pretrained Language Model-based Text-Classification Performance, Metrics, & Results)\n",
    "\n",
    "```python\n",
    "##### CS410: Natural Language Processing, Fall 2023 - 11/13/2023\n",
    "##### A3: Pretrained Language Models (PLMs), Dan Jang - Main Implementation\n",
    "#### Objective: Exploring Natural Language Processing (NLP), by building a text-classifier\n",
    "#### for a text classification task, predicting whether a piece of text is \"positive\" or \"negative.\"\n",
    "#### ...focusing on two (2) pretrained language models (PLMs),\n",
    "#### ...BERT (Bidirectional Encoder Representations from Transformers) & OpenAI's GPT-2 (Generative Pretrained Transformer),\n",
    "#### ...and using the respective toenizers to each PLM to perform the text-classification task as aforementioned\n",
    "\n",
    "### 1.1.a) Logistic Regression algorithm using sklearn.linear_model.LogisticRegression\n",
    "### https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "### Returns four (4) thingys:\n",
    "# I.) accuracy_score,\n",
    "# II.) f1_score,\n",
    "# III.) confusion_matrix,\n",
    "# & IV.) classification_report.\n",
    "def plm(model, xtrain, ytrain, xtest, ytest):\n",
    "    \n",
    "    lreg = LogisticRegression()\n",
    "    \n",
    "    lreg.fit(xtrain, ytrain)\n",
    "    predictionresults = lreg.predict(xtest)\n",
    "    \n",
    "    return accuracy_score(ytest, predictionresults), f1_score(ytest, predictionresults), confusion_matrix(ytest, predictionresults), classification_report(ytest, predictionresults)\n",
    "\n",
    "### A3-Specific: Implementation functions for implementing the two pretrained language models, BERT & GPT-2.\n",
    "## Pretrained Language Model (PLM) Tokenizer Implementation Function\n",
    "def plmodel(words, model, tokenizer):\n",
    "    if tokenizer.pad_token is None:\n",
    "        raise ValueError(\"[Debug A3.1 - PLModel()]: Tokenizer has no padding token for the current model!\")\n",
    "    wordlist = tokenizer(words, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        results = model(**wordlist)\n",
    "    \n",
    "    return results.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    # wordlist = words.split()\n",
    "    # vecs = [model[w] for w in wordlist if w in model]\n",
    "    # if vecs:\n",
    "    #     return sum(vecs) / len(vecs)\n",
    "    # else:\n",
    "    #     return [0] * model.vector_size\n",
    "    \n",
    "# ## GPT(ransformer)-2 Model Implementation Function\n",
    "# def bumblebee(words, model):\n",
    "#     beds = [avgwordvec(w, model) for w in words]\n",
    "#     return beds\n",
    "\n",
    "def main(): #trainfile, testfile):\n",
    "    print(\"Welcome, this is the main program for A3: Pretrained Language Models.\")\n",
    "    print(\"Written by Dan J. for CS410: Natural Language Processing, Fall 2023.\")\n",
    "    print(\"\\nWe will use two (2) pretrained language models (PLM), BERT & GPT-2.\\n...to create a text-classifier to guess negative or positive sentimentiality based on various text-reviews of products.\")\n",
    "\n",
    "    # 1.0.I.A) Debug Statements #1a for dataset loading times:\n",
    "    print(\"\\nLoading the training & testing datasets...\")\n",
    "    # with open(trainfile, \"r\") as trainfile:\n",
    "    with open(\"sentiment_train.json\", \"r\") as trainfile:\n",
    "        #traindata = json.load(trainfile)\n",
    "        for row in trainfile:\n",
    "            traindata.append(json.loads(row))\n",
    "        \n",
    "    trainframe = pandas.DataFrame(traindata)\n",
    "        \n",
    "    # with open(testfile, \"r\") as testfile:\n",
    "    with open(\"sentiment_test.json\", \"r\") as testfile:\n",
    "        #testdata = json.load(testfile)\n",
    "        for row in testfile:\n",
    "            testdata.append(json.loads(row))\n",
    "        \n",
    "    testframe = pandas.DataFrame(testdata)\n",
    "\n",
    "    # 1.0.I.B) Debug Statements #1b for dataset loading times:\n",
    "    print(\"Successfully loaded the training & testing datasets!\\n\")\n",
    "    \n",
    "    ## 1.0.1.) Initial Preprocessing of the training & testing data\n",
    "    ## First, we isolate our two (2) columns, \"review_title\" & \"stars.\"\n",
    "    ## Second, we will convert values in the \"stars\" column so that 1 [negative] = 0 & 5 [positive] = 1.\n",
    "    ## This will allow us to make the negative or positive sentiment a binary value-based thingy.\n",
    "    trainframe = trainframe[['review_title', 'stars']]\n",
    "    trainframe['stars'] = trainframe['stars'].apply(lambda x: 1 if x == 5 else 0)\n",
    "    \n",
    "    testframe = testframe[['review_title', 'stars']]\n",
    "    testframe['stars'] = testframe['stars'].apply(lambda x: 1 if x == 5 else 0)\n",
    "    \n",
    "    ## A3-Specific: From our Slack channel (#nlp_f23), using tip to only use 25% of training dataset, evenly split\n",
    "    ## Credits to Classmate Will McIntosh from the Slack thread started by classmate Saurav Kumar Singh\n",
    "    ## & also, full credits to classmate Will McIntosh for the following code for GPU usage:\n",
    "    \n",
    "    #### Credits to Will McIntosh (11/11/2023):\n",
    "    # Testing\n",
    "    print(f\"Is a GPU available: {torch.cuda.is_available()}\")\n",
    "    #print(f\"Is this instance using a GPU?: {next(model.parameters()).is_cuda}\")\n",
    "    #### From Slack, #nlp_f23.\n",
    "\n",
    "    trainframe = trainframe.sample(frac=1, random_state=69)\n",
    "    trainframe = trainframe.iloc[:int(0.25 * len(trainframe))]\n",
    "    \n",
    "    # y2train = trainframe['stars']\n",
    "    # print(\"[A3 Debug Size-Print #3] y2train\", len(y2train))\n",
    "    \n",
    "    ytest = testframe['stars']\n",
    "    # print(\"[A3 Debug Size-Print #4] ytest\", len(ytest))\n",
    "    \n",
    "    ## Evenly split frames\n",
    "    x3train1, x3train2 = train_test_split(trainframe, test_size=0.5, random_state=69)\n",
    "    \n",
    "    y3train1 = x3train1['stars']\n",
    "    y3train2 = x3train2['stars']\n",
    "    \n",
    "    #print(\"[A3 Debug Size-Print #1] x3train1 & x3train2\", len(x3train1), len(x3train2))\n",
    "    ## A3-Specific: Applying BERT & GPT-2 PLM Specific Tokenization\n",
    "    print(\"Tokenizing the training & testing datasets for BERT...\")\n",
    "    x2train1 = x3train1['review_title'].apply(lambda x: plmodel(x, bertie, bertie_tokens))\n",
    "    #x2train1 = trainframe['review_title'].apply(lambda x: plmodel(x, bertie, bertie_tokens))\n",
    "    x2test1 = testframe['review_title'].apply(lambda x: plmodel(x, bertie, bertie_tokens))\n",
    "    print(\"BERT Tokenization has been applied to its training & testing datasets!\")\n",
    "    \n",
    "    #print(\"[A3 Debug Size-Print #2a] x2train1 & x2test1\", len(x2train1), len(x2test1))\n",
    "    \n",
    "    print(\"Tokenizing the training & testing datasets for GPT-2...\")\n",
    "    x2train2 = x3train2['review_title'].apply(lambda x: plmodel(x, bumblebee, bumblebee_tokens))\n",
    "    #x2train2 = trainframe['review_title'].apply(lambda x: plmodel(x, bumblebee, bumblebee_tokens))\n",
    "    x2test2 = testframe['review_title'].apply(lambda x: plmodel(x, bumblebee, bumblebee_tokens))\n",
    "    print(\"GPT-2's tokenization has been applied to its training & testing datasets!\")\n",
    "    \n",
    "    #print(\"[A3 Debug Size-Print #2b] x2train2 & x2test2\", len(x2train2), len(x2test2))\n",
    "\n",
    "    ### 1.0.2b) Run Text-Classification Algorithms & Print the Model Results\n",
    "    print(\"-----\\n\")\n",
    "    print(\"Running text-classification model le training & testing datasets (with pretrained language models, BERT & GPT-2)...\")\n",
    "\n",
    "    \n",
    "    print(\"Running Logistic Regression algorithm, version A.) BERT...\")\n",
    "    bed1accuracy, bed1f1, bed1cmatrix, bed1creport = plm(bertie, x2train1.tolist(), y3train1, x2test1.tolist(), ytest)\n",
    "    print(\"..The data processing from our first PLM, BERT, is done!\")\n",
    "    \n",
    "    print(\"Running Logistic Regression algorithm, version B.) GPT-2...\")\n",
    "    bed2accuracy, bed2f1, bed2cmatrix, bed2creport = plm(bumblebee, x2train2.tolist(), y3train2, x2test2.tolist(), ytest)\n",
    "    print(\"..The data processing from our second PLM, GPT-2, is done!\")\n",
    "    \n",
    "    print(\"...All Done!\")\n",
    "    print(\"-----\\n\")\n",
    "    \n",
    "    print(\"Here are le results [Logistic Regression, with comparative results between two (2) PLMs, BERT & GPT-2]...\\n\")\n",
    "    print(\"Logistic Regression Algorithm, Version A: BERT Pretrained Language Model-based Text-Classification Performance, Metrics, & Results:\")\n",
    "    print(\"...Accuracy was found to be, \", bed1accuracy * percentness, \"%,\")\n",
    "    print(\"...F1 Score was found to be: \", bed1f1, \",\")\n",
    "    print(\"...with a Confusion Matrix: \\n\", bed1cmatrix, \",\")\n",
    "    print(\"...& lastly, the classification Report: \\n\", bed1creport)\n",
    "    print(\"-----\\n\")\n",
    "    \n",
    "    print(\"Logistic Regression Algorithm, Version B: GPT-2 Pretrained Language Model-based Text-Classification Performance, Metrics, & Results:\")\n",
    "    print(\"...Accuracy was found to be, \", bed2accuracy * percentness, \"%,\")\n",
    "    print(\"...F1 Score was found to be: \", bed2f1, \",\")\n",
    "    print(\"...with a Confusion Matrix: \\n\", bed2cmatrix, \",\")\n",
    "    print(\"...& lastly, the classification Report: \\n\", bed2creport)\n",
    "    print(\"-----\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "##### **A3 Results**: Raw Output from [*BERT*](https://arxiv.org/abs/1810.04805) & *OpenAI*'s [*GPT-2*](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) Pretrained Language Models (PLMs), using the Logistic Regression Algorithm\n",
    "    Here are le results [Logistic Regression, with comparative results between two (2) PLMs, BERT & GPT-2]...\n",
    "\n",
    "    Logistic Regression Algorithm, Version A: BERT Pretrained Language Model-based Text-Classification Performance, Metrics, & Results:\n",
    "    ...Accuracy was found to be,  91.55 %,\n",
    "    ...F1 Score was found to be:  0.9158785465405676 ,\n",
    "    ...with a Confusion Matrix: \n",
    "    [[911  89]\n",
    "    [ 80 920]] ,\n",
    "    ...& lastly, the classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.92      0.91      0.92      1000\n",
    "            1       0.91      0.92      0.92      1000\n",
    "\n",
    "        accuracy                           0.92      2000\n",
    "    macro avg       0.92      0.92      0.92      2000\n",
    "    weighted avg       0.92      0.92      0.92      2000\n",
    "\n",
    "    -----\n",
    "\n",
    "    Logistic Regression Algorithm, Version B: GPT-2 Pretrained Language Model-based Text-Classification Performance, Metrics, & Results:\n",
    "    ...Accuracy was found to be,  87.4 %,\n",
    "    ...F1 Score was found to be:  0.872598584428716 ,\n",
    "    ...with a Confusion Matrix: \n",
    "    [[885 115]\n",
    "    [137 863]] ,\n",
    "    ...& lastly, the classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.87      0.89      0.88      1000\n",
    "            1       0.88      0.86      0.87      1000\n",
    "\n",
    "        accuracy                           0.87      2000\n",
    "    macro avg       0.87      0.87      0.87      2000\n",
    "    weighted avg       0.87      0.87      0.87      2000\n",
    "\n",
    "-----\n",
    "\n",
    "### **A2 Results**: Raw Output from ```Word2Vec``` & GloVe Embedding Results, using the Logistic Regression Algorithm\n",
    "\n",
    "    Logistic Regression Algorithm, Version A: Word2Vec Pretrained Model-based Embeddings Performance, Metrics, & Results:\n",
    "    ...Accuracy was found to be,  86.3 %,\n",
    "    ...F1 Score was found to be:  0.86105476673428 ,\n",
    "    ...with a Confusion Matrix: \n",
    "    [[1754  246]\n",
    "    [ 302 1698]] ,\n",
    "    ...& lastly, the classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.85      0.88      0.86      2000\n",
    "            1       0.87      0.85      0.86      2000\n",
    "\n",
    "        accuracy                           0.86      4000\n",
    "    macro avg       0.86      0.86      0.86      4000\n",
    "    weighted avg       0.86      0.86      0.86      4000\n",
    "\n",
    "    -----\n",
    "\n",
    "    Logistic Regression Algorithm, Version B: GloVe Pretrained Model-based Embeddings Performance, Metrics, & Results:\n",
    "    ...Accuracy was found to be,  69.69999999999999 %,\n",
    "    ...F1 Score was found to be:  0.7313829787234042 ,\n",
    "    ...with a Confusion Matrix: \n",
    "    [[1138  862]\n",
    "    [ 350 1650]] ,\n",
    "    ...& lastly, the classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.76      0.57      0.65      2000\n",
    "            1       0.66      0.82      0.73      2000\n",
    "\n",
    "        accuracy                           0.70      4000\n",
    "    macro avg       0.71      0.70      0.69      4000\n",
    "    weighted avg       0.71      0.70      0.69      4000\n",
    "\n",
    "    -----\n",
    "\n",
    "#### ***From A1 Results for Reference:*** Initial Full 80k-Row Processing Results Raw Output\n",
    "\n",
    "    Algorithm #1, Version A: Gaussian N√§ive Bayes Performance, Metrics, & Results:\n",
    "    ...Accuracy was found to be,  59.199999999999996 %,\n",
    "    ...F1 Score was found to be:  0.3664596273291925 ,\n",
    "    ...with a Confusion Matrix: \n",
    "    [[948  52]\n",
    "    [764 236]] ,\n",
    "    ...& lastly, the classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.55      0.95      0.70      1000\n",
    "            1       0.82      0.24      0.37      1000\n",
    "\n",
    "        accuracy                           0.59      2000\n",
    "    macro avg       0.69      0.59      0.53      2000\n",
    "    weighted avg       0.69      0.59      0.53      2000\n",
    "\n",
    "    -----\n",
    "\n",
    "    Algorithm #2, Version A: Logistic Regression Performance, Metrics, & Results:\n",
    "    ...Accuracy was found to be,  92.7 %,\n",
    "    ...F1 Score was found to be:  0.9272908366533865 ,\n",
    "    ...with a Confusion Matrix: \n",
    "    [[923  77]\n",
    "    [ 69 931]] ,\n",
    "    ...& lastly, the classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.93      0.92      0.93      1000\n",
    "            1       0.92      0.93      0.93      1000\n",
    "\n",
    "        accuracy                           0.93      2000\n",
    "    macro avg       0.93      0.93      0.93      2000\n",
    "    weighted avg       0.93      0.93      0.93      2000\n",
    "\n",
    "    -----\n",
    "\n",
    "    Algorithm #1, Version B: Gaussian N√§ive Bayes Performance, Metrics, & Results:\n",
    "    ...Accuracy was found to be,  59.3 %,\n",
    "    ...F1 Score was found to be:  0.36899224806201547 ,\n",
    "    ...with a Confusion Matrix: \n",
    "    [[948  52]\n",
    "    [762 238]] ,\n",
    "    ...& lastly, the classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.55      0.95      0.70      1000\n",
    "            1       0.82      0.24      0.37      1000\n",
    "\n",
    "        accuracy                           0.59      2000\n",
    "    macro avg       0.69      0.59      0.53      2000\n",
    "    weighted avg       0.69      0.59      0.53      2000\n",
    "\n",
    "    -----\n",
    "\n",
    "    Algorithm #2, Version B: Logistic Regression Performance, Metrics, & Results:\n",
    "    ...Accuracy was found to be,  92.80000000000001 %,\n",
    "    ...F1 Score was found to be:  0.9281437125748503 ,\n",
    "    ...with a Confusion Matrix: \n",
    "    [[926  74]\n",
    "    [ 70 930]] ,\n",
    "    ...& lastly, the classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.93      0.93      0.93      1000\n",
    "            1       0.93      0.93      0.93      1000\n",
    "\n",
    "        accuracy                           0.93      2000\n",
    "    macro avg       0.93      0.93      0.93      2000\n",
    "    weighted avg       0.93      0.93      0.93      2000\n",
    "\n",
    "    -----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
