{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS410: Natural Language Processing, Fall 2023\n",
    "## A4: Multilingual Large Language Models (LLMs), Dan Jang - 11/27/2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Content Warning**: *Alike many \"natural language processing\" oriented or sources for text & data, the text & data used in this project may have been collected from public websites or sources, and may be unfiltered. Thus, some text & data may be disturbing, disagreeable, or otherwise objectionable.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of Assignment\n",
    "\n",
    "### Introduction\n",
    "As the training & testing datasets from our first & second assignments has become defunct recently ([*The Multilingual Amazon Reviews Corpus* by Phillip Keung, Yichao Lu, GyÃ¶rgy Szarvas, & Noah A. Smith (October 6th, 2020)](https://arxiv.org/abs/2010.02573) [1]), in this assignment, *A4: Multilingual Large Language Models (LLMs)*, we will be using a new dataset, the \"[*Unified Multilingual Sentiment Analysis Benchmark*](https://huggingface.co/datasets/cardiffnlp/tweet_sentiment_multilingual)\" [2] - where we will be exploring & comparing the performance of two, specific ***Large Language Models*** (**LLMs**):\n",
    "\n",
    "1. [*Meta*](https://ai.meta.com/blog/large-language-model-llama-meta-ai/)'s [***```LLaMA 2```***](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) (*Large Language Model Meta AI 2*) [3], [4]\n",
    "\n",
    "&\n",
    "\n",
    "2. [*OpenAI*](https://platform.openai.com/docs/models/)'s [***```ChatGPT```***](https://openai.com/blog/chatgpt) (As of November 27th, 2023, ***```ChatGPT```*** is currently running the November 6th, 2023 Update model version of *GPT-3.5 Turbo* [***```gpt-3.5-turbo-1106```***](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)) [5]-[7]\n",
    "\n",
    "Like the previous two assignment, this assignment focuses on using specific NLP models - specifically, [*Meta*](https://ai.meta.com/blog/large-language-model-llama-meta-ai/)'s [***```LLaMA 2```***](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) [3], [4] and [*OpenAI*](https://platform.openai.com/docs/models/)'s [***```ChatGPT```***](https://openai.com/blog/chatgpt) ***Large Language Models*** (**LLMs**) [5], [6].\n",
    "\n",
    "As a different approach in comparison to our previous three (3) assignments, in this current assignment, *A4*, instead of implementing a full text-classification model for sentiment-prediction, we will instead be exploring the techniques of [*prompt engineering*](https://arxiv.org/abs/2310.04438) [8] in creating a text-classifier - in comparison to previous assignment, *A3*, where we had used the monolingual *Pretrained Language Models* (***PLMs***) of [***```BERT```***](https://arxiv.org/abs/1810.04805) [9] and [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) [10].\n",
    "\n",
    "### Data Preparation\n",
    "In *A4*, we will be using a new dataset, [*Unified Multilingual Sentiment Analysis Benchmark*](https://huggingface.co/datasets/cardiffnlp/tweet_sentiment_multilingual) [2], which is a multilingual dataset with Tweets labeled with a ```sentiment``` value, where the classification labels are as follows:\n",
    "\n",
    "```0``` indicating *```negative```*,\n",
    "\n",
    "```1``` indicating *```neutral```*,\n",
    "\n",
    "...and a ```2``` indicating *```positive```*.\n",
    "\n",
    "The [*Unified Multilingual Sentiment Analysis Benchmark*](https://huggingface.co/datasets/cardiffnlp/tweet_sentiment_multilingual) [2] dataset contains sentiment-labeled Tweets in Arabic, English, French, German, Hindi, Italian, Portuguese, and Spanish.\n",
    "\n",
    "Instead of using several thousands of data-entries, for both a training and a testing dataset to train and test our text-classification model like we've explored in our previous assignments, of assignments *A1* through *A3* - in this assignment, *A4*, we will *instead* be utilizing a __much smaller subset of data__ from our [***new dataset***](https://huggingface.co/datasets/cardiffnlp/tweet_sentiment_multilingual) [2] & only utilizing a testing dataset based on a sampling of Tweet-entries across the different languages, in lieu of using both a testing and training dataset.\n",
    "\n",
    "Specifically, we will be creating a multilingual, Tweet-entry-based testing dataset, of a recommended amount of ~50 instances (the actual number used in my implementation will be specified below in my ***Text Classification Through Prompt Engineering*** section) of the testing dataset for each language as aforementioned, and sentimentality-wise, we will only look at the Tweet-entries with either the *```positive```* or *```negative```* sentiment labels, and will be ignoring Tweet-entries with the *```neutral```* sentiment label.\n",
    "\n",
    "Although no training will be required, we will still need to carefully prepare a balanced test set across both classes, and throughout our various languages sampled from the [***new dataset***](https://huggingface.co/datasets/cardiffnlp/tweet_sentiment_multilingual) [2].\n",
    "\n",
    "### Prompt Engineering\n",
    "\n",
    "Researcher Golam Md. Muktadir of the University of California, Santa Cruz, Computer Science and Engineering (muktadir@ucsc.edu) describes in good detail, the past-to-present history of *prompt engineering* in their relatively recent September 30th, 2023 paper, \"[*A Brief History of Prompt: Leveraging Language Models*](https://arxiv.org/abs/2310.04438)\" [8].\n",
    "\n",
    "Modern day *prompt engineering* can be described by the following description...\n",
    "\n",
    "\"...***researchers and practitioners [who] have explored various techniques to harness the full potential of language models, leveraging the power of prompts to guide, control, and tailor the output of these sophisticated AI systems.***...\"\n",
    "\n",
    "...where this *__very description__* of *prompt engineering* originates from a generated text borne of *prompt engineering* itself, as given by Researcher Muktadir in their paper's introduction, from these two prompts [8]:\n",
    "\n",
    "\"**Prompt #1**: *You are a scholar in machine learning and language models. I am writing a paper on the history of prompt engineering and generation. Can you give me a timeline for prompt engineering evolution? (We used this timeline to create prompts for each section later)*\"\n",
    "\n",
    "&\n",
    "\n",
    "\"**Prompt #2**: *Write the introduction of this paper. Emphasize that this paper focuses on how language prompts and queries have been used so far.*\"\n",
    "\n",
    "While the techniques of *prompt engineering*, seemingly only recently popularized, or rather, been made the spotlight mainstream subject of discussion of recent, in regard to the wide field of *Natural Language Processing* (***NLP***) - this popularization, of course, significantly due to the rise of [*OpenAI*](https://platform.openai.com/docs/models/)'s [***```ChatGPT```***](https://openai.com/blog/chatgpt) [6], [7] - however, nonetheless are a set of techniques a history long prior to this very novel advent of widely accessible platforms providing services based on *Natural Language Processing* (***NLP***) study & more specifically, through *Large Language Models* (***LLMs***) [8].\n",
    "\n",
    "As section-titled in their paper, Researcher Muktadir describes an early era of *prompt engineering* in \"*Prehistoric Prompting: Pre NN-Era*\", which describes among the most preliminary studies into what we know as *prompt engineering* today - as far back as the 1960s through the 1970s with \"*Early Natural Language Interfaces*\", the 1990s through the 2010s with \"*Advances in Natural Language Processing*\", e.g. *Neural Networks* (***NNs***) and *Machine Learning* (***ML***), to modern-day, featuring the rise of **Multilingual Large Language Models** (***MLLMs***) of those, we will exploring of two such ***MLLMs*** [8].\n",
    "\n",
    "\n",
    "### Multilingual Large Language Models (Multilingual LLMs, or 'MLLMs')\n",
    "\n",
    "One way we can/will be accessing our two **Multilingual Large Language Models** (***MLLMs***), [*Meta*](https://ai.meta.com/blog/large-language-model-llama-meta-ai/)'s [***```LLaMA 2```***](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) [3], [4] and [*OpenAI*](https://platform.openai.com/docs/models/)'s [***```ChatGPT```***](https://openai.com/blog/chatgpt) **Multilingual Large Language Models** (***MLLMs***) [5], [6], would be through the graphical interface chat platform provided through [*HuggingFace* Chat](https://huggingface.co/chat) [11].\n",
    "\n",
    "### Meta's Open-Source **LLaMA 2** (*Large Language Model Meta AI 2*) Multilingual Large Language Model (MLLM) [3], [4]\n",
    "The first ***MLLM*** we will be exploring will be the ***__open-source__***, [***```LLaMA 2```***](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) (*Large Language Model Meta AI 2*) [4] ***MLLM***, which was first co-released by [*Meta* and *Microsoft*](https://ai.meta.com/blog/llama-2/), back on June 17th, 2023 [14].\n",
    "\n",
    "This ***MLLM*** was first released & described as ***```LLaMA (Version 1)```*** in the paper, [\"*LLaMA: Open and Efficient Foundation Language Models*\"](https://arxiv.org/abs/2302.13971) [13] - which was first published earlier this year in February 27th, 2023, by the [*Meta AI*](https://ai.meta.com/blog/large-language-model-llama-meta-ai/) team [3] - authored by Researchers Hugo Touvron, and thirteen (13) other authors.\n",
    "\n",
    "This first version of ***```LLaMA```*** was described as \"*a collection of foundation language models ranging from 7B to 65B parameters*\", these models having been trained on \"*trillions of tokens*\", and importantly, claiming that *it is* and *was possible* to \"*train state-of-the-art models using publicly available datasets* **__exclusively__**\" - claiming, that the-then ***```LLaMA-13B```*** model outperformed the ***```GPT-3 (175B)```*** model on \"*most benchmarks*\" [13].\n",
    "\n",
    "The second iteration, ***```LLaMA 2```*** has been released as part of various ***fine-tuned LLMs***, e.g. *```LLaMA 2-Chat```*, *```Code LLaMA 2```*, etc. - as to represent the latest efforts by [*Meta* and *Microsoft*](https://ai.meta.com/blog/llama-2/) to provide their updated advancements of their open-source ***LLM*** - where, in their [**paper**](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/), they claim that the latest ***```LLaMA 2```*** ***LLM(s)*** are capable and *very* comparable in performance to those of other closed-source ***LLMs***, either in regard to a closed-source based set of techniques used to create the ***LLM(s)*** and/or a closed-source service that is provided as *SaaS* (***Software as a Service***), e.g. [*OpenAI*](https://platform.openai.com/docs/models/)'s\n",
    " [***```ChatGPT```***](https://openai.com/blog/chatgpt) [14], [4], [5], [6]. \n",
    "\n",
    "### OpenAI's Proprietary **ChatGPT** (*Chat Generative Pretrained Transformer*) Multilingual Large Language Model (MLLM) [5]-[7]\n",
    "The most famous ***MLLM*** model as of recent mainstream attention, is the proprietary model [***```ChatGPT```***](https://openai.com/blog/chatgpt) (based on *Generative Pretrained Transformer 3.5 Turbo*, currently on the November 6th, 2023 update-iteration, [***```gpt-3.5-turbo-1106```***](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)) is based off advancements made by [*OpenAI*](https://platform.openai.com/docs/models/) on the closed-source 'release' of [***```GPT-3```***](https://arxiv.org/abs/2005.14165), and of course, follows the progress made from the second predecessor, [***```GPT-2```***](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) [6], [7], [5], [15], [10].\n",
    "\n",
    "Recalling from *A3*:\n",
    "In ***```GPT-2```***'s [technical paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), *OpenAI* researchers Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever describes the ***```GPT-2```*** model - a ***__unidirectional__*** model trained on a dataset of *8 million web pages* with **1.5 billion parameters** ([*OpenAI*](https://openai.com/research/better-language-models), [Radford et al.](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) [10].\n",
    "\n",
    "However, if we were to make a quick GPT lineage comparison between ***```gpt-3.5-turbo-1106```*** vs. ***```GPT-2```***, of course, the former, ***```gpt-3.5-turbo-1106```***, has up to ***175 billion parameters*** & the latter, at least the largest-parameter version of ***```GPT-2```***, has still only ***1.5 billion parameters*** - representing an extremely significant increase in parameter-power in ***```gpt-3.5-turbo-1106```*** over its long predecessor ***```GPT-2```*** model.\n",
    "\n",
    "### Comparison of **LLaMA-2** vs. **ChatGPT** **Multilingual Large Language Models** (***MLLMs***) Architectures & Designs\n",
    "\n",
    "When comparing [**```LLaMA 2```**](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) vs. [**```ChatGPT```**](https://openai.com/blog/chatgpt) ([***```gpt-3.5-turbo-1106```***](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)), the first difference we can note is that, since [**```LLaMA 1```**](https://ai.meta.com/blog/large-language-model-llama-meta-ai/) was released & now with [**```LLaMA 2```**](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/), [**```LLaMA```**](https://arxiv.org/abs/2302.13971) has remained available as a series of ***open-source*** **Multilingual Large Language Models** (***MLLM***), as released & created by *Meta's AI Team* (& [*Microsoft* for version 2](https://ai.meta.com/blog/llama-2/)) - where anyone is able to request & receive access to freely download the model & its fine-tuned derivatives as per the nature open-source vs. the proprietary nature of [**```ChatGPT```**](https://openai.com/blog/chatgpt), which is only offered indirectly, as a **Software-as-a-Service** (***SaaS***) through the free version of *OpenAI*'s main *```ChatGPT```* *web-platform* ([chat.openai.com](https://chat.openai.com/)) or as a paid service through *OpenAI*'s [*API platform*](https://platform.openai.com/docs/models/) [4], [6], [7], [3], [13], [14], [6], [5].\n",
    "\n",
    "From September 1st, 2023, Enterprise AI Strategist Sunil Ramlochan of the *Prompt Engineering Institute* described in the summary of their article, [*How Does Llama-2 Compare to GPT-4/3.5 and Other AI Language Models*](https://promptengineering.org/how-does-llama-2-compare-to-gpt-and-other-ai-language-models/) that [**```LLaMA 2```**](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) \"*competes on accuracy despite lower complexity* [vs. the complexity of GPT-3.5]\" [16] - where such strong accuracy could be through *Meta*'s various improvements from [**```LLaMA 1```**](https://ai.meta.com/blog/large-language-model-llama-meta-ai/) to [**```LLaMA 2```**](https://ai.meta.com/blog/llama-2/), e.g. *Ghost Attention*, which may improve \"*dialogue context tracking*\" [16], [4], [14].\n",
    "\n",
    "As cited in [16], Researcher Waleed Kadous of *anyscale* performed an analysis of our two **Multilingual Large Language Models** (***MLLMs***), in their supportively titled (of the claims made in [16]) article, \"[*Llama 2 is about as factually accurate as GPT-4 for summaries and is 30X cheaper*](https://www.anyscale.com/blog/llama-2-is-about-as-factually-accurate-as-gpt-4-for-summaries-and-is-30x-cheaper?ref=promptengineering.org)\" [17]:\n",
    "\n",
    "![Comparative Analysis of GPT-4, GPT-3.5-Turbo (ChatGPT), vs. LLaMA 2, from [17] - as cited in [16]](GPT35-4-vs-LLaMa-2.png)\n",
    "\n",
    "\n",
    "### Text Classification Through Prompt Engineering\n",
    "To perform our text-classification task on these, we will __follow these steps__:\n",
    "\n",
    "1. Prepare a balanced test set of a specific, constant chosen-size of test Tweet-instances for each of the eight different languages from our new dataset, [*Unified Multilingual Sentiment Analysis Benchmark*](https://huggingface.co/datasets/cardiffnlp/tweet_sentiment_multilingual) [2]. Specifically, we need to pick either 'positive' or 'negative' classes, ergo, ignoring Tweet-instances with the 'neutral' label.\n",
    "\n",
    "In my implementation, I will be using a number of ***twenty-four (24)*** testing-instance / Tweets, per language - for a total of ***192*** testing-instances / Tweets (across our ***eight (8)*** languages).\n",
    "\n",
    "2. Using Prompt Engineering, we will have each of the ***Multilingual LLMs*** attempt to predict the sentiment, neither 'positive' nor 'negative', of each of the Tweet-instances in our test set, for each language.\n",
    "\n",
    "### Questions to Answer Through Results & Analysis\n",
    "We will, of course, be comparing the two ***Multilingual LLMs*** by the metrics such as the *F1-score* or other relevant metrics like from our previous assignments we've explored this lovely Fall term.\n",
    "\n",
    "Specific to this assignment, *A4*, we will be focusing on __answering the following questions__ through our exploration of the two aforementioned ***MLLMs*** - and the results and analysis from our exploration:\n",
    "\n",
    "1. \"*How do the two LLMs perform? Which one is better? Any possible explanation?*\"\n",
    "\n",
    "2. \"*Comparing the results across the six different languages, what do you observe? Any possible explanation?*\"\n",
    "\n",
    "3. \"*What challenges did you face?*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A4 - Libraries & Constants Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CS410: Natural Language Processing, Fall 2023 - 11/27/2023\n",
    "##### A4: Multilingual Large Language Models (LLMs), Dan Jang - Initializations: Libraries, Models, Data, & Constants\n",
    "\n",
    "### 0.) Libraries\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "# from sklearn.metrics import classification_report\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "#import torch\n",
    "import json# as jn\n",
    "#import pandas\n",
    "import os\n",
    "#import huggingface_hub\n",
    "import numpy# as np\n",
    "#import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A4 - The \"Multilingual And Normalization Generator from Original Set (M.A.N.G.O.S.)\" Function\n",
    "### The M.A.N.G.O.S. function was borne out of a desire to remain sane whilst having properly Python-parsed testing instances for each of our eight (8) language-based datasets, heh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multilingual And Normalization Generator from Original Set (M.A.N.G.O.S.) Function is now loading...\n",
      "parsed_arabic has been successfully parsed...\n",
      "parsed_english has been successfully parsed...\n",
      "parsed_french has been successfully parsed...\n",
      "parsed_german has been successfully parsed...\n",
      "parsed_hindi has been successfully parsed...\n",
      "parsed_italian has been successfully parsed...\n",
      "parsed_portuguese has been successfully parsed...\n",
      "parsed_spanish has been successfully parsed...\n",
      "Multilingual And Normalization Generator from Original Set (M.A.N.G.O.S.) Function has successfully parsed all the tweet-datasets!\n"
     ]
    }
   ],
   "source": [
    "##### CS410: Natural Language Processing, Fall 2023 - 11/30/2023\n",
    "##### A4: Multilingual Large Language Models (LLMs), Dan Jang - \"Multilingual And Normalization Generator from Original Set (M.A.N.G.O.S.)\"\n",
    "##### Just a cute name for a function that will parse .jsonl files properly, as for some reason, Visual Studio Code does not have any sorting extensions compatible with .jsonl files directly - at least for the raw formats of our new dataset, hehe.\n",
    "\n",
    "### A4: 1.) Multilingual And Normalization Generator from Original Set (M.A.N.G.O.S.) Function\n",
    "\n",
    "## A4: 1a.) Path & files to pre-processed testing instance-sets\n",
    "datapath = 'data/tweet_sentiment_multilingual/data/processed/processedv2'\n",
    "# tweetsets = [\n",
    "#     'processed_arabic.jsonl',\n",
    "#     'processed_english.jsonl',\n",
    "#     'processed_french.jsonl',\n",
    "#     'processed_german.jsonl',\n",
    "#     'processed_hindi.jsonl',\n",
    "#     'processed_italian.jsonl',\n",
    "#     'processed_portuguese.jsonl',\n",
    "#     'processed_spanish.jsonl'\n",
    "# ]\n",
    "langlist = [\n",
    "    'arabic',\n",
    "    'english',\n",
    "    'french',\n",
    "    'german',\n",
    "    'hindi',\n",
    "    'italian',\n",
    "    'portuguese',\n",
    "    'spanish'\n",
    "]\n",
    "\n",
    "print(\"Multilingual And Normalization Generator from Original Set (M.A.N.G.O.S.) Function is now loading...\")\n",
    "\n",
    "# A4: 1b.) Main data array for holding the mangos (parsed .jsonl files -> Python data arrays)\n",
    "mangos = {lang: [] for lang in langlist}\n",
    "\n",
    "# A4: 1c.) The main M.A.N.G.O. parser loop\n",
    "for lang in langlist:\n",
    "    lepath = os.path.join(datapath, f'processed_{lang}.jsonl')\n",
    "    \n",
    "    with open(lepath, 'r', encoding='utf-8') as mango:\n",
    "        # peel = line; mango = file; e.g., for each line in current processed_lang.jsonl file, etc.\n",
    "        for peel in mango:\n",
    "            tweet = jn.loads(peel)\n",
    "            if tweet[\"label\"] == \"1\":\n",
    "                print(\"uh oh, we got a mango with no taste! (a neutral tweet detected, whoopsie\")\n",
    "            else:\n",
    "                mangos[lang].append({\n",
    "                    'tweet': tweet[\"text\"],\n",
    "                    'actual-sentiment': tweet[\"label\"],\n",
    "                    'chatgpt-sentiment': None,\n",
    "                    'llama2-sentiment': None,\n",
    "                })\n",
    "\n",
    "# A4: 1d.) Box all the freshly peeled mangos into a nice & tidy, new Python file for implementation\n",
    "with open('tweety_mangos.py', 'w', encoding='utf-8') as candiedmangos:\n",
    "    for lang, tweet in mangos.items():\n",
    "        instancename = f'parsed_{lang}'\n",
    "        #outfile.write(f'{list_name} = {json.dumps(data, ensure_ascii=False, indent=4)}\\n\\n')\n",
    "        candiedmangos.write(f'{instancename} = {jn.dumps(tweet, ensure_ascii=False, indent=4)}\\n\\n')\n",
    "        print(f'{instancename} has been successfully parsed...')\n",
    "        \n",
    "print(\"Multilingual And Normalization Generator from Original Set (M.A.N.G.O.S.) Function has successfully parsed all the tweet-datasets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A4 - Prompt Engineering, Section I: ChatGPT Examples\n",
    "\n",
    "To perform the sentiment analysis, a series of similar, *engineered prompts* like shown below were used to get the Tweet-testing instance's text data sentiment analyzed by *```ChatGPT```*.\n",
    "\n",
    "Specifically, the prompt below was used to complete parts of each list of our parsed language testing-datasets, where the resultant output from prompts engineered were designed to produce a specifically formatted output, where we wanted *```ChatGPT```* to both perform sentiment analysis, and additionally, fill-in the sentiment rating within our parsed Python list format.\n",
    "\n",
    "Since *```ChatGPT```* does seem to have a tangible - but, at least a reasonably wide - *per-request* generation-output limit on its output messages (& where you have to click **```Continue Generating```**) - for each parsed language data list, e.g. ```parsed_arabic```, multiple prompts were used & resultant generated outputs combined cumulatively - **until**, *and* **for all**, Tweet-instances in each of our eight (8) language-based datasets were successully analyzed by *```ChatGPT```* & values for sentiment prediction rating were given *```ChatGPT```*.\n",
    "\n",
    "The *engineered prompt* resembled the following:\n",
    "\n",
    "**Prompt #1** - Me: \"...\n",
    "\n",
    "    I need you to perform sentiment prediction as a text classification model.\n",
    "\n",
    "    Fill in the \"chatgpt_sentiment\" values for each item in parsed_arabic, based your own analysis of the  sentiment of the text-data contained within each item's \"tweet\" key.\n",
    "    Specifically, fill in either a value of \"0\" for negative sentiment or a value of \"2\" for positive sentiment.\n",
    "\n",
    "    Here is the data for parsed_arabic:\n",
    "    ```py\n",
    "    parsed_arabic = [\n",
    "        {\n",
    "            \"tweet\": \"Ù†ÙˆØ§Ù„ Ø§Ù„Ø²ØºØ¨ÙŠ (Ø§Ù„Ø´Ø§Ø¨ Ø®Ø§Ù„Ø¯ Ù„ÙŠØ³ Ø¹Ø§Ù„Ù…ÙŠ) Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡ Ø£ØªÙØ±Ø¬ÙŠ Ø¹Ù„Ù‰ Ù‡Ø§ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙŠØ§ Ù…Ø¨ØªØ¯Ø¦Ø© http vÃ­a @user\",\n",
    "            \"chatgpt-sentiment\": None,\n",
    "            \"llama2-sentiment\": None\n",
    "        },\n",
    "        {\n",
    "            \"tweet\": \"Ù†ÙˆØ§Ù„ Ø§Ù„Ø²ØºØ¨ÙŠ Ù„Ø·ÙŠÙÙ‡ Ø§Ù„ÙÙ†Ø§Ù†Ù‡ Ø§Ù„ÙˆØ­ÙŠØ¯Ù‡ Ø§Ù„Ù„ÙŠ ÙƒÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙƒÙ„ÙŠØ¨Ø§Øª ØªØ¨Ø¹Ù‡Ø§ Ù…Ø§ØªØ³Ø¨Ø¨ ØªÙ„ÙˆØ« Ø¨ØµØ±ÙŠ ÙˆÙ„Ø§ Ø³Ù…Ø¹ÙŠ Ù„Ùˆ ØµÙˆØªÙ‡Ø§ Ø§Ù‚Ù„ Ù…Ù† Ø¹Ø§Ø¯ÙŠ\",\n",
    "            \"chatgpt-sentiment\": None,\n",
    "            \"llama2-sentiment\": None\n",
    "        },\n",
    "        {\n",
    "            \"tweet\": \"Ù„Ù…Ø§ Ù‚Ø§Ù„Øª Ù†ÙˆØ§Ù„ Ø§Ù„Ø²ØºØ¨ÙŠ Ù„Ø§Ø¨Ù‚Ù„Ù‡Ø§ Ù‡Ø§Ù„Ù„Ù‚Ø¨ ÙØ±Ø­ÙˆØ§ ÙØ§Ù†Ø²Ù‡Ø§ ğŸ˜‚ğŸ˜‚ğŸ˜‚ÙƒØ§Ù† Ù„Ø§Ø²Ù… ÙŠØ§Ø®Ø¯ÙˆÙ‡Ø§ Ø§Ù‡Ø§Ù†Ø© Ù…Ø´ Ø«Ù†Ø§Ø¡ http\",\n",
    "            \"chatgpt-sentiment\": None,\n",
    "            \"llama2-sentiment\": None\n",
    "        },\n",
    "        #... so on yes, until tweet/item[23]\n",
    "        {\n",
    "            \"tweet\": \"Ù†ÙˆØ§Ù„ Ø§Ù„Ø²ØºØ¨ÙŠ Ù…Ù† Ø§Ù„Ø­Ø¬Ø§Øª Ø§Ù„ÙŠ Ù‡ØªÙØ¶Ù„ Ø¬Ù…ÙŠÙ„Ù‡ Ø¹Ù„ÙŠ Ø·ÙˆÙ„...ğŸ˜ğŸ˜ğŸ˜ğŸ˜ http\",\n",
    "            \"chatgpt-sentiment\": None,\n",
    "            \"llama2-sentiment\": None\n",
    "        }\n",
    "    ]\n",
    "    ```\n",
    "...\"\n",
    "\n",
    "**Response** - [*```ChatGPT```*](https://chat.openai.com): \"...\n",
    "\n",
    "    Sure, I can help you with sentiment prediction. Please note that sentiment analysis can be subjective, and the following predictions are based on my interpretation of the given Arabic text.\n",
    "\n",
    "    parsed_arabic = [\n",
    "        {\n",
    "            \"tweet\": \"Ù†ÙˆØ§Ù„ Ø§Ù„Ø²ØºØ¨ÙŠ (Ø§Ù„Ø´Ø§Ø¨ Ø®Ø§Ù„Ø¯ Ù„ÙŠØ³ Ø¹Ø§Ù„Ù…ÙŠ) Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡ Ø£ØªÙØ±Ø¬ÙŠ Ø¹Ù„Ù‰ Ù‡Ø§ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙŠØ§ Ù…Ø¨ØªØ¯Ø¦Ø© http vÃ­a @user\",\n",
    "            \"chatgpt-sentiment\": 2,\n",
    "            \"llama2-sentiment\": None\n",
    "        },\n",
    "        {\n",
    "            \"tweet\": \"Ù†ÙˆØ§Ù„ Ø§Ù„Ø²ØºØ¨ÙŠ Ù„Ø·ÙŠÙÙ‡ Ø§Ù„ÙÙ†Ø§Ù†Ù‡ Ø§Ù„ÙˆØ­ÙŠØ¯Ù‡ Ø§Ù„Ù„ÙŠ ÙƒÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙƒÙ„ÙŠØ¨Ø§Øª ØªØ¨Ø¹Ù‡Ø§ Ù…Ø§ØªØ³Ø¨Ø¨ ØªÙ„ÙˆØ« Ø¨ØµØ±ÙŠ ÙˆÙ„Ø§ Ø³Ù…Ø¹ÙŠ Ù„Ùˆ ØµÙˆØªÙ‡Ø§ Ø§Ù‚Ù„ Ù…Ù† Ø¹Ø§Ø¯ÙŠ\",\n",
    "            \"chatgpt-sentiment\": 2,\n",
    "            \"llama2-sentiment\": None\n",
    "        },\n",
    "        {\n",
    "            \"tweet\": \"Ù„Ù…Ø§ Ù‚Ø§Ù„Øª Ù†ÙˆØ§Ù„ Ø§Ù„Ø²ØºØ¨ÙŠ Ù„Ø§Ø¨Ù‚Ù„Ù‡Ø§ Ù‡Ø§Ù„Ù„Ù‚Ø¨ ÙØ±Ø­ÙˆØ§ ÙØ§Ù†Ø²Ù‡Ø§ ğŸ˜‚ğŸ˜‚ğŸ˜‚ÙƒØ§Ù† Ù„Ø§Ø²Ù… ÙŠØ§Ø®Ø¯ÙˆÙ‡Ø§ Ø§Ù‡Ø§Ù†Ø© Ù…Ø´ Ø«Ù†Ø§Ø¡ http\",\n",
    "            \"chatgpt-sentiment\": 2,\n",
    "            \"llama2-sentiment\": None\n",
    "        },\n",
    "        {\n",
    "            \"tweet\": \"@user ØªØ°ÙƒØ±Ù†ÙŠ Ø¨Ø£ØºÙ†ÙŠØ© Ù†ÙˆØ§Ù„ Ø§Ù„Ø²ØºØ¨ÙŠ \\\"Ø¹ÙŠÙ†ÙŠÙƒ ÙƒØ¯Ø§Ø¨ÙŠÙ†\\\"\",\n",
    "            \"chatgpt-sentiment\": 2,\n",
    "            \"llama2-sentiment\": None\n",
    "        },\n",
    "        {\n",
    "            \"tweet\": \"Ø¨Ù„Ø§ ØªØ´ÙÙŠØ±- Ø£Ù…Ù„ Ø­Ù…Ø§Ø¯ÙŠ Ø¨ØªÙ†ØªÙ‚Ø¯ Ø§Ù„Ù†Ø¬Ù…Ø© Ù†ÙˆØ§Ù„ Ø§Ù„Ø²ØºØ¨ÙŠ:\\\" Ù†ØªÙŠ... http\",\n",
    "            \"chatgpt-sentiment\": 0,\n",
    "            \"llama2-sentiment\": None\n",
    "        },\n",
    "        {\n",
    "            \"tweet\": \"ÙÙ†Ø§Ù†Ø© Ù„Ø¨Ù†Ø§Ù†ÙŠØ© ÙƒØ¨ÙŠØ±Ø©  ØµÙˆØªÙ‡Ø§ Ø¥Ø³Ø·ÙˆØ±ÙŠ ØŸ! #Ù…Ø§Ø¬Ø¯Ø©_Ø§Ù„Ø±ÙˆÙ…ÙŠ   @user #Ù†Ø¬ÙˆÙ‰_ÙƒØ±Ù…  @user #Ù†ÙˆØ§Ù„_Ø§Ù„Ø²ØºØ¨ÙŠ  @user #Ø¬ÙˆÙ„ÙŠØ§_Ø¨Ø·Ø±Ø³\",\n",
    "            \"chatgpt-sentiment\": 2,\n",
    "            \"llama2-sentiment\": None\n",
    "        },\n",
    "        {\n",
    "            \"tweet\": \"#Ù„Ø¨Ù†Ø§Ù†ÙŠ_ÙŠÙ‚ÙŠÙ…_Ø¯ÙˆØ±Ù‡_Ù…ÙƒÙŠØ§Ø¬_Ø¨Ø§Ù„Ø±ÙŠØ§Ø¶Ù…Ø§Ù†ÙƒØ¨Ù†Ù‰ ØºÙŠØ± Ø¨Ø±Ø§Ø·Ù… Ù†ÙˆØ§Ù„ Ø§Ù„Ø²ØºØ¨ÙŠÙˆÙ†Ù‡ÙˆØ¯ Ø§Ù„ÙŠØ³Ø§ÙˆÙ…Ø¤Ø®Ø±Ø© Ù‡ÙŠÙØ§Ø¡ ÙˆÙ‡Ø¨ÙŠÙ‚Ù„Ù†Ø§Ù„ÙƒÙ… ÙƒÙ„Ù‡ Ù†ÙØ® ÙˆÙ‡ÙˆØ§Ø¨Ø³ğŸ˜‰ğŸ˜‰ ÙŠØ§Ø²ÙŠÙ† Ù†ÙØ®Ù‡Ù… ÙŠÙ„Ø·Ø´\",\n",
    "            \"chatgpt-sentiment\": 0,\n",
    "            \"llama2-sentiment\": None\n",
    "        },\n",
    "        #... so on yes, until tweet/item[23], or usually, a few of the last tweets/items would be incomplete due to the free-version rate-limiting, ergo, I'd then click continue generating\n",
    "...\"\n",
    "\n",
    "*Although*, it might be more long-term beneficial to simply create a set of functions, such that just generates a 24-element, one-line JSON line that contains all twenty-four (24) Tweet-instances per given language out of our eight (8) language testing-sets, I used the above method for the sake being able to more readily/visually verify that *```ChatGPT```* (or for *```LLaMA 2```*) is, hopefully not, generating mismatched sentiments or nonsensical sentiments.\n",
    "\n",
    "E.g. if some very obviously polarized Tweet or multiple super obviously one-sided Tweet-items, where the sentiment should be super on-the-nose, e.g. ```\"tweet\": \"Trying to have a conversation with my dad about vegetarianism is the most pointless infuriating thing ever #caveman \"```, being set to \"2\" as somehow a positive sentiment; if such occurs, I'd be able to hopefully spot out errors.\n",
    "\n",
    "However, besides that aspect, the aforementioned is a thought of possible long-term streamlining the process in regards to this specific task of Tweet sentiment analysis through *prompt engineering*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A4 - Prompt Engineering, Section II: LLaMA 2 Examples\n",
    "\n",
    "*```LLaMA 2```*, interestingly, was actually easier to work with & get more in-depth results from - yet, this clear advantage was massively overshadowed by an unfortunate aspect that the specific Chat-based instance of *```LLaMA 2```* to be used in this assignment kept on freezing, becoming unresponsive even with only a moderate to small input given-data in various kinds of engineered prompts, and has been completely down, service-wise, for the most of my attempts to use / check if it was back up, which heavily delayed my progress for my *A4* assignment - although, still, I apologize for my own delays nonetheless.\n",
    "\n",
    "As a compromise, due to [*```HuggingChat```*](https://huggingface.co/chat/) being unavailable for multiple hours at the time, I instead explored a specific version of *```LLaMA-2```*, the [*```LLaMA-2-7b-chat```*](https://huggingface.co/meta-llama/Llama-2-7b-chat) model [19], which, was also hosted on & one of [*````HuggingFace```*](https://huggingface.co/spaces/huggingface-projects/llama-2-7b-chat)'s [```huggingface-projects```](https://huggingface.co/huggingface-projects) [20], but of course, the [*```LLaMA-2-7b-chat```*] specific version, quantitatively has *ten-fold* less parameters than the model used by [*```HuggingChat```*](https://huggingface.co/chat), which is/was [*```meta-llama/Llama-2-70b-chat-hf```*](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf) [21] had offered & the lack of file-upload functionality is slightly a bummer.\n",
    "\n",
    "In that thought, initially, my *engineered prompts*  for *```LLaMA 2```* were much grander & had more input-text per prompt, e.g.:\n",
    "\n",
    "**Prompt #2a** - Me: \"...\n",
    "\n",
    "    Could you please perform sentiment analysis for each item in the following Python language testing data-lists, where there are eight different language-data lists, each containing 24 elements.\n",
    "\n",
    "    In each of these elements, there is a \"tweet\" key with a value correlating to a random Tweet message in the language of that given language testing data-lists. Using the text data of \"tweet\" in each of the elements, provide & set a sentiment rating, for each element based on its \"tweet\" text-data, in the \"llama2-sentiment\" key.\n",
    "\n",
    "    Specifically, provide & set the \"llama2-sentiment\" key-value to either a \"0\" for negative sentiment or a \"2\" for positive sentiment:\n",
    "    \n",
    "    ```py # pretend this represents an uploaded file of the parsed Python data lists, full_tweety_mangos.py\n",
    "    parsed_arabic = [\n",
    "        ...\n",
    "    ]\n",
    "\n",
    "    parsed_english = [\n",
    "        ...\n",
    "    ]\n",
    "\n",
    "    #... etc.\n",
    "    ```\n",
    "...\"\n",
    "\n",
    "However, using the alternative, [*```LLaMA-2-7b-chat```*](https://huggingface.co/meta-llama/Llama-2-7b-chat) model-chat [19], my *prompt engineering* trended back in resembling to the *piece-by-piece* sentiment analysis gathering strategy used for *```ChatGPT```* - however, I still used the same prompt as above, just that I was not able to do more than one whole language-testing instance-set of 24 Tweets at a time:\n",
    "\n",
    "**Prompt #2b** - Me: \"...\n",
    "\n",
    "    Could you please perform sentiment analysis for each item in the following Python language testing data-lists, where there are eight different language-data lists, each containing 24 elements.\n",
    "\n",
    "    In each of these elements, there is a \"tweet\" key with a value correlating to a random Tweet message in the language of that given language testing data-lists. Using the text data of \"tweet\" in each of the elements, provide & set a sentiment rating, for each element based on its \"tweet\" text-data, in the \"llama2-sentiment\" key.\n",
    "\n",
    "    Specifically, provide & set the \"llama2-sentiment\" key-value to either a \"0\" for negative sentiment or a \"2\" for positive sentiment:\n",
    "    \n",
    "    ```py\n",
    "    parsed_arabic = [\n",
    "        ...\n",
    "    ]\n",
    "\n",
    "![Engineered Prompt #2b for LLaMA-2 7B Chat [20]](PromptEngSectionII-Prompt2b.png)\n",
    "\n",
    "...\"\n",
    "\n",
    "**Response #2b** - *```LLaMA-2-7b-chat```*: \"...\n",
    "\n",
    "    Sure, I can assist you in assigning sentiment scores to each tweet in the provided list using the llama2-sentiment key. Here are the results:\n",
    "![Response from LLaMA-2 7B Chat [20]](PromptEngSectionII-Response2b.png)\n",
    "\n",
    "...\"\n",
    "\n",
    "*However*, I found another alternative for accessing [*```meta-llama/Llama-2-70b-chat-hf```*](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf) [21] whilst [*```HuggingChat```*](https://huggingface.co/chat/) [11] remained down (at 503 service error code) - which was through [llama2.ai](https://www.llama2.ai/) [22] which luckily - supposedly - also runs the ```70B```-illion parameter version of *```LLaMA 2```* - ...but that also broke during usage.\n",
    "\n",
    "So, I just was stuck with using [*```LLaMA-2-7b-chat```*](https://huggingface.co/meta-llama/Llama-2-7b-chat) model-chat [19]. Just in case, I am downloading the LLaMA 2 70B model directly in the case that even this ```7B``` parameter model-chat goes down as well.\n",
    "\n",
    "With the [*```LLaMA-2-7b-chat```*](https://huggingface.co/meta-llama/Llama-2-7b-chat) model-chat [19], I engineered a pretty consistent-output prompt that seemed to be effective for the piece-by-piece approach as aforementioned:\n",
    "\n",
    "**Prompt #3a** - Me: \"...\n",
    "\n",
    "    Please fill in the \"llama2-sentiment\" key in the following elements, based on your sentiment analysis of each element's text-data from the \"tweet\" element - specifically, fill in with either a value of \"0\" for a negative sentiment or a value of \"2\" for a positive sentiment:\n",
    "    ```py\n",
    "\n",
    "    ```\n",
    "    \n",
    "...\"\n",
    "\n",
    "**Example Response #3** - [*```LLaMA-2-7b-chat```*](https://huggingface.co/meta-llama/Llama-2-7b-chat): \"...\n",
    "\n",
    "![Example Response from LLaMA-2 7B Chat [20]](PromptEngSectionII-ExampleResponse3.png)\n",
    "\n",
    "\n",
    "Sometimes, [*```LLaMA-2-7b-chat```*](https://huggingface.co/meta-llama/Llama-2-7b-chat) seemed to be struggling to choose between a positive or negative sentiment, as it had given 'neutral' scores at times, which is not valid. Ergo, I tweaked my original **Prompt #3a** as follows:\n",
    "\n",
    "**Prompt #3b** - Me: \"...\n",
    "\n",
    "    Please fill in the \"llama2-sentiment\" key in the following elements, based on your sentiment analysis of each element's text-data from the \"tweet\" element - specifically, you must only fill in either a value of \"0\" for a negative sentiment or a value of \"2\" for a positive sentiment - there cannot be any neutral sentiment scores given:\n",
    "    ```py\n",
    "\n",
    "    ```\n",
    "    \n",
    "...\"\n",
    "\n",
    "As an additional *prompt engineering* technique, I used a 'cleaned' version of my language testing-instance sets, such that there are no pre-existing sentiment scores from *```ChatGPT```*, *```LLaMA 2```*, or the actual sentiment scores that may possibly influence the sentiment analysis outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A4 - Main Implementation: Intermodel, Multilingual F1-Score Metrics, Performance, & Analysis Function-Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language #1, Arabic: The Open-Sourced Challenger, Meta's LLaMA 2 MLLM - F1 Scores for Tweet-Sentimentality: 0.9230769231\n",
      "Language #1, Arabic: The Closed-Source Champion, OpenAI's ChatGPT MLLM - F1 Scores for Tweet-Sentimentality: 0.8000000000\n",
      "----\n",
      "\n",
      "Language #2, English: The Open-Sourced Challenger, Meta's LLaMA 2 MLLM - F1 Scores for Tweet-Sentimentality: 0.7272727273\n",
      "Language #2, English: The Closed-Source Champion, OpenAI's ChatGPT MLLM - F1 Scores for Tweet-Sentimentality: 0.9600000000\n",
      "----\n",
      "\n",
      "Language #3, French: The Open-Sourced Challenger, Meta's LLaMA 2 MLLM - F1 Scores for Tweet-Sentimentality: 0.4000000000\n",
      "Language #3, French: The Closed-Source Champion, OpenAI's ChatGPT MLLM - F1 Scores for Tweet-Sentimentality: 0.6086956522\n",
      "----\n",
      "\n",
      "Language #4, German: The Open-Sourced Challenger, Meta's LLaMA 2 MLLM - F1 Scores for Tweet-Sentimentality: 0.6666666667\n",
      "Language #4, German: The Closed-Source Champion, OpenAI's ChatGPT MLLM - F1 Scores for Tweet-Sentimentality: 0.8800000000\n",
      "----\n",
      "\n",
      "Language #5, Hindi: The Open-Sourced Challenger, Meta's LLaMA 2 MLLM - F1 Scores for Tweet-Sentimentality: 0.6000000000\n",
      "Language #5, Hindi: The Closed-Source Champion, OpenAI's ChatGPT MLLM - F1 Scores for Tweet-Sentimentality: 0.6666666667\n",
      "----\n",
      "\n",
      "Language #6, Italian: The Open-Sourced Challenger, Meta's LLaMA 2 MLLM - F1 Scores for Tweet-Sentimentality: 0.7272727273\n",
      "Language #6, Italian: The Closed-Source Champion, OpenAI's ChatGPT MLLM - F1 Scores for Tweet-Sentimentality: 0.8333333333\n",
      "----\n",
      "\n",
      "Language #7, Portuguese: The Open-Sourced Challenger, Meta's LLaMA 2 MLLM - F1 Scores for Tweet-Sentimentality: 0.8571428571\n",
      "Language #7, Portuguese: The Closed-Source Champion, OpenAI's ChatGPT MLLM - F1 Scores for Tweet-Sentimentality: 0.8000000000\n",
      "----\n",
      "\n",
      "Language #8, Spanish: The Open-Sourced Challenger, Meta's LLaMA 2 MLLM - F1 Scores for Tweet-Sentimentality: 0.7000000000\n",
      "Language #8, Spanish: The Closed-Source Champion, OpenAI's ChatGPT MLLM - F1 Scores for Tweet-Sentimentality: 0.8000000000\n",
      "----\n",
      "\n",
      "Average LLaMA 2 F1-Score: 0.7001789877\n",
      "Average ChatGPT F1-Score: 0.7935869565\n"
     ]
    }
   ],
   "source": [
    "##### CS410: Natural Language Processing, Fall 2023 - 11/27/2023\n",
    "##### A4: Multilingual Large Language Models (LLMs)\n",
    "#### Objective: Exploring Natural Language Processing (NLP),\n",
    "#### for a text classification task, predicting whether a piece of text is \"positive\" or \"negative.\"\n",
    "#### ...focusing on two (2) Multilingual Large Language Models (MLLMs),\n",
    "#### ...Meta's LLaMA 2 [4] & OpenAI's ChatGPT (gpt-3.5-turbo-1106) [6],\n",
    "#### ...and using prompt engineering to perform the classification task on eight (8) language-based, Tweet-testing-datasets.\n",
    "\n",
    "# 3a.) Initialize tweet-index / language-dictionary\n",
    "#rosetta_tablet = {\n",
    "    # 'parsed_arabic': parsed_arabic,\n",
    "    # 'parsed_english': parsed_english,\n",
    "    # 'parsed_french': parsed_french,\n",
    "    # 'parsed_german': parsed_german,\n",
    "    # 'parsed_hindi': parsed_hindi,\n",
    "    # 'parsed_italian': parsed_italian,\n",
    "    # 'parsed_portuguese': parsed_portuguese,\n",
    "    # 'parsed_spanish': parsed_spanish\n",
    "#}\n",
    "\n",
    "#langlist = {\n",
    "rosetta_tablet = {\n",
    "    'Arabic': parsed_arabic,\n",
    "    'English': parsed_english,\n",
    "    'French': parsed_french,\n",
    "    'German': parsed_german,\n",
    "    'Hindi': parsed_hindi,\n",
    "    'Italian': parsed_italian,\n",
    "    'Portuguese': parsed_portuguese,\n",
    "    'Spanish': parsed_spanish\n",
    "}\n",
    "\n",
    "# 3b.) Tallying up the sentiment scores!\n",
    "#actualsentiments = [tweet['actual-sentiment']]\n",
    "\n",
    "# 3c.) F1-score functions!\n",
    "def sentimental_values(tweets, actualsentiments):\n",
    "    \n",
    "    actual = [int(tweet['actual-sentiment']) // 2 for tweet in tweets] # mitosis lol\n",
    "    guesses = [int(tweet[actualsentiments]) // 2 for tweet in tweets]\n",
    "    \n",
    "    return f1_score(actual, guesses)\n",
    "\n",
    "def tally(iPad):\n",
    "    llama2_f1_scoreboard = {}\n",
    "    chatgpt_f1_scoreboard = {}\n",
    "    \n",
    "    for lang, tweets in iPad.items():\n",
    "        llama2_f1_scoreboard[lang] = sentimental_values(tweets, 'llama2-sentiment')\n",
    "        chatgpt_f1_scoreboard[lang] = sentimental_values(tweets, 'chatgpt-sentiment')\n",
    "    \n",
    "    idx = 1\n",
    "    for lang in iPad.keys():\n",
    "        #for langy in langz:\n",
    "        print(f\"Language #{idx}, {lang}: The Open-Sourced Challenger, Meta's LLaMA 2 MLLM - F1 Scores for Tweet-Sentimentality: {llama2_f1_scoreboard[lang]:.10f}\")\n",
    "        print(f\"Language #{idx}, {lang}: The Closed-Source Champion, OpenAI's ChatGPT MLLM - F1 Scores for Tweet-Sentimentality: {chatgpt_f1_scoreboard[lang]:.10f}\")\n",
    "        print(\"----\\n\")\n",
    "        idx = idx + 1\n",
    "        \n",
    "        \n",
    "    average_llama2_f1score = sum(llama2_f1_scoreboard.values()) / len(llama2_f1_scoreboard)\n",
    "    print(f\"Average LLaMA 2 F1-Score: {average_llama2_f1score:.10f}\")\n",
    "    \n",
    "    average_chatgpt_f1score = sum(chatgpt_f1_scoreboard.values()) / len(chatgpt_f1_scoreboard)\n",
    "    print(f\"Average ChatGPT F1-Score: {average_chatgpt_f1score:.10f}\")\n",
    "\n",
    "\n",
    "tally(rosetta_tablet)\n",
    "\n",
    "\n",
    "# for lang, tweets in rosetta_tablet.items():\n",
    "#     tally(lang, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *A4: Multilingual Large Language Models (MLLMs)* - Results and Analysis: Meta's LLaMA-2 vs. OpenAI's ChatGPT!\n",
    "\n",
    "### Inter-Multilingual Performance & Analysis\n",
    "Let us first compare language-by-language, the performance differences of [**```LLaMA 2```**](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) [4] vs. [**```ChatGPT```**](https://openai.com/blog/chatgpt) ([***```gpt-3.5-turbo-1106```***](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)) [6], [7]:\n",
    "\n",
    "From our *first* language-Tweet results: **Arabic**:\n",
    "\n",
    "    Language #1, Arabic: The Open-Sourced Challenger, Meta's LLaMA 2 MLLM - F1 Scores for Tweet-Sentimentality: 0.9230769231\n",
    "    Language #1, Arabic: The Closed-Source Champion, OpenAI's ChatGPT MLLM - F1 Scores for Tweet-Sentimentality: 0.8000000000\n",
    "\n",
    "...We see that [***```LLaMA 2```***](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) [4] is the winner, by **~0.123**.\n",
    "\n",
    "These F1-scores for both models represent among the highest F1-scores for both models, which is interesting.\n",
    "\n",
    "Perhaps, the high F1-score of 0.92307 seen of [**```LLaMA 2```**](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) [4] indicates it may have had more training with Arabic texts & data.\n",
    "\n",
    "\n",
    "From our *second* language-Tweet results: **English**:\n",
    "\n",
    "    Language #2, English: The Open-Sourced Challenger, Meta's LLaMA 2 MLLM - F1 Scores for Tweet-Sentimentality: 0.7272727273\n",
    "    Language #2, English: The Closed-Source Champion, OpenAI's ChatGPT MLLM - F1 Scores for Tweet-Sentimentality: 0.9600000000\n",
    "\n",
    "...We see that [***```ChatGPT```***](https://openai.com/blog/chatgpt) ([***```gpt-3.5-turbo-1106```***](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)) [6], [7] is the winner, by **~0.233**.\n",
    "\n",
    "Interestingly, the F1-score of 0.72727 seen of [**```LLaMA 2```**](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) which seems slightly odd, but maybe that is just a Western society-bias-belief kind of thing to expect.\n",
    "\n",
    "\n",
    "From our *third* language-Tweet results: **French**:\n",
    "\n",
    "    Language #3, French: The Open-Sourced Challenger, Meta's LLaMA 2 MLLM - F1 Scores for Tweet-Sentimentality: 0.4000000000\n",
    "    Language #3, French: The Closed-Source Champion, OpenAI's ChatGPT MLLM - F1 Scores for Tweet-Sentimentality: 0.6086956522\n",
    "\n",
    "...We see that [***```ChatGPT```***](https://openai.com/blog/chatgpt) ([***```gpt-3.5-turbo-1106```***](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)) [6], [7] is the 'winner', by **~0.208** - but relatively, they both sucked.\n",
    "\n",
    "The language of love, neither [***```LLaMA 2```***](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) [4] nor [***```ChatGPT```***](https://openai.com/blog/chatgpt) ([***```gpt-3.5-turbo-1106```***](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)) [6], [7] speak ***at all fluently***.\n",
    "But, [***```ChatGPT```***](https://openai.com/blog/chatgpt) ([***```gpt-3.5-turbo-1106```***](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)), in comparison, speaks at least with a F1-score of ~0.608.\n",
    "\n",
    "\n",
    "From our *fourth* language-Tweet results: **German**:\n",
    "\n",
    "    Language #4, German: The Open-Sourced Challenger, Meta's LLaMA 2 MLLM - F1 Scores for Tweet-Sentimentality: 0.6666666667\n",
    "    Language #4, German: The Closed-Source Champion, OpenAI's ChatGPT MLLM - F1 Scores for Tweet-Sentimentality: 0.8800000000\n",
    "\n",
    "...We see that [***```ChatGPT```***](https://openai.com/blog/chatgpt) ([***```gpt-3.5-turbo-1106```***](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)) [6], [7] is the winner, by **~0.213**.\n",
    "\n",
    "\n",
    "From our *fifth* language-Tweet results: **Hindi**:\n",
    "\n",
    "    Language #5, Hindi: The Open-Sourced Challenger, Meta's LLaMA 2 MLLM - F1 Scores for Tweet-Sentimentality: 0.6000000000\n",
    "    Language #5, Hindi: The Closed-Source Champion, OpenAI's ChatGPT MLLM - F1 Scores for Tweet-Sentimentality: 0.6666666667\n",
    "\n",
    "...We see that [***```ChatGPT```***](https://openai.com/blog/chatgpt) ([***```gpt-3.5-turbo-1106```***](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)) [6], [7] is the winner, by **~0.067**. However, it appears both LLMs are sort-of around the same range in terms of how close the F1-scores are to each other.\n",
    "\n",
    "\n",
    "From our *sixth* language-Tweet results: **Italian**:\n",
    "\n",
    "    Language #6, Italian: The Open-Sourced Challenger, Meta's LLaMA 2 MLLM - F1 Scores for Tweet-Sentimentality: 0.7272727273\n",
    "    Language #6, Italian: The Closed-Source Champion, OpenAI's ChatGPT MLLM - F1 Scores for Tweet-Sentimentality: 0.8333333333\n",
    "\n",
    "...We see that [***```ChatGPT```***](https://openai.com/blog/chatgpt) ([***```gpt-3.5-turbo-1106```***](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)) [6], [7] is the winner, by **~0.106**.\n",
    "\n",
    "\n",
    "From our *seventh* language-Tweet results: **Portuguese**:\n",
    "\n",
    "    Language #7, Portuguese: The Open-Sourced Challenger, Meta's LLaMA 2 MLLM - F1 Scores for Tweet-Sentimentality: 0.8571428571\n",
    "    Language #7, Portuguese: The Closed-Source Champion, OpenAI's ChatGPT MLLM - F1 Scores for Tweet-Sentimentality: 0.8000000000\n",
    "\n",
    "...We see that [***```LLaMA 2```***](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) [4] is the winner, by **~0.057**.\n",
    "\n",
    "\n",
    "From our *eighth* language-Tweet results: **Spanish**:\n",
    "\n",
    "    Language #8, Spanish: The Open-Sourced Challenger, Meta's LLaMA 2 MLLM - F1 Scores for Tweet-Sentimentality: 0.7000000000\n",
    "    Language #8, Spanish: The Closed-Source Champion, OpenAI's ChatGPT MLLM - F1 Scores for Tweet-Sentimentality: 0.8000000000\n",
    "\n",
    "...We see that [***```ChatGPT```***](https://openai.com/blog/chatgpt) ([***```gpt-3.5-turbo-1106```***](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)) [6], [7] is the winner, **~0.100**.\n",
    "\n",
    "\n",
    "### Overall Average F1-Score Performance Analysis\n",
    "\n",
    "Averaged F1-scores for each LLM, over F1-scores across all eight (8) language-Tweet testing instances yielded:\n",
    "\n",
    "    Average LLaMA 2 F1-Score: 0.7001789877\n",
    "    Average ChatGPT F1-Score: 0.7935869565\n",
    "\n",
    "...Both in a relatively alright ballpack - with a **~0.0935** lead by [***```ChatGPT```***](https://openai.com/blog/chatgpt) ([***```gpt-3.5-turbo-1106```***](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)) [6], [7] - but for both **M***LLMs*, through our exploration of these models in this *A4* assignment, we do see that these are ***Multilingual*** *Large Language Models*, indeed!\n",
    "\n",
    "\n",
    "### Multilingual Large Language Model (MLLM) Comparative Analysis & Discussion\n",
    "\n",
    "It is very exciting to see an *open-sourced* model be *very much* in competition with the most *state-of-the-art*, *closed-source* models such as with [***```ChatGPT```***](https://openai.com/blog/chatgpt) ([***```gpt-3.5-turbo-1106```***](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)) [6], [7]!\n",
    "\n",
    "That [***```LLaMA 2```***](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) [4] competes with [***```ChatGPT```***](https://openai.com/blog/chatgpt) ([***```gpt-3.5-turbo-1106```***](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)) [6], [7] at this kind of *multilingual* level & performance, especially, was really awesome to see!\n",
    "\n",
    "### Results and Analysis: Le Questions\n",
    "\n",
    "1. \"*How do the two LLMs perform? Which one is better? Any possible explanation?*\"\n",
    "\n",
    "    As discussed in the introductory sections, one of the most prominent differences in these *Large Language Models* (***LLMs***) is that [***```LLaMA 2```***](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) [4] is an *open-source* model, while [***```ChatGPT```***](https://openai.com/blog/chatgpt) ([***```gpt-3.5-turbo-1106```***](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)) [6], [7] is a *closed-source* model.\n",
    "\n",
    "    Surely, performance by pursuit of profit, is not the only factor in the *slight* edge that [***```ChatGPT```***](https://openai.com/blog/chatgpt) ([***```gpt-3.5-turbo-1106```***](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)) [6], [7] has shown over its 100% free, open-source [***```LLaMA 2```***](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) [4] model - however, with more funding & profit, may have led to a much greater ability for *Research & Development* of *GPT* models over time, where speaking of, *OpenAI* has been long developing its *GPT lineage*. \n",
    "\n",
    "2. \"*Comparing the results across the six different languages, what do you observe? Any possible explanation?*\"\n",
    "\n",
    "    I think two most interesting things noted were firstly, the Arabic prowess of [***```LLaMA 2```***](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) [4] over [***```ChatGPT```***](https://openai.com/blog/chatgpt) ([***```gpt-3.5-turbo-1106```***](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)) [6], [7], and secondly - rightfully so, the French language being the most difficult for both our ***MLLMs*** to predict sentiment, a hope that perhaps some things still remain just a little bit within the creativity of humans.\n",
    "\n",
    "    As briefly mentioned, I think the most obvious answer for [***```LLaMA 2```***](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) [4] having a edge over [***```ChatGPT```***](https://openai.com/blog/chatgpt) ([***```gpt-3.5-turbo-1106```***](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)) [6], [7] in the Arabic language may have been greater exposure to Arabic texts & training data. Or it could also have been a statistical fluke, too.\n",
    "\n",
    "3. \"*What challenges did you face?*\"\n",
    "\n",
    "    Using a GUI for genuine *Natural Language Processing* (***NLP***) projects & analysis, was very surprisingly one of the biggest challenges. A limitation & an almost felt-delay of the simulated human-typing, letter-crawling as you wait for one of your 50+ prompts to finish being outputted by either GUI of *```ChatGPT```* or *```LLaMA 2```* was a bit of a challenge.\n",
    "\n",
    "    However, the biggest challenge, I think, was the unfortunate, massive delay caused due to *```HuggingChat```* being down - but *```HuggingFace```* is still & has been a wonderful, great (and very vital) resource in the realm of ***NLP***, CS, & in general, a super cool resource - so I think it was a fluke, but one that was a bit significant & caused a lot of stress, as I tried to scramble a way to get our last assignment done - alas, we survive. Hopefully.\n",
    "\n",
    "### Prompt Engineering Challenges & Limitations\n",
    "\n",
    "Even with great, simple, & pinpoint prompt engineering - I still saw that outputs still can & did vary wildly for no reason. However, this was only really present when using the [*```LLaMA-2-7b-chat```*](https://huggingface.co/meta-llama/Llama-2-7b-chat) - if I had access to set the ***temperature*** or other settings through the API, I think such output-variational issues *could* have been mitigated slightly - yet, it was not a significant challenge.\n",
    "\n",
    "A limitation in the literal sense, however, was that even with fairly simple text-classification or parsing-related prompts, a ratelimit or limit to the *'generatable output-in-one-go'* were present in both LLM platforms, where using an automated API does seem, if we instead used, *A4* may not have been as much of a struggle - however, the struggle was worth it, I think. I can say I know a little about how *ChatGPT* works now, heheh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References & Resources\n",
    "\n",
    "## Libraries, Dependencies, & Credits\n",
    "\n",
    "[HuggingFace_hub](https://huggingface.co/docs/hub/models-libraries)\n",
    "\n",
    "[*HuggingFace*](https://huggingface.co/docs/hub/models-libraries)'s [*transformers*](https://huggingface.co/docs/transformers/index)\n",
    "\n",
    "[sklearn.metrics.f1_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)\n",
    "\n",
    "[nbconvert](https://nbconvert.readthedocs.io/en/latest/)\n",
    "\n",
    "Credits to GitHub Copilot & ChatGPT for code implementation assistance.\n",
    "\n",
    "\n",
    "## IEEE Formatted References\n",
    "\n",
    "[1] [P. Keung, Y. Lu, G. Szarvas, and N. A. Smith, â€œThe Multilingual Amazon Reviews Corpus.â€ arXiv, Oct. 06, 2020. doi: 10.48550/arXiv.2010.02573.](https://arxiv.org/abs/2010.02573)\n",
    "\n",
    "[2] [\"cardiffnlp/tweet_sentiment_multilingual Â· Datasets at Hugging Face.\" [Online]. Available: https://huggingface.co/datasets/cardiffnlp/tweet_sentiment_multilingual](https://huggingface.co/datasets/cardiffnlp/tweet_sentiment_multilingual)\n",
    "\n",
    "[3] [\"Introducing LLaMA: A foundational, 65-billion-parameter language model.\" [Online]. Available: https://ai.meta.com/blog/large-language-model-llama-meta-ai/](https://ai.meta.com/blog/large-language-model-llama-meta-ai/)\n",
    "\n",
    "[4] [\"Llama 2: Open Foundation and Fine-Tuned Chat Models | Research - AI at Meta.\" [Online]. Available: https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)\n",
    "\n",
    "[5] [\"OpenAI Platform.\" [Online]. Available: https://platform.openai.com](https://platform.openai.com/docs/models/)\n",
    "\n",
    "[6] [\"Introducing ChatGPT.\" [Online]. Available: https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt)\n",
    "\n",
    "[7] [\"New models and developer products announced at DevDay.\" [Online]. Available: https://openai.com/blog/new-models-and-developer-products-announced-at-devday](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)\n",
    "\n",
    "[8] [G. M. Muktadir, \"A Brief History of Prompt: Leveraging Language Models. (Through Advanced Prompting).\" arXiv, Nov. 28, 2023. doi: 10.48550/arXiv.2310.04438.](https://arxiv.org/abs/2310.04438)\n",
    "\n",
    "[9] [J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\" arXiv, May 24, 2019. doi: 10.48550/arXiv.1810.04805.](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "[10] [A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, \"Language Models are Unsupervised Multitask Learners\".](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "\n",
    "[11] [\"HuggingChat.\" [Online]. Available: https://huggingface.co/chat](https://huggingface.co/chat)\n",
    "\n",
    "[12] [\"ğŸ¤— Transformers.\" [Online]. Available: https://huggingface.co/docs/transformers/index](https://huggingface.co/docs/transformers/index)\n",
    "\n",
    "[13] [H. Touvron et al., \"LLaMA: Open and Efficient Foundation Language Models.\" arXiv, Feb. 27, 2023. doi: 10.48550/arXiv.2302.13971.](https://arxiv.org/abs/2302.13971)\n",
    "\n",
    "[14] [\"Meta and Microsoft Introduce the Next Generation of Llama.\" [Online]. Available: https://ai.meta.com/blog/llama-2/](https://ai.meta.com/blog/llama-2/)\n",
    "\n",
    "[15] [T. B. Brown et al., \"Language Models are Few-Shot Learners.\" arXiv, Jul. 22, 2020. doi: 10.48550/arXiv.2005.14165.](https://arxiv.org/abs/2005.14165)\n",
    "\n",
    "[16] [\"How Does Llama-2 Compare to GPT-4/3.5 and Other AI Language Models,\" Prompt Engineering Institute. [Online]. Available: https://promptengineering.org/how-does-llama-2-compare-to-gpt-and-other-ai-language-models/](https://promptengineering.org/how-does-llama-2-compare-to-gpt-and-other-ai-language-models/)\n",
    "\n",
    "[17] [\"Llama 2 vs. GPT-4: Nearly As Accurate and 30X Cheaper,\" Anyscale. [Online]. Available: https://www.anyscale.com/blog/llama-2-is-about-as-factually-accurate-as-gpt-4-for-summaries-and-is-30x-cheaper?ref=promptengineering.org](https://www.anyscale.com/blog/llama-2-is-about-as-factually-accurate-as-gpt-4-for-summaries-and-is-30x-cheaper?ref=promptengineering.org)\n",
    "\n",
    "[18] [A. Arshad, \"How to Use Llama 2 from Hugging Face to Solve Real-World Problems,\" Medium. [Online]. Available: https://medium.com/@anasarshad22/how-to-use-llama-2-from-hugging-face-to-solve-real-world-problems-f0216e32ef](https://medium.com/@anasarshad22/how-to-use-llama-2-from-hugging-face-to-solve-real-world-problems-f0216e32ef62)\n",
    "\n",
    "[19] [\"meta-llama/Llama-2-7b-chat Â· Hugging Face.\" [Online]. Available: https://huggingface.co/meta-llama/Llama-2-7b-chat](https://huggingface.co/meta-llama/Llama-2-7b-chat)\n",
    "\n",
    "[20] [\"Llama 2 7B Chat - a Hugging Face Space by huggingface-projects.\" [Online]. Available: https://huggingface.co/spaces/huggingface-projects/llama-2-7b-chat](https://huggingface.co/spaces/huggingface-projects/llama-2-7b-chat)\n",
    "\n",
    "[21] [\"meta-llama/Llama-2-70b-chat-hf Â· Hugging Face.\" [Online]. Available: https://huggingface.co/meta-llama/Llama-2-70b-chat-hf](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf)\n",
    "\n",
    "[22] [\"Chat with Llama 2.\" [Online]. Available: https://www.llama2.ai/](https://www.llama2.ai/)\n",
    "\n",
    "\n",
    "## Special Thanks\n",
    "\n",
    "A huge shoutout to our awesome & wonderful TA Bahareh G. for their patience in dealing with my delays & for having to read my often super long written analysis / my messy code!\n",
    "\n",
    "Thanks & credits to my fellow classmate Ahmad A. for tips & suggestions on making a Python-specific data array for a more intuitive method to calculate metrics in the Jupyter Notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Stuff, pt. 1.\n",
    "\n",
    "##### **The Eight (8) Raw A4 Pre-Processing Testing Language Tweet-Instance-Sets from the New Dataset [2]**\n",
    "\n",
    "##### Dataset Source: [2] [\"cardiffnlp/tweet_sentiment_multilingual Â· Datasets at Hugging Face.\" [Online]. Available: https://huggingface.co/datasets/cardiffnlp/tweet_sentiment_multilingual](https://huggingface.co/datasets/cardiffnlp/tweet_sentiment_multilingual)\n",
    "\n",
    "#### **Content Warning**: *Alike many \"natural language processing\" oriented or sources for text & data, the text & data used in this project may have been collected from public websites or sources, and may be unfiltered. Thus, some text & data may be disturbing, disagreeable, or otherwise objectionable.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.) processed_arabic.jsonl\n",
    "```json\n",
    "{\"text\": \"\\u0646\\u0648\\u0627\\u0644 \\u0627\\u0644\\u0632\\u063a\\u0628\\u064a (\\u0627\\u0644\\u0634\\u0627\\u0628 \\u062e\\u0627\\u0644\\u062f \\u0644\\u064a\\u0633 \\u0639\\u0627\\u0644\\u0645\\u064a) \\u0647\\u0647\\u0647\\u0647\\u0647\\u0647\\u0647\\u0647 \\u0623\\u062a\\u0641\\u0631\\u062c\\u064a \\u0639\\u0644\\u0649 \\u0647\\u0627 \\u0627\\u0644\\u0641\\u064a\\u062f\\u064a\\u0648 \\u064a\\u0627 \\u0645\\u0628\\u062a\\u062f\\u0626\\u0629 http v\\u00eda @user\", \"label\": \"0\"}\n",
    "{\"text\": \"\\u0646\\u0648\\u0627\\u0644 \\u0627\\u0644\\u0632\\u063a\\u0628\\u064a \\u0644\\u0637\\u064a\\u0641\\u0647 \\u0627\\u0644\\u0641\\u0646\\u0627\\u0646\\u0647 \\u0627\\u0644\\u0648\\u062d\\u064a\\u062f\\u0647 \\u0627\\u0644\\u0644\\u064a \\u0643\\u0644 \\u0627\\u0644\\u0641\\u064a\\u062f\\u064a\\u0648 \\u0643\\u0644\\u064a\\u0628\\u0627\\u062a \\u062a\\u0628\\u0639\\u0647\\u0627 \\u0645\\u0627\\u062a\\u0633\\u0628\\u0628 \\u062a\\u0644\\u0648\\u062b \\u0628\\u0635\\u0631\\u064a \\u0648\\u0644\\u0627 \\u0633\\u0645\\u0639\\u064a \\u0644\\u0648 \\u0635\\u0648\\u062a\\u0647\\u0627 \\u0627\\u0642\\u0644 \\u0645\\u0646 \\u0639\\u0627\\u062f\\u064a\", \"label\": \"2\"}\n",
    "{\"text\": \"\\u0644\\u0645\\u0627 \\u0642\\u0627\\u0644\\u062a \\u0646\\u0648\\u0627\\u0644 \\u0627\\u0644\\u0632\\u063a\\u0628\\u064a \\u0644\\u0627\\u0628\\u0642\\u0644\\u0647\\u0627 \\u0647\\u0627\\u0644\\u0644\\u0642\\u0628 \\u0641\\u0631\\u062d\\u0648\\u0627 \\u0641\\u0627\\u0646\\u0632\\u0647\\u0627 \\ud83d\\ude02\\ud83d\\ude02\\ud83d\\ude02\\u0643\\u0627\\u0646 \\u0644\\u0627\\u0632\\u0645 \\u064a\\u0627\\u062e\\u062f\\u0648\\u0647\\u0627 \\u0627\\u0647\\u0627\\u0646\\u0629 \\u0645\\u0634 \\u062b\\u0646\\u0627\\u0621 http\", \"label\": \"0\"}\n",
    "{\"text\": \"@user \\u062a\\u0630\\u0643\\u0631\\u0646\\u064a \\u0628\\u0623\\u063a\\u0646\\u064a\\u0629 \\u0646\\u0648\\u0627\\u0644 \\u0627\\u0644\\u0632\\u063a\\u0628\\u064a \\\"\\u0639\\u064a\\u0646\\u064a\\u0643 \\u0643\\u062f\\u0627\\u0628\\u064a\\u0646\\\"\", \"label\": \"2\"}\n",
    "{\"text\": \"\\u0628\\u0644\\u0627 \\u062a\\u0634\\u0641\\u064a\\u0631- \\u0623\\u0645\\u0644 \\u062d\\u0645\\u0627\\u062f\\u064a \\u0628\\u062a\\u0646\\u062a\\u0642\\u062f \\u0627\\u0644\\u0646\\u062c\\u0645\\u0629 \\u0646\\u0648\\u0627\\u0644 \\u0627\\u0644\\u0632\\u063a\\u0628\\u064a:\\\" \\u0646\\u062a\\u064a... http\", \"label\": \"0\"}\n",
    "{\"text\": \"\\u0641\\u0646\\u0627\\u0646\\u0629 \\u0644\\u0628\\u0646\\u0627\\u0646\\u064a\\u0629 \\u0643\\u0628\\u064a\\u0631\\u0629  \\u0635\\u0648\\u062a\\u0647\\u0627 \\u0625\\u0633\\u0637\\u0648\\u0631\\u064a \\u061f! #\\u0645\\u0627\\u062c\\u062f\\u0629_\\u0627\\u0644\\u0631\\u0648\\u0645\\u064a   @user #\\u0646\\u062c\\u0648\\u0649_\\u0643\\u0631\\u0645  @user #\\u0646\\u0648\\u0627\\u0644_\\u0627\\u0644\\u0632\\u063a\\u0628\\u064a  @user #\\u062c\\u0648\\u0644\\u064a\\u0627_\\u0628\\u0637\\u0631\\u0633\", \"label\": \"2\"}\n",
    "{\"text\": \"#\\u0644\\u0628\\u0646\\u0627\\u0646\\u064a_\\u064a\\u0642\\u064a\\u0645_\\u062f\\u0648\\u0631\\u0647_\\u0645\\u0643\\u064a\\u0627\\u062c_\\u0628\\u0627\\u0644\\u0631\\u064a\\u0627\\u0636\\u0645\\u0627\\u0646\\u0643\\u0628\\u0646\\u0649 \\u063a\\u064a\\u0631 \\u0628\\u0631\\u0627\\u0637\\u0645 \\u0646\\u0648\\u0627\\u0644 \\u0627\\u0644\\u0632\\u063a\\u0628\\u064a\\u0648\\u0646\\u0647\\u0648\\u062f \\u0627\\u0644\\u064a\\u0633\\u0627\\u0648\\u0645\\u0624\\u062e\\u0631\\u0629 \\u0647\\u064a\\u0641\\u0627\\u0621 \\u0648\\u0647\\u0628\\u064a\\u0642\\u0644\\u0646\\u0627\\u0644\\u0643\\u0645 \\u0643\\u0644\\u0647 \\u0646\\u0641\\u062e \\u0648\\u0647\\u0648\\u0627\\u0628\\u0633\\ud83d\\ude09\\ud83d\\ude09 \\u064a\\u0627\\u0632\\u064a\\u0646 \\u0646\\u0641\\u062e\\u0647\\u0645 \\u064a\\u0644\\u0637\\u0634\", \"label\": \"0\"}\n",
    "{\"text\": \"\\u0633\\u064a\\u0644\\u0641\\u064a \\u0644\\u0644\\u0641\\u0646\\u0627\\u0646\\u0629 \\ud83d\\udc47\\u0627\\u0644\\u0630\\u0647\\u0628\\u064a\\u0629 #\\u0646\\u0648\\u0627\\u0644_\\u0627\\u0644\\u0632\\u063a\\u0628\\u064a @user \\u0628\\u0627\\u0637\\u0644\\u0627\\u0644\\u0629 \\u0631\\u0642\\u064a\\u0642\\u0629 \\u062c\\u0645\\u064a\\u0644\\u0629 \\u0635\\u0628\\u0627\\u062d \\u0627\\u0644\\u064a\\u0648\\u0645 \\ud83d\\udc8b\\u2764\\ud83c\\udf39\\u0635\\u0628\\u0627\\u062d \\u0627\\u0644\\u062e\\u064a\\u0631 \\ud83d\\ude0d http\", \"label\": \"2\"}\n",
    "{\"text\": \"\\u0645\\u0646\\u0627\\u0641\\u0633\\u0629 \\u0643\\u0628\\u064a\\u0631\\u0629 \\u0628\\u064a\\u0646 #\\u0646\\u0648\\u0627\\u0644_\\u0627\\u0644\\u0632\\u063a\\u0628\\u064a \\u0648 #\\u0623\\u0645\\u0644_\\u062d\\u062c\\u0627\\u0632\\u064a  \\u0648\\u0645\\u0634\\u0627\\u0643\\u0644 \\u0648\\u063a\\u064a\\u0631\\u0629 \\u0645\\u062a\\u0628\\u0627\\u062f\\u0644\\u0629 \\u0645\\u0646 \\u0647\\u064a \\u0646\\u062c\\u0645\\u0629 \\u0634\\u0631\\u0643\\u0629 #\\u0644\\u0627\\u064a\\u0641_\\u0633\\u062a\\u0627\\u064a\\u0644\\u0632_\\u0627\\u0633\\u062a\\u0648\\u062f\\u064a\\u0648\\u0632#\\u062a\\u0648\\u0642\\u0639\\u0627\\u062a2017\", \"label\": \"0\"}\n",
    "{\"text\": \"\\u0631\\u0648\\u0626\\u064a\\u062a\\u0643 #\\u062a\\u0631\\u064a\\u062d #\\u0627\\u0644\\u0646\\u0638\\u0631 #\\u0633\\u0645\\u0627\\u0639\\u0643 #\\u064a\\u0631\\u064a\\u062d #\\u0627\\u0644\\u0627\\u0639\\u0635\\u0627\\u0628 #\\u0643\\u0644\\u0645\\u0627\\u062a\\u0643 #\\u0628\\u0644\\u0633\\u0645 #\\u0627\\u0644\\u062c\\u0631\\u0648\\u062d \\u064a #\\u0646\\u062c\\u0645\\u062a\\u064a #\\u0627\\u0644\\u0630\\u0647\\u0628\\u064a\\u0629 #\\u0646\\u0648\\u0627\\u0644_\\u0627\\u0644\\u0632\\u063a\\u0628\\u064a #\\u0628\\u062d\\u0628\\u0643 @user\\u2026 http\", \"label\": \"2\"}\n",
    "{\"text\": \"#star_news@Cheb_Khaled_ \\u0631\\u062f \\u0627\\u0644\\u0634\\u0627\\u0628 \\u062e\\u0627\\u0644\\u062f \\u0639\\u0644\\u0649 \\u0646\\u0648\\u0627\\u0644 \\u0627\\u0644\\u0632\\u063a\\u0628\\u064a \\u0645\\u0636\\u062d\\u0643 \\u0647\\u0647\\u0647\\u0647\\u0647\\u0647\\u0647\\u0647\\u0647\\u0647\\u0647  \\u0646\\u0648\\u0627\\u0644 \\u0644\\u0627\\u0632\\u0645 \\u0645\\u0627 \\u062a\\u062a\\u0643\\u0644\\u0645\\u064a \\u0639\\u0644\\u0649 \\u0627\\u0633\\u064a\\u0627\\u062f\\u0643http\", \"label\": \"0\"}\n",
    "{\"text\": \"\\u0639\\u0645 \\u0628\\u062d\\u0643\\u064a \\u0645\\u0639 \\u062d\\u0627\\u0644\\u064a #\\u0646\\u0648\\u0627\\u0644_\\u0627\\u0644\\u0632\\u063a\\u0628\\u064a http\", \"label\": \"2\"}\n",
    "{\"text\": \"\\u0644\\u0648 \\u0627\\u0634\\u0648\\u0641\\u0647 \\u0642\\u062f\\u0627\\u0645\\u064a \\u0630\\u0628\\u062d\\u062a\\u0647 \\u0628\\u0633 \\u0637\\u0628\\u0639\\u0627 \\u0645\\u0648 \\u0642\\u0628\\u0644 \\u0627\\u0644\\u062a\\u062d\\u0642\\u064a\\u0642  \\ud83d\\ude05\\ud83d\\ude05 @user #nadinenassibnjeim #nnn #star #actress\\u2026 http\", \"label\": \"0\"}\n",
    "{\"text\": \"@user - \\u0622\\u0644\\u0639\\u0622\\u0642\\u0644 \\u0622\\u0644\\u0622\\u06af\\u062b\\u0631 \\u062c\\u0646\\u0648\\u0646\\u0622 \\u0628\\u0622\\u0644\\u0630\\u0647\\u0628\\u06cc\\u0629\\u0629 #\\u0646\\u0648\\u0622\\u0644_\\u0622\\u0644\\u0632\\u063a\\u0628\\u064a \\u2764\\ud83d\\ude0d\\ud83d\\udc97\\ud83d\\udc97 http\", \"label\": \"2\"}\n",
    "{\"text\": \"@user \\u0647\\u0646\\u0627\\u0643 \\u0644\\u0639\\u0628\\u0629 \\u0648\\u062a\\u0628\\u0627\\u062f\\u0644 \\u0627\\u062f\\u0648\\u0627\\u0631 \\u0628\\u064a\\u0646 \\u0645\\u064a\\u0634\\u0627\\u0644 \\u0639\\u0648\\u0646 \\u0648\\u062d\\u0632\\u0628 \\u0627\\u0644\\u0644\\u0647 \\u0644\\u062e\\u062f\\u0627\\u0639 \\u0627\\u0644\\u0633\\u0639\\u0648\\u062f\\u064a\\u0647 \\u062d\\u062a\\u0649 \\u064a\\u062d\\u0635\\u0644\\u0648\\u0627 \\u0639\\u0644\\u0649 \\u0627\\u0644\\u0647\\u0628\\u0629 \\u0627\\u0644\\u0645\\u0627\\u0644\\u064a\\u0647 \\u0641\\u062d\\u0632\\u0628 \\u0627\\u0644\\u0644\\u0647 \\u0641\\u064a \\u0636\\u0627\\u0626\\u0642\\u0647 \\u0645\\u0627\\u0644\\u064a\\u0647 .\", \"label\": \"0\"}\n",
    "{\"text\": \"@user - \\ufe91\\ufef4\\ufee6 \\u06af\\u064f\\u0644 \\\" \\ufee7\\u0650\\ufed4\\ufeb2 \\ufed3\\ufef4\\u0651\\ufee8\\ufef2 \\\" \\u0635\\u064f\\u0648\\u062a\\u06af\\u06af\\\"  #\\u0646\\u0648\\u0622\\u0644_\\u0622\\u0644\\u0632\\u063a\\u0628\\u064a #\\u0622\\u063a\\u0644\\u0649_\\u0622\\u0644\\u062d\\u0628\\u0622\\u064a\\u0628 \\ud83d\\udc99\\ud83d\\udc97\\ud83d\\udc97\\ud83d\\ude0d\\u2764 http\", \"label\": \"2\"}\n",
    "{\"text\": \"-\\u0628\\u064a\\u0627\\u0631 \\u0631\\u0628\\u0651\\u0627\\u0637: \\u0644\\u0627 \\u0647\\u0644\\u0651\\u0642 \\u0628\\u062f\\u064a \\u0627\\u0633\\u0626\\u0644\\u0643\\u060c \\u0645\\u064a\\u0646 \\u0635\\u062f\\u064a\\u0642\\u0643 \\u0623\\u0643\\u062a\\u0631 \\\" \\u0627\\u0644\\u0631\\u0651\\u0626\\u064a\\u0633 \\u0645\\u064a\\u0634\\u0627\\u0644 \\u0639\\u0648\\u0646 \\u0623\\u0648 \\u0633\\u0644\\u064a\\u0645\\u0627\\u0646 \\u0641\\u0631\\u0646\\u062c\\u064a\\u0629\\u061f\\\"\\u0645\\u0634 \\u0639\\u0644\\u064a\\u0646\\u0627 \\u0647\\u0627\\u0644\\u062d\\u0631\\u0643\\u0627\\u062a. \\u0625\\u0646\\u0651\\u0648 \\u0627\\u0639\\u062a\\u0631\\u0641 \\u0628\\u063a\\u0644\\u0637\\u062a\\u0643 #\\u0628\\u0644\\u0627_\\u0647\\u0628\\u0644Part 2\", \"label\": \"0\"}\n",
    "{\"text\": \"@user - \\u0639\\u0644\\u0622\\u062c \\ufedf\\u064f\\ufee0\\ufee4\\ufeb0\\u0622\\ufe9d \\u0622\\ufedf\\ufee4\\ufeae \\ufed3\\ufef2 \\u0635\\u064f\\u0640\\u0648\\u062a\\u06af\\u06af  \\ud83d\\udc9b\\ud83d\\udc97\\ud83d\\udc99\\u2764\\ud83d\\udc99#\\u0646\\u0648\\u0622\\u0644_\\u0622\\u0644\\u0632\\u063a\\u0628\\u064a #\\u06af\\u064f\\u0644_\\u06cc\\u0648\\u0645_\\u062c\\u0645\\u0639\\u0629\\u0629 \\ud83d\\udc97\\ud83d\\udc9b http\", \"label\": \"2\"}\n",
    "{\"text\": \"\\u0648\\u062f\\u0627\\u062f \\u062c\\u0627\\u0628\\u0631: \\u0645\\u064a\\u0634\\u0627\\u0644 \\u0639\\u0648\\u0646 \\u0644\\u0646 \\u064a\\u0643\\u0645\\u0644 \\u0648\\u0644\\u0627\\u064a\\u062a\\u0647 #\\u0644\\u0644\\u0646\\u0634\\u0631\", \"label\": \"0\"}\n",
    "{\"text\": \"I liked a @user video from @user http \\u0646\\u0648\\u0627\\u0644 \\u0627\\u0644\\u0632\\u063a\\u0628\\u064a \\u0648 \\u0648\\u0627\\u0626\\u0644 \\u0643\\u0641\\u0648\\u0631\\u064a \\u0645\\u064a\\u0646 \\u062d\\u0628\\u064a\\u0628\\u064a\", \"label\": \"2\"}\n",
    "{\"text\": \"\\u0648\\u0645\\u0646 \\u063a\\u064a\\u0631 #\\u0627\\u0644\\u0631\\u0626\\u064a\\u0633_\\u0645\\u064a\\u0634\\u0627\\u0644_\\u0639\\u0648\\u0646 \\u064a\\u0633\\u062a\\u0623\\u0647\\u0644 \\u0630\\u0644\\u0643 \\u0641\\u064a \\u0648\\u062c\\u0647 \\u0627\\u0644\\u0641\\u0633\\u0627\\u062f \\u0648\\u0627\\u0644\\u062d\\u0631\\u0627\\u0645\\u064a\\u064a  #\\u062c\\u0647\\u0632\\u0648\\u0627_\\u0633\\u0648\\u0627\\u0639\\u062f\\u0643\\u0645 http\", \"label\": \"0\"}\n",
    "{\"text\": \"\\u0646\\u0648\\u0627\\u0644 \\u0627\\u0644\\u0632\\u063a\\u0628\\u064a  \\u0635\\u0648\\u062a \\u0627\\u0644\\u0647\\u062f\\u0648\\u0621 \\u2764\\ufe0f\\u2764\\ufe0f\", \"label\": \"2\"}\n",
    "{\"text\": \"\\u0631\\u0623\\u0649 \\u0627\\u0644\\u0631\\u0626\\u064a\\u0633 \\u0645\\u064a\\u0634\\u0627\\u0644 \\u0639\\u0648\\u0646 \\u0623\\u0646 (\\u0627\\u0644\\u0631\\u0626\\u064a\\u0633 \\u0627\\u0644\\u0633\\u0648\\u0631\\u064a) \\u0628\\u0634\\u0627\\u0631 \\u0627\\u0644\\u0623\\u0633\\u062f \\u0644\\u0648 \\u062e\\u0633\\u0631 \\u0627\\u0644\\u062d\\u0631\\u0628\\u060c \\u0644\\u062a\\u062d\\u0648\\u0644\\u062a \\u0633\\u0648\\u0631\\u064a\\u0629 \\u0625\\u0644\\u0649 \\u0644\\u064a\\u0628\\u064a\\u0627 \\u062b\\u0627\\u0646\\u064a\\u0629\\\"\\u060c\", \"label\": \"0\"}\n",
    "{\"text\": \"\\u0646\\u0648\\u0627\\u0644 \\u0627\\u0644\\u0632\\u063a\\u0628\\u064a \\u0645\\u0646 \\u0627\\u0644\\u062d\\u062c\\u0627\\u062a \\u0627\\u0644\\u064a \\u0647\\u062a\\u0641\\u0636\\u0644 \\u062c\\u0645\\u064a\\u0644\\u0647 \\u0639\\u0644\\u064a \\u0637\\u0648\\u0644...\\ud83d\\ude0d\\ud83d\\ude0d\\ud83d\\ude0d\\ud83d\\ude0d http\", \"label\": \"2\"}\n",
    "```\n",
    "\n",
    "#### # 2.) processed_english.jsonl (Pre-Processed by manually filtering out any jsonl objects with a \"1\" value for the \"label\" key, which would be the 'neutral' rating)\n",
    "```json\n",
    "{\"text\": \"Trying to have a conversation with my dad about vegetarianism is the most pointless infuriating thing ever #caveman \", \"label\": \"0\"}\n",
    "{\"text\": \"@user You are a stand up guy and a Gentleman Vice President Pence \", \"label\": \"2\"}\n",
    "{\"text\": \"@user @user @user Looks like Flynn isn't too pleased with me, he blocked me. You blocked by Flynn too @user \", \"label\": \"0\"}\n",
    "{\"text\": \"i'm not even catholic, but pope francis is my dude. like i just need him to hug me and tell me everything is okay. \", \"label\": \"2\"}\n",
    "{\"text\": \"@user for al the crying you do about how middle America is left out-they have twice as much voting power \", \"label\": \"0\"}\n",
    "{\"text\": \"Samsung to Bring Android 7.0 Nougat to Galaxy S6, S6 edge, Note 5, and Tab S2 - Softpedia News \", \"label\": \"2\"}\n",
    "{\"text\": \"Israel deems comatose Gaza man who needs treatment in West Bank  a security threat. #Palestine  via @user \", \"label\": \"0\"}\n",
    "{\"text\": \"I will go so far to say s1 of westworld isn't just good, it's brilliant. A story within a story within a story about storytelling \", \"label\": \"2\"}\n",
    "{\"text\": \"Iraqi Forces set to storm 3 areas of #Mosul, #AlQahira, #alMasarif &  #alAmn#MosulOps#mosuloffensive#iraq#ISIS\\u2026 \", \"label\": \"0\"}\n",
    "{\"text\": \"#NationalFastFoodDay Would love to live there. Chick-fil-A \\ud83d\\ude0d \", \"label\": \"2\"}\n",
    "{\"text\": \"Electoral College must reject Trump #rejecttrump #notmypresident #takingbackdemocracy #blacklivesmatter #nevertrump \", \"label\": \"0\"}\n",
    "{\"text\": \"The Reputation Doctor weighs in on Tony Romo #NFL @user joins @user on #TheMorningRush LISTEN: \", \"label\": \"2\"}\n",
    "{\"text\": \"Stop #fracking #Cuadrilla persecuting land defenders through the legal system #BankruptCuadrilla #ECOCIDE \", \"label\": \"0\"}\n",
    "{\"text\": \"So proud of way @user & #trumpTransitionteam are molding strong leadership group for #America #TeamTrump #MakeAmericaGreatAgain \", \"label\": \"2\"}\n",
    "{\"text\": \"@user Everyone's thinking far too short-termed. When all fossil fuels run out with no substitutes, then the crisis will come. \", \"label\": \"0\"}\n",
    "{\"text\": \"@user @user - #ScreamQueens so lucky to get to work with TK. \", \"label\": \"2\"}\n",
    "{\"text\": \"@user @user @user Why is it that today society is casually comfortable about being pubicly distasteful?#deplorables \", \"label\": \"0\"}\n",
    "{\"text\": \"This is a big deal and a smart move by Microsoft: \", \"label\": \"2\"}\n",
    "{\"text\": \"Ben Carson for Housing & Urban Development?? \\ud83d\\ude10 I just can't \\ud83d\\ude12 \", \"label\": \"0\"}\n",
    "{\"text\": \"Digesting while watching #ScreamQueens s1 Thanksgiving & Black Friday eps. SO SO GOOD!! \", \"label\": \"2\"}\n",
    "{\"text\": \"Israel's New Racism: Persecution of African Migrants in Holy Land #gaza #palestine #israel #BDS \", \"label\": \"0\"}\n",
    "{\"text\": \"I've been listening to Leonard Cohen nonstop for the past month. Only to find out he passed literally weeks ago. Wow my heart is so overwhel \", \"label\": \"2\"}\n",
    "{\"text\": \"@user She just didn't get them in areas were she needed them. Lots of voter suppression going on. Hacking & tampering\\ud83d\\udc99 \", \"label\": \"0\"}\n",
    "{\"text\": \"Listen to #NBAwards Winner @user interview on @user \", \"label\": \"2\"}\n",
    "```\n",
    "\n",
    "#### # 3.) processed_french.jsonl (Pre-Processed by manually filtering out any jsonl objects with a \"1\" value for the \"label\" key, which would be the 'neutral' rating)\n",
    "```json\n",
    "{\"text\": \"Royal: le pr\\u00e9sident n'aime pas les pauvres? \\\"c'est n'importe quoi\\\" http \\u2026\", \"label\": \"0\"}\n",
    "{\"text\": \"#press #seo: Nager vert avec les piscines \\u00e9cologiques http #piscine\\u00e9cologique\", \"label\": \"2\"}\n",
    "{\"text\": \"#Biodiversit\\u00e9 #Gard : Des L\\u00e9zards ocell\\u00e9s d\\u00e9truits sans autorisation \\u00e0 Saint-Gilles ! http\", \"label\": \"0\"}\n",
    "{\"text\": \"Etat des lieux et analyse des m\\u00e9canismes de soutien public aux \\u00e9nergies renouvelables en France \\u25ba\\u25ba http\", \"label\": \"2\"}\n",
    "{\"text\": \"Les esp\\u00e8ces disparaissent 1.000 fois plus depuis que l'Homme existe | @user http\", \"label\": \"0\"}\n",
    "{\"text\": \"Une entreprise allemande propose une solution \\u00e9co-efficace de ferme urbaine http #NRJ(Canada) #RegionsofFrance\", \"label\": \"2\"}\n",
    "{\"text\": \"R\\u00e9chauffement : l\\u2019ONU pr\\u00e9dit un v\\u00e9ritable \\u00abenfer climatique\\u00bb d\\u2019ici 2050 http http\", \"label\": \"0\"}\n",
    "{\"text\": \"@user @user Ah Oui, les \\u00e9oliennes, pourrait aussi les \\u00e9loigner .. ouf Merci...J'en veux une dans ma cour...\", \"label\": \"2\"}\n",
    "{\"text\": \"Info: \\u00c9oliennes - Le r\\u00e9sultat de l\\u2019enqu\\u00eate publique est tomb\\u00e9 pour Esnes et Montz\\u00e9ville : avis d\\u00e9favorable\", \"label\": \"0\"}\n",
    "{\"text\": \"R\\u00e9chauffement climatique: il est grand temps d'agir - http\", \"label\": \"2\"}\n",
    "{\"text\": \"#GCO : les \\u00e9cologistes strasbourgeois sont \\\"stup\\u00e9faits\\\" - http\", \"label\": \"0\"}\n",
    "{\"text\": \"J'aime ma bio parce que je mange bio et je travail dans la biologie et je roule en biocarburant.\", \"label\": \"2\"}\n",
    "{\"text\": \"Pol\\u00e9mique autour de l\\u2019abattage massif de bouquetins en Haute-Savoie | Eco(lo) http\", \"label\": \"0\"}\n",
    "{\"text\": \"Echange autour du th\\u00e8me de l'agriculture raisonn\\u00e9e \\u00e0 la R\\u00e9union http\", \"label\": \"2\"}\n",
    "{\"text\": \"Un r\\u00e9seau de journalistes pour lutter contre les n\\u00e9gationnistes du changement climatique. http\", \"label\": \"0\"}\n",
    "{\"text\": \"@user \\\"Illimit\\u00e9es\\\", cela fera sourire les environnementalistes. Et le th\\u00e8me du d\\u00e9clin am\\u00e9ricain est r\\u00e9curent depuis les ann\\u00e9es 1970...\", \"label\": \"2\"}\n",
    "{\"text\": \"L\\u2019exportation de l\\u2019\\u00e9nergie \\u00e9olienne, une source de profits boud\\u00e9e par Hydro-Qu\\u00e9bec  | Michel Lagac\\u00e9... http\", \"label\": \"0\"}\n",
    "{\"text\": \"@user ah \\u00e7a c'est s\\u00fbr. J'ai fais du volont\\u00e9riat en \\u00e9cologie.\", \"label\": \"2\"}\n",
    "{\"text\": \"L\\u2019exportation de l\\u2019\\u00e9nergie \\u00e9olienne, une source de profits boud\\u00e9e par Hydro-Qu\\u00e9bec \\u00a0|\\u00a0Michel\\u00a0Lagac\\u00e9 http\", \"label\": \"0\"}\n",
    "{\"text\": \"En France aussi, la police / gendarmerie est au service d'une minorit\\u00e9... http #Sivens #\\u00c9cologie http\", \"label\": \"2\"}\n",
    "{\"text\": \"La lutte contre la mosa\\u00efque du manioc aux Comores: La s\\u00e9lection vari\\u00e9tale pour lutter contre la mosa\\u00efque du manioc http\", \"label\": \"0\"}\n",
    "{\"text\": \"Il nous sort \\\"J'vais pas allumer la lumi\\u00e8re parce que je veux pas faire marcher des centrales \\u00e9lectriques.\\\" Ah.\", \"label\": \"2\"}\n",
    "{\"text\": \"Cuba: Mesures contre les effets du changement http #Cuba\", \"label\": \"0\"}\n",
    "{\"text\": \"@user @user @user Je mets au d\\u00e9fis ce d\\u00e9put\\u00e9 bobo \\u00e9cologiste d'aller au chantier de Saint-Nazaire #chiche\", \"label\": \"2\"}\n",
    "```\n",
    "\n",
    "#### # 4.) processed_german.jsonl (Pre-Processed by manually filtering out any jsonl objects with a \"1\" value for the \"label\" key, which would be the 'neutral' rating)\n",
    "```json\n",
    "{\"text\": \"@user korrekt! Verstehe sowas nicht...\", \"label\": \"0\"}\n",
    "{\"text\": \"Der Dubbletimepart von Julien war ja mal sowas von genial! :D (@user http\", \"label\": \"2\"}\n",
    "{\"text\": \"#Instachallenge #Day16 #what #i #am #reading #Fratzensammler #Horror #Wattpad http\", \"label\": \"0\"}\n",
    "{\"text\": \"@user ah. Hatte nur bis radolfzell geschaut wegen ticket. Aber so isses fast normal ;)\", \"label\": \"2\"}\n",
    "{\"text\": \"@user \\u00dcberall lauert Gefahr. Unverhofft. #Achtung\", \"label\": \"0\"}\n",
    "{\"text\": \"Sonntag \\u2714 Fitness \\u2714 Nichts tun\\u2714\", \"label\": \"2\"}\n",
    "{\"text\": \"@user da werden wir uns wohl nicht einig werden, bef\\u00fcrchte ich!\", \"label\": \"0\"}\n",
    "{\"text\": \"@user Toll. Sehr l\\u00f6blich. Sieht echt klasse aus.\", \"label\": \"2\"}\n",
    "{\"text\": \"\\\"Absurde Unwahrheiten\\\" -Hoene\\u00df stocksauer auf den \\\"Stern\\\" http #Absurde #Nachrichten #N24\", \"label\": \"0\"}\n",
    "{\"text\": \"RT @user: Linda, perfeita, guapa, Wundersch\\u00f6n, Bella, Beautiful !! http\", \"label\": \"2\"}\n",
    "{\"text\": \"Dank @user spiele ich jetzt ohne Schei\\u00df Candy Crush Saga ... es regt mich ma\\u00dflos auf aber aufh\\u00f6ren kann ich auch ned -.-\", \"label\": \"0\"}\n",
    "{\"text\": \"Andreas Sp\\u00e4ck aus der Crowd sucht ein originelles Hochzeitgeschenk... ;-) Wer kann helfen? Merci ! Chuy You... http\", \"label\": \"2\"}\n",
    "{\"text\": \"Nicht nur gg B\\u00fcrger, sondern auch gg eigene Mitarbeiter mobben die Beh\\u00f6rden. Diese F\\u00e4lle bearbeitet http\", \"label\": \"0\"}\n",
    "{\"text\": \"@user OMG Titanium! Unbelievable Jahmene\", \"label\": \"2\"}\n",
    "{\"text\": \"Unn\\u00f6tiger schei\\u00df. Ernsthaft.\", \"label\": \"0\"}\n",
    "{\"text\": \"Bei http l\\ufffduft zur zeit EminemSing For The Moment also schnell einschalten :)\", \"label\": \"2\"}\n",
    "{\"text\": \"RT @user: liebe @user, dein Duschgel schmeckt nicht #tranzparenztweet\", \"label\": \"0\"}\n",
    "{\"text\": \"@user Aw, na gut, dann schlaf fein, klein Delalein :)\", \"label\": \"2\"}\n",
    "{\"text\": \"@user kann man viel gelesen haben davon, nona, aber grunds\\u00e4tzlich so nat\\u00fcrlich l\\u00e4cherlich, die Liste! ff\", \"label\": \"0\"}\n",
    "{\"text\": \"@user kale chips= life\", \"label\": \"2\"}\n",
    "{\"text\": \"Arschl\\u00f6cher!!! MOBIL http\", \"label\": \"0\"}\n",
    "{\"text\": \"RT @user: Das beste kommt zum Schluss? Ils\\u00f6: \\\"Manchmal kommt der Beste auch erst in den letzten 45min.\\\"\\u2026 http\", \"label\": \"2\"}\n",
    "{\"text\": \"RT @user: Wach liegen, Gedankenkarussell, unbeantwortete Fragen, Bangen, Hoffen, Zweifeln, Stolz, Verblendung, Leere, Einsamkeit\\u2026\", \"label\": \"0\"}\n",
    "{\"text\": \"@user Mich haben se nach vorne geschickt, konnt ne stille stehen ^^\", \"label\": \"2\"}\n",
    "```\n",
    "\n",
    "#### # 5.) processed_hindi.jsonl (Pre-Processed by manually filtering out any jsonl objects with a \"1\" value for the \"label\" key, which would be the 'neutral' rating)\n",
    "```json\n",
    "{\"text\": \"CONGRESS na ye party kabhi bani hoti na india ka partition hota nd na hi humari country itni khokhli hoti   @ \", \"label\": \"0\"}\n",
    "{\"text\": \"ha ha ha ha ha ha ha .  .  .  .  . :d ye mast tha .  .  .  . cute aur comic .  .  .  . :)\", \"label\": \"2\"}\n",
    "{\"text\": \"kejariwal tum apana soch ka dayra badho ab is misunderstanding se kam nahi chalane wala hai tumhara\", \"label\": \"0\"}\n",
    "{\"text\": \"jai hind\", \"label\": \"2\"}\n",
    "{\"text\": \"result me konsa bc univrsty tope mari ... jo itna wait kr rha h \", \"label\": \"0\"}\n",
    "{\"text\": \"rt @mukhijanidhi: hai teri hasti aisi jo dekhe wo mar mite #msgyouthicon #msgrevolution\", \"label\": \"2\"}\n",
    "{\"text\": \"Dhyan rakh ?? \", \"label\": \"0\"}\n",
    "{\"text\": \"aisa school ho to me kabhi ghar hi na jau .  .  . \", \"label\": \"2\"}\n",
    "{\"text\": \"pratiyogita darpan ki spelling mistake hai :)\", \"label\": \"0\"}\n",
    "{\"text\": \"i'll never forget that first girl i was crazy about in 5th grade .  i still got her valentine day card in my secret safe . \", \"label\": \"2\"}\n",
    "{\"text\": \"BC porn site ki tarah advertise karo tum bas ..... !!!! male hoe .. \", \"label\": \"0\"}\n",
    "{\"text\": \"hahaha sai m .  .  ek baar class se nikal jate the to pura school naap ke wapas ate the\", \"label\": \"2\"}\n",
    "{\"text\": \"post of the night: anonymous (id: excv6jkr) 11/02/12(fri)00:52:39 no . 434329650 this thread is like watching a monkey fuck a coconut\", \"label\": \"0\"}\n",
    "{\"text\": \"Jaanma main bol rahi hu ki,tum mere twits dekho :/\", \"label\": \"2\"}\n",
    "{\"text\": \"Did \", \"label\": \"0\"}\n",
    "{\"text\": \"rt psharma2525: rt komalinsaan: gurmeetramrahim #lovetoseemsg #msgincinemas schi agr duniya ye movi dekh le to yaha swrg bn skta h\", \"label\": \"2\"}\n",
    "{\"text\": \"Woh  bhi \", \"label\": \"0\"}\n",
    "{\"text\": \"rare ,  300kg meteorite discovered in poland ,  biggest in eastern europe and may provide clues about earth's core: http/URL\", \"label\": \"2\"}\n",
    "{\"text\": \"band karo ye atyachaar. #indvsuae\", \"label\": \"0\"}\n",
    "{\"text\": \"durdarshan wah wah  .  .  .  . \", \"label\": \"2\"}\n",
    "{\"text\": \"Ruk \", \"label\": \"0\"}\n",
    "{\"text\": \"Salman bai ap kese hen ma b ap ka fen hun\", \"label\": \"2\"}\n",
    "{\"text\": \"aur jab koi ek excuse maar deta tha toh dusra kehta saale ye maine socha hua tha tune kyun bol diya\", \"label\": \"0\"}\n",
    "{\"text\": \"kiranji ko delhi ka cm ummidwar ghosit karna modiji ki doodarshita wa samghdaari ko darshata hai . \", \"label\": \"2\"}\n",
    "```\n",
    "\n",
    "#### # 6.) processed_italian.jsonl (Pre-Processed by manually filtering out any jsonl objects with a \"1\" value for the \"label\" key, which would be the 'neutral' rating)\n",
    "```json\n",
    "{\"text\": \"@user @user Ma Ferrero? il compagno Ferrero? ma il suo partito esiste ancora? allora stiamo proprio frecati !!!\", \"label\": \"0\"}\n",
    "{\"text\": \"Non vedi l'ora che venga qui almeno lo sentir\\u00f2 pi\\u00f9 spesso e potr\\u00f2 finalmente stare con lui alla faccia di chi mi ha friendzonato.\", \"label\": \"2\"}\n",
    "{\"text\": \"@user @user solo che poi arriva @user e decide sempre tutto lui\", \"label\": \"0\"}\n",
    "{\"text\": \"@user Tutta la pasticceria! \\ud83d\\udc8f Grazie amore mio! \\u2764\", \"label\": \"2\"}\n",
    "{\"text\": \"Quanto mi stai cazzo troia\", \"label\": \"0\"}\n",
    "{\"text\": \"Perch\\u00e9 alla fine siamo una famiglia e la famiglia non va mai abbandonata\\ud83d\\udc95 #WeWillMeetAgain1D http\", \"label\": \"2\"}\n",
    "{\"text\": \"@user @user @user @user @user calcio \\u00e8 la quintessenza della noia. Sport frenetici come basket o volley no\", \"label\": \"0\"}\n",
    "{\"text\": \"@user Ciao!Mia sorella ha un canale YouTube che si chiama corinnelife97potresti visitarlo e iscriverti se ti piace?Grazie\\ud83d\\udc9a\", \"label\": \"2\"}\n",
    "{\"text\": \"cazzo sono tutti sti tweet in tl di \\\\\\\"innocente\\\\\\\" \\\\\\\"colpevole\\\\\\\" ao\", \"label\": \"0\"}\n",
    "{\"text\": \"La bellezza della mie Steve Madden Fringly \\ud83d\\udc97 http\", \"label\": \"2\"}\n",
    "{\"text\": \"@user per\\u00f2 ha detto che \\u00e8 collegato positivamente alla pedofilia quindi ci vede qualcosa di buono\", \"label\": \"0\"}\n",
    "{\"text\": \"La pi\\u00f9 grande prova d'amore \\u00e8 sempre stata togliersi dai coglioni.\", \"label\": \"2\"}\n",
    "{\"text\": \"@user @user Quando invece riaprii i miei occhi mi accorsi che non era un sogno...Era tutto nitido e reale!Grigio e freddo...\", \"label\": \"0\"}\n",
    "{\"text\": \"\\u2013 tomlinsik;Profilo molto carino in generale abbinato molto bene.\", \"label\": \"2\"}\n",
    "{\"text\": \"Io faccio di tutto e lei ha ancora la faccia di ignorarmi perch\\u00e9 sono cos\\u00ec stupida? L'avrei dovuta mandare a fanculo da tempo.\", \"label\": \"0\"}\n",
    "{\"text\": \"@user @user @user @user @user @user prima ora e dopo \\ud83d\\ude0d\\ud83d\\ude0d\\ud83d\\ude1a\", \"label\": \"2\"}\n",
    "{\"text\": \"@user quanto ti odio da uno a dieci\", \"label\": \"0\"}\n",
    "{\"text\": \"@user @user @user ti vogliamo bene anche noi Harry\", \"label\": \"2\"}\n",
    "{\"text\": \"Io mi lamento della gente che scrive ancora \\\\\\\"freddy mercury\\\\\\\" ma anche quella che scrive \\\\\\\"jhonny cash\\\\\\\" non scherza\", \"label\": \"0\"}\n",
    "{\"text\": \"Oggi cominciamo cos\\u00ec! Con i nostri saluti per voi di venerd\\u00ec scorso dopo \\\\\\\"Notre Dame de Paris\\\\\\\". http\", \"label\": \"2\"}\n",
    "{\"text\": \"Dal prossimo anno Audi non sar\\u00e0 pi\\u00f9 uno sponsor del Milan il testimone passer\\u00e0 alla Piaggio...e via con gli Apetti!!\", \"label\": \"0\"}\n",
    "{\"text\": \"IL MIO DIARIO - Pagine d'estate quando ti innamori piano piano dietro uno schermo. Grazie Greta. @user \\u2764\\ufe0f http\", \"label\": \"2\"}\n",
    "{\"text\": \"@user @user il fatto che una persona che vada in discoteca implichi che questa si droghi \\u00e8 una fesseria...\", \"label\": \"0\"}\n",
    "{\"text\": \"@user che la forza sia con noi\", \"label\": \"2\"}\n",
    "```\n",
    "\n",
    "#### 7.) processed_portuguese.jsonl (Pre-Processed by manually filtering out any jsonl objects with a \"1\" value for the \"label\" key, which would be the 'neutral' rating)\n",
    "```json\n",
    "{\"text\": \"todos os meus favoritos na prova de elimina\\u00e7\\u00e3o #MasterChefBR\", \"label\": \"0\"}\n",
    "{\"text\": \"Augusto Cury \\u00e9 o cara\\ud83d\\ude09 entrevista myito boa do #ConversaComBial\", \"label\": \"2\"}\n",
    "{\"text\": \"Mano Vitor, t\\u00e1 muito feio! Decep\\u00e7\\u00e3o. #MasterChefBR\", \"label\": \"0\"}\n",
    "{\"text\": \"Morrendo de amores pelo @user no programa do @user. #TheNoite\", \"label\": \"2\"}\n",
    "{\"text\": \"MIRIAN CHOORANDO PELA PROVA PASSADA #MasterChefBR\", \"label\": \"0\"}\n",
    "{\"text\": \"a cada epis\\u00f3dio q passa Fabrizio continua um gato ne \\ud83d\\ude0f #MasterChefBR\", \"label\": \"2\"}\n",
    "{\"text\": \"Traz o @user e tira a Mirian Pvr    #MasterChefBR\", \"label\": \"0\"}\n",
    "{\"text\": \"A maior e melhor cantora que voc\\u00ea respeita @user no @user. #SandyLeah #Encontro #SandynoEncontro\", \"label\": \"2\"}\n",
    "{\"text\": \"Imagina que insuport\\u00e1vel ter de dar de comer pra uma gente que calcula CADA CALORIA que come? Jesus... #MasterChefBR\", \"label\": \"0\"}\n",
    "{\"text\": \"Baixa caloria? Frango desfiado e salada. T\\u00e1 \\u00f3timo  #MasterChefBR\", \"label\": \"2\"}\n",
    "{\"text\": \"A\\u00ed que agonia essa Marian, meu Deus do c\\u00e9u, mulher!! #MasterChefBR\", \"label\": \"0\"}\n",
    "{\"text\": \"\\\"Eu adoro fazer piquenique no TEMPRO\\u201d  haha te amo yuko #MasterChefBR\", \"label\": \"2\"}\n",
    "{\"text\": \"\\\"uma mulher mais nova com um homem mais velho \\u00e9 normal\\\" normal dizer que ela ta com ele por dinheiro n\\u00e9? #Encontro\", \"label\": \"0\"}\n",
    "{\"text\": \"Que amor esse @user no #Encontro \\u2764\\u2764\\u2764\", \"label\": \"2\"}\n",
    "{\"text\": \"Vanessa da Mata acabou com a fofurice do parto da mala da Bela Gil kkkkkk #AltasHoras\", \"label\": \"0\"}\n",
    "{\"text\": \"f\\u00e1tima bernardes maravilhosa no #VideoShowAoVivo\", \"label\": \"2\"}\n",
    "{\"text\": \"Al\\u00f4 NET, voces t\\u00e3o fora do ar?! Arrumem por favor que eu quero assistir #MasterChefBR hojeeee\", \"label\": \"0\"}\n",
    "{\"text\": \"#MasterChefBR Hummm deu vontade de comer uma sardinha com p\\u00e3ozinho kkk partiu abrir uma latinha! \\ud83d\\ude01\", \"label\": \"2\"}\n",
    "{\"text\": \"Pq o programa n\\u00e3o ta ao vivo? @user  #MaisVoc\\u00ea\", \"label\": \"0\"}\n",
    "{\"text\": \"Mr Catra dando um show no The Noite ! Falando da pol\\u00edtica no pa\\u00eds \\ud83d\\udc4f #TheNoite @user @user\", \"label\": \"2\"}\n",
    "{\"text\": \"Se algu\\u00e9m me fala que \\u00e9 termog\\u00eanico, eu devolvo o prato na hora #MasterChefBR\", \"label\": \"0\"}\n",
    "{\"text\": \"Ai eu me derreto todinha com a Paola\\ud83d\\ude2d\\ud83d\\ude0d #MasterChefBR\", \"label\": \"2\"}\n",
    "{\"text\": \"#AltasHoras assistindo o programa com uma cx de len\\u00e7os ao lado! Haja\\u2764\\ud83d\\ude22\", \"label\": \"0\"}\n",
    "{\"text\": \"Legal est\\u00e1 prova tomara que tenha mais #DomingoLegal\", \"label\": \"2\"}\n",
    "```\n",
    "\n",
    "#### 8.) processed_spanish.jsonl (Pre-Processed by manually filtering out any jsonl objects with a \"1\" value for the \"label\" key, which would be the 'neutral' rating)\n",
    "```json\n",
    "{\"text\": \"@user jajajaja dale, hacete la boluda vos jajaja igual a vos nunca se te puede tomar en serio te mando un abrazo desde Per\\u00fa!\", \"label\": \"0\"}\n",
    "{\"text\": \"@user MAAAAE RAJADO! Pero lo bueno es q uno se va independizando!y logrando metas\", \"label\": \"2\"}\n",
    "{\"text\": \"Bueno hoy fui a almorzar a Nanay con otras 3 dras xq la capacitaci\\u00f3n mal organizada no nos di\\u00f3 almuerzo y encima nos mandan a comer 2pm\", \"label\": \"0\"}\n",
    "{\"text\": \"@user \\u00a1Hola Tom\\u00e1s! \\u00bfHab\\u00e9is visto los nuevos #dinos de #TierraMagna? Es normal que haya colas antes de que comience el espect\\u00e1culo\", \"label\": \"2\"}\n",
    "{\"text\": \"@user la hijueputa tela se me sali\\u00f3. yo quer\\u00eda volver a quedar acostada.\", \"label\": \"0\"}\n",
    "{\"text\": \"@user pues no est\\u00e1 nada mal\", \"label\": \"2\"}\n",
    "{\"text\": \"@user quiz\\u00e1 para profesionales no sea mucho,pero hay no remunerados principalmente femenino para quienes es un sue\\u00f1o, pasa en mi pa\\u00eds\", \"label\": \"0\"}\n",
    "{\"text\": \"Hora de seguir so\\u00f1ando muy bonito\", \"label\": \"2\"}\n",
    "{\"text\": \"Me estoy muriendo. Ojal\\u00e1 mi jefa me haga trabajar viernes y s\\u00e1bado en la ma\\u00f1ana. Porque cerrar va a ser fatal\", \"label\": \"0\"}\n",
    "{\"text\": \"La felicidad tiene un nombr #Tailandia  Si es tu PRIMERA VEZ, apunta estos #consejos!   http http\", \"label\": \"2\"}\n",
    "{\"text\": \"@user @user justamente ah\\u00ed es tu equivocada suposici\\u00f3n, qxq estudi\\u00f3 en 1 colegio ficho ser\\u00e1 pulcro? La religi\\u00f3n le importa mda\", \"label\": \"0\"}\n",
    "{\"text\": \"@user Alpha suerte y ten un feliz a\\u00f1o 2017\", \"label\": \"2\"}\n",
    "{\"text\": \"Que viene Nonpa y fijo es para mayores. Ya lloro.\", \"label\": \"0\"}\n",
    "{\"text\": \"@user pues s\\u00ed, s\\u00e9 lo que se siente. Pero pi\\u00e9nselo como el \\u00faltimo esfuerzo, que ya ahorita llegan las vacaciones\", \"label\": \"2\"}\n",
    "{\"text\": \"@user @user @user @user HAHAHAHHA MMM NOS VAMOS EN 2 semanas y no tenemos ni hotel Pa la primera noche \\ud83d\\ude02\\ud83d\\ude02\\ud83d\\ude02\", \"label\": \"0\"}\n",
    "{\"text\": \"@user Que bonito,y yo tengo una sorpresa para Ti ,que te gustara much\\u00edsimo!\", \"label\": \"2\"}\n",
    "{\"text\": \"No me sorprende que 2ne1 se haya separado, man. Pero a\\u00fan as\\u00ed me dio en el cora bc es un grupo legendario y empec\\u00e9 mi era kpop con ellas\", \"label\": \"0\"}\n",
    "{\"text\": \"@user pero yo te kiero\", \"label\": \"2\"}\n",
    "{\"text\": \"@user tu te planteas comprar gr\\u00e1fica. Yo si lo planteo tengo que comprar todo, mi pc es un venerable anciano\", \"label\": \"0\"}\n",
    "{\"text\": \"Cambiar \\\"El mejor sistema\\u2122\\\" y la educaci\\u00f3n m\\u00e9dica en Espa\\u00f1a ser\\u00e1 complicado, pero con gente como vosotros tengo fe  @user @user\", \"label\": \"2\"}\n",
    "{\"text\": \"@user @user fallecido hace meses. Hay q contrastar.\", \"label\": \"0\"}\n",
    "{\"text\": \"Que haya una planta de tamarindo afuera de la que posiblemente va a ser la casa en la que vivas de vez en cuando, es una buena se\\u00f1al\", \"label\": \"2\"}\n",
    "{\"text\": \"no me deis confianza que luego os digo las cosas muy claras y os jode\", \"label\": \"0\"}\n",
    "{\"text\": \"@user ay, gracias. Lo malo es que ese es solo para primer viaje. Pero mil gracias igualmente\", \"label\": \"2\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A4 - Formatted JSON->Python Data Loading from Post-Classified Sentiment Ratings from ChatGPT & LLaMA 2\n",
    "### (full_tweety_mangos.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_arabic = [\n",
    "    {\n",
    "        \"tweet\": \"Ù†ÙˆØ§Ù„ Ø§Ù„Ø²ØºØ¨ÙŠ (Ø§Ù„Ø´Ø§Ø¨ Ø®Ø§Ù„Ø¯ Ù„ÙŠØ³ Ø¹Ø§Ù„Ù…ÙŠ) Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡ Ø£ØªÙØ±Ø¬ÙŠ Ø¹Ù„Ù‰ Ù‡Ø§ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙŠØ§ Ù…Ø¨ØªØ¯Ø¦Ø© http vÃ­a @user\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Ù†ÙˆØ§Ù„ Ø§Ù„Ø²ØºØ¨ÙŠ Ù„Ø·ÙŠÙÙ‡ Ø§Ù„ÙÙ†Ø§Ù†Ù‡ Ø§Ù„ÙˆØ­ÙŠØ¯Ù‡ Ø§Ù„Ù„ÙŠ ÙƒÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙƒÙ„ÙŠØ¨Ø§Øª ØªØ¨Ø¹Ù‡Ø§ Ù…Ø§ØªØ³Ø¨Ø¨ ØªÙ„ÙˆØ« Ø¨ØµØ±ÙŠ ÙˆÙ„Ø§ Ø³Ù…Ø¹ÙŠ Ù„Ùˆ ØµÙˆØªÙ‡Ø§ Ø§Ù‚Ù„ Ù…Ù† Ø¹Ø§Ø¯ÙŠ\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Ù„Ù…Ø§ Ù‚Ø§Ù„Øª Ù†ÙˆØ§Ù„ Ø§Ù„Ø²ØºØ¨ÙŠ Ù„Ø§Ø¨Ù‚Ù„Ù‡Ø§ Ù‡Ø§Ù„Ù„Ù‚Ø¨ ÙØ±Ø­ÙˆØ§ ÙØ§Ù†Ø²Ù‡Ø§ ğŸ˜‚ğŸ˜‚ğŸ˜‚ÙƒØ§Ù† Ù„Ø§Ø²Ù… ÙŠØ§Ø®Ø¯ÙˆÙ‡Ø§ Ø§Ù‡Ø§Ù†Ø© Ù…Ø´ Ø«Ù†Ø§Ø¡ http\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user ØªØ°ÙƒØ±Ù†ÙŠ Ø¨Ø£ØºÙ†ÙŠØ© Ù†ÙˆØ§Ù„ Ø§Ù„Ø²ØºØ¨ÙŠ \\\"Ø¹ÙŠÙ†ÙŠÙƒ ÙƒØ¯Ø§Ø¨ÙŠÙ†\\\"\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Ø¨Ù„Ø§ ØªØ´ÙÙŠØ±- Ø£Ù…Ù„ Ø­Ù…Ø§Ø¯ÙŠ Ø¨ØªÙ†ØªÙ‚Ø¯ Ø§Ù„Ù†Ø¬Ù…Ø© Ù†ÙˆØ§Ù„ Ø§Ù„Ø²ØºØ¨ÙŠ:\\\" Ù†ØªÙŠ... http\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"ÙÙ†Ø§Ù†Ø© Ù„Ø¨Ù†Ø§Ù†ÙŠØ© ÙƒØ¨ÙŠØ±Ø©  ØµÙˆØªÙ‡Ø§ Ø¥Ø³Ø·ÙˆØ±ÙŠ ØŸ! #Ù…Ø§Ø¬Ø¯Ø©_Ø§Ù„Ø±ÙˆÙ…ÙŠ   @user #Ù†Ø¬ÙˆÙ‰_ÙƒØ±Ù…  @user #Ù†ÙˆØ§Ù„_Ø§Ù„Ø²ØºØ¨ÙŠ  @user #Ø¬ÙˆÙ„ÙŠØ§_Ø¨Ø·Ø±Ø³\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"#Ù„Ø¨Ù†Ø§Ù†ÙŠ_ÙŠÙ‚ÙŠÙ…_Ø¯ÙˆØ±Ù‡_Ù…ÙƒÙŠØ§Ø¬_Ø¨Ø§Ù„Ø±ÙŠØ§Ø¶Ù…Ø§Ù†ÙƒØ¨Ù†Ù‰ ØºÙŠØ± Ø¨Ø±Ø§Ø·Ù… Ù†ÙˆØ§Ù„ Ø§Ù„Ø²ØºØ¨ÙŠÙˆÙ†Ù‡ÙˆØ¯ Ø§Ù„ÙŠØ³Ø§ÙˆÙ…Ø¤Ø®Ø±Ø© Ù‡ÙŠÙØ§Ø¡ ÙˆÙ‡Ø¨ÙŠÙ‚Ù„Ù†Ø§Ù„ÙƒÙ… ÙƒÙ„Ù‡ Ù†ÙØ® ÙˆÙ‡ÙˆØ§Ø¨Ø³ğŸ˜‰ğŸ˜‰ ÙŠØ§Ø²ÙŠÙ† Ù†ÙØ®Ù‡Ù… ÙŠÙ„Ø·Ø´\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Ø³ÙŠÙ„ÙÙŠ Ù„Ù„ÙÙ†Ø§Ù†Ø© ğŸ‘‡Ø§Ù„Ø°Ù‡Ø¨ÙŠØ© #Ù†ÙˆØ§Ù„_Ø§Ù„Ø²ØºØ¨ÙŠ @user Ø¨Ø§Ø·Ù„Ø§Ù„Ø© Ø±Ù‚ÙŠÙ‚Ø© Ø¬Ù…ÙŠÙ„Ø© ØµØ¨Ø§Ø­ Ø§Ù„ÙŠÙˆÙ… ğŸ’‹â¤ğŸŒ¹ØµØ¨Ø§Ø­ Ø§Ù„Ø®ÙŠØ± ğŸ˜ http\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Ù…Ù†Ø§ÙØ³Ø© ÙƒØ¨ÙŠØ±Ø© Ø¨ÙŠÙ† #Ù†ÙˆØ§Ù„_Ø§Ù„Ø²ØºØ¨ÙŠ Ùˆ #Ø£Ù…Ù„_Ø­Ø¬Ø§Ø²ÙŠ  ÙˆÙ…Ø´Ø§ÙƒÙ„ ÙˆØºÙŠØ±Ø© Ù…ØªØ¨Ø§Ø¯Ù„Ø© Ù…Ù† Ù‡ÙŠ Ù†Ø¬Ù…Ø© Ø´Ø±ÙƒØ© #Ù„Ø§ÙŠÙ_Ø³ØªØ§ÙŠÙ„Ø²_Ø§Ø³ØªÙˆØ¯ÙŠÙˆØ²#ØªÙˆÙ‚Ø¹Ø§Øª2017\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Ø±ÙˆØ¦ÙŠØªÙƒ #ØªØ±ÙŠØ­ #Ø§Ù„Ù†Ø¸Ø± #Ø³Ù…Ø§Ø¹Ùƒ #ÙŠØ±ÙŠØ­ #Ø§Ù„Ø§Ø¹ØµØ§Ø¨ #ÙƒÙ„Ù…Ø§ØªÙƒ #Ø¨Ù„Ø³Ù… #Ø§Ù„Ø¬Ø±ÙˆØ­ ÙŠ #Ù†Ø¬Ù…ØªÙŠ #Ø§Ù„Ø°Ù‡Ø¨ÙŠØ© #Ù†ÙˆØ§Ù„_Ø§Ù„Ø²ØºØ¨ÙŠ #Ø¨Ø­Ø¨Ùƒ @userâ€¦ http\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"#star_news@Cheb_Khaled_ Ø±Ø¯ Ø§Ù„Ø´Ø§Ø¨ Ø®Ø§Ù„Ø¯ Ø¹Ù„Ù‰ Ù†ÙˆØ§Ù„ Ø§Ù„Ø²ØºØ¨ÙŠ Ù…Ø¶Ø­Ùƒ Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡  Ù†ÙˆØ§Ù„ Ù„Ø§Ø²Ù… Ù…Ø§ ØªØªÙƒÙ„Ù…ÙŠ Ø¹Ù„Ù‰ Ø§Ø³ÙŠØ§Ø¯Ùƒhttp\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Ø¹Ù… Ø¨Ø­ÙƒÙŠ Ù…Ø¹ Ø­Ø§Ù„ÙŠ #Ù†ÙˆØ§Ù„_Ø§Ù„Ø²ØºØ¨ÙŠ http\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Ù„Ùˆ Ø§Ø´ÙˆÙÙ‡ Ù‚Ø¯Ø§Ù…ÙŠ Ø°Ø¨Ø­ØªÙ‡ Ø¨Ø³ Ø·Ø¨Ø¹Ø§ Ù…Ùˆ Ù‚Ø¨Ù„ Ø§Ù„ØªØ­Ù‚ÙŠÙ‚  ğŸ˜…ğŸ˜… @user #nadinenassibnjeim #nnn #star #actressâ€¦ http\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user - Ø¢Ù„Ø¹Ø¢Ù‚Ù„ Ø¢Ù„Ø¢Ú¯Ø«Ø± Ø¬Ù†ÙˆÙ†Ø¢ Ø¨Ø¢Ù„Ø°Ù‡Ø¨ÛŒØ©Ø© #Ù†ÙˆØ¢Ù„_Ø¢Ù„Ø²ØºØ¨ÙŠ â¤ğŸ˜ğŸ’—ğŸ’— http\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user Ù‡Ù†Ø§Ùƒ Ù„Ø¹Ø¨Ø© ÙˆØªØ¨Ø§Ø¯Ù„ Ø§Ø¯ÙˆØ§Ø± Ø¨ÙŠÙ† Ù…ÙŠØ´Ø§Ù„ Ø¹ÙˆÙ† ÙˆØ­Ø²Ø¨ Ø§Ù„Ù„Ù‡ Ù„Ø®Ø¯Ø§Ø¹ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠÙ‡ Ø­ØªÙ‰ ÙŠØ­ØµÙ„ÙˆØ§ Ø¹Ù„Ù‰ Ø§Ù„Ù‡Ø¨Ø© Ø§Ù„Ù…Ø§Ù„ÙŠÙ‡ ÙØ­Ø²Ø¨ Ø§Ù„Ù„Ù‡ ÙÙŠ Ø¶Ø§Ø¦Ù‚Ù‡ Ù…Ø§Ù„ÙŠÙ‡ .\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user - ïº‘ï»´ï»¦ Ú¯ÙÙ„ \\\" ï»§Ùï»”ïº² ï»“ï»´Ù‘ï»¨ï»² \\\" ØµÙÙˆØªÚ¯Ú¯\\\"  #Ù†ÙˆØ¢Ù„_Ø¢Ù„Ø²ØºØ¨ÙŠ #Ø¢ØºÙ„Ù‰_Ø¢Ù„Ø­Ø¨Ø¢ÙŠØ¨ ğŸ’™ğŸ’—ğŸ’—ğŸ˜â¤ http\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"-Ø¨ÙŠØ§Ø± Ø±Ø¨Ù‘Ø§Ø·: Ù„Ø§ Ù‡Ù„Ù‘Ù‚ Ø¨Ø¯ÙŠ Ø§Ø³Ø¦Ù„ÙƒØŒ Ù…ÙŠÙ† ØµØ¯ÙŠÙ‚Ùƒ Ø£ÙƒØªØ± \\\" Ø§Ù„Ø±Ù‘Ø¦ÙŠØ³ Ù…ÙŠØ´Ø§Ù„ Ø¹ÙˆÙ† Ø£Ùˆ Ø³Ù„ÙŠÙ…Ø§Ù† ÙØ±Ù†Ø¬ÙŠØ©ØŸ\\\"Ù…Ø´ Ø¹Ù„ÙŠÙ†Ø§ Ù‡Ø§Ù„Ø­Ø±ÙƒØ§Øª. Ø¥Ù†Ù‘Ùˆ Ø§Ø¹ØªØ±Ù Ø¨ØºÙ„Ø·ØªÙƒ #Ø¨Ù„Ø§_Ù‡Ø¨Ù„Part 2\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user - Ø¹Ù„Ø¢Ø¬ ï»ŸÙï» ï»¤ïº°Ø¢ïº Ø¢ï»Ÿï»¤ïº® ï»“ï»² ØµÙÙ€ÙˆØªÚ¯Ú¯  ğŸ’›ğŸ’—ğŸ’™â¤ğŸ’™#Ù†ÙˆØ¢Ù„_Ø¢Ù„Ø²ØºØ¨ÙŠ #Ú¯ÙÙ„_ÛŒÙˆÙ…_Ø¬Ù…Ø¹Ø©Ø© ğŸ’—ğŸ’› http\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"ÙˆØ¯Ø§Ø¯ Ø¬Ø§Ø¨Ø±: Ù…ÙŠØ´Ø§Ù„ Ø¹ÙˆÙ† Ù„Ù† ÙŠÙƒÙ…Ù„ ÙˆÙ„Ø§ÙŠØªÙ‡ #Ù„Ù„Ù†Ø´Ø±\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"I liked a @user video from @user http Ù†ÙˆØ§Ù„ Ø§Ù„Ø²ØºØ¨ÙŠ Ùˆ ÙˆØ§Ø¦Ù„ ÙƒÙÙˆØ±ÙŠ Ù…ÙŠÙ† Ø­Ø¨ÙŠØ¨ÙŠ\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"ÙˆÙ…Ù† ØºÙŠØ± #Ø§Ù„Ø±Ø¦ÙŠØ³_Ù…ÙŠØ´Ø§Ù„_Ø¹ÙˆÙ† ÙŠØ³ØªØ£Ù‡Ù„ Ø°Ù„Ùƒ ÙÙŠ ÙˆØ¬Ù‡ Ø§Ù„ÙØ³Ø§Ø¯ ÙˆØ§Ù„Ø­Ø±Ø§Ù…ÙŠÙŠ  #Ø¬Ù‡Ø²ÙˆØ§_Ø³ÙˆØ§Ø¹Ø¯ÙƒÙ… http\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Ù†ÙˆØ§Ù„ Ø§Ù„Ø²ØºØ¨ÙŠ  ØµÙˆØª Ø§Ù„Ù‡Ø¯ÙˆØ¡ â¤ï¸â¤ï¸\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Ø±Ø£Ù‰ Ø§Ù„Ø±Ø¦ÙŠØ³ Ù…ÙŠØ´Ø§Ù„ Ø¹ÙˆÙ† Ø£Ù† (Ø§Ù„Ø±Ø¦ÙŠØ³ Ø§Ù„Ø³ÙˆØ±ÙŠ) Ø¨Ø´Ø§Ø± Ø§Ù„Ø£Ø³Ø¯ Ù„Ùˆ Ø®Ø³Ø± Ø§Ù„Ø­Ø±Ø¨ØŒ Ù„ØªØ­ÙˆÙ„Øª Ø³ÙˆØ±ÙŠØ© Ø¥Ù„Ù‰ Ù„ÙŠØ¨ÙŠØ§ Ø«Ø§Ù†ÙŠØ©\\\"ØŒ\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Ù†ÙˆØ§Ù„ Ø§Ù„Ø²ØºØ¨ÙŠ Ù…Ù† Ø§Ù„Ø­Ø¬Ø§Øª Ø§Ù„ÙŠ Ù‡ØªÙØ¶Ù„ Ø¬Ù…ÙŠÙ„Ù‡ Ø¹Ù„ÙŠ Ø·ÙˆÙ„...ğŸ˜ğŸ˜ğŸ˜ğŸ˜ http\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    }\n",
    "]\n",
    "\n",
    "parsed_english = [\n",
    "    {\n",
    "        \"tweet\": \"Trying to have a conversation with my dad about vegetarianism is the most pointless infuriating thing ever #caveman \",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user You are a stand up guy and a Gentleman Vice President Pence \",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user @user @user Looks like Flynn isn't too pleased with me, he blocked me. You blocked by Flynn too @user \",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"i'm not even catholic, but pope francis is my dude. like i just need him to hug me and tell me everything is okay. \",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user for al the crying you do about how middle America is left out-they have twice as much voting power \",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Samsung to Bring Android 7.0 Nougat to Galaxy S6, S6 edge, Note 5, and Tab S2 - Softpedia News \",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Israel deems comatose Gaza man who needs treatment in West Bank  a security threat. #Palestine  via @user \",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"I will go so far to say s1 of westworld isn't just good, it's brilliant. A story within a story within a story about storytelling \",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Iraqi Forces set to storm 3 areas of #Mosul, #AlQahira, #alMasarif &  #alAmn#MosulOps#mosuloffensive#iraq#ISISâ€¦ \",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"#NationalFastFoodDay Would love to live there. Chick-fil-A ğŸ˜ \",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Electoral College must reject Trump #rejecttrump #notmypresident #takingbackdemocracy #blacklivesmatter #nevertrump \",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"The Reputation Doctor weighs in on Tony Romo #NFL @user joins @user on #TheMorningRush LISTEN: \",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Stop #fracking #Cuadrilla persecuting land defenders through the legal system #BankruptCuadrilla #ECOCIDE \",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"So proud of way @user & #trumpTransitionteam are molding strong leadership group for #America #TeamTrump #MakeAmericaGreatAgain \",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user Everyone's thinking far too short-termed. When all fossil fuels run out with no substitutes, then the crisis will come. \",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user @user - #ScreamQueens so lucky to get to work with TK. \",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user @user @user Why is it that today society is casually comfortable about being pubicly distasteful?#deplorables \",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"This is a big deal and a smart move by Microsoft: \",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Ben Carson for Housing & Urban Development?? ğŸ˜ I just can't ğŸ˜’ \",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Digesting while watching #ScreamQueens s1 Thanksgiving & Black Friday eps. SO SO GOOD!! \",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Israel's New Racism: Persecution of African Migrants in Holy Land #gaza #palestine #israel #BDS \",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"I've been listening to Leonard Cohen nonstop for the past month. Only to find out he passed literally weeks ago. Wow my heart is so overwhel \",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user She just didn't get them in areas were she needed them. Lots of voter suppression going on. Hacking & tamperingğŸ’™ \",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Listen to #NBAwards Winner @user interview on @user \",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    }\n",
    "]\n",
    "\n",
    "parsed_french = [\n",
    "    {\n",
    "        \"tweet\": \"Royal: le prÃ©sident n'aime pas les pauvres? \\\"c'est n'importe quoi\\\" http â€¦\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"#press #seo: Nager vert avec les piscines Ã©cologiques http #piscineÃ©cologique\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"#BiodiversitÃ© #Gard : Des LÃ©zards ocellÃ©s dÃ©truits sans autorisation Ã  Saint-Gilles ! http\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Etat des lieux et analyse des mÃ©canismes de soutien public aux Ã©nergies renouvelables en France â–ºâ–º http\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Les espÃ¨ces disparaissent 1.000 fois plus depuis que l'Homme existe | @user http\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Une entreprise allemande propose une solution Ã©co-efficace de ferme urbaine http #NRJ(Canada) #RegionsofFrance\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"RÃ©chauffement : lâ€™ONU prÃ©dit un vÃ©ritable Â«enfer climatiqueÂ» dâ€™ici 2050 http http\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user @user Ah Oui, les Ã©oliennes, pourrait aussi les Ã©loigner .. ouf Merci...J'en veux une dans ma cour...\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Info: Ã‰oliennes - Le rÃ©sultat de lâ€™enquÃªte publique est tombÃ© pour Esnes et MontzÃ©ville : avis dÃ©favorable\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"RÃ©chauffement climatique: il est grand temps d'agir - http\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"#GCO : les Ã©cologistes strasbourgeois sont \\\"stupÃ©faits\\\" - http\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"J'aime ma bio parce que je mange bio et je travail dans la biologie et je roule en biocarburant.\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"PolÃ©mique autour de lâ€™abattage massif de bouquetins en Haute-Savoie | Eco(lo) http\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Echange autour du thÃ¨me de l'agriculture raisonnÃ©e Ã  la RÃ©union http\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Un rÃ©seau de journalistes pour lutter contre les nÃ©gationnistes du changement climatique. http\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user \\\"IllimitÃ©es\\\", cela fera sourire les environnementalistes. Et le thÃ¨me du dÃ©clin amÃ©ricain est rÃ©curent depuis les annÃ©es 1970...\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Lâ€™exportation de lâ€™Ã©nergie Ã©olienne, une source de profits boudÃ©e par Hydro-QuÃ©bec  | Michel LagacÃ©... http\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user ah Ã§a c'est sÃ»r. J'ai fais du volontÃ©riat en Ã©cologie.\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Lâ€™exportation de lâ€™Ã©nergie Ã©olienne, une source de profits boudÃ©e par Hydro-QuÃ©bec Â |Â MichelÂ LagacÃ© http\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"En France aussi, la police / gendarmerie est au service d'une minoritÃ©... http #Sivens #Ã‰cologie http\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"La lutte contre la mosaÃ¯que du manioc aux Comores: La sÃ©lection variÃ©tale pour lutter contre la mosaÃ¯que du manioc http\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Il nous sort \\\"J'vais pas allumer la lumiÃ¨re parce que je veux pas faire marcher des centrales Ã©lectriques.\\\" Ah.\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Cuba: Mesures contre les effets du changement http #Cuba\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user @user @user Je mets au dÃ©fis ce dÃ©putÃ© bobo Ã©cologiste d'aller au chantier de Saint-Nazaire #chiche\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    }\n",
    "]\n",
    "\n",
    "parsed_german = [\n",
    "    {\n",
    "        \"tweet\": \"@user korrekt! Verstehe sowas nicht...\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Der Dubbletimepart von Julien war ja mal sowas von genial! :D (@user http\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"#Instachallenge #Day16 #what #i #am #reading #Fratzensammler #Horror #Wattpad http\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user ah. Hatte nur bis radolfzell geschaut wegen ticket. Aber so isses fast normal ;)\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user Ãœberall lauert Gefahr. Unverhofft. #Achtung\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Sonntag âœ” Fitness âœ” Nichts tunâœ”\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user da werden wir uns wohl nicht einig werden, befÃ¼rchte ich!\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user Toll. Sehr lÃ¶blich. Sieht echt klasse aus.\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"\\\"Absurde Unwahrheiten\\\" -HoeneÃŸ stocksauer auf den \\\"Stern\\\" http #Absurde #Nachrichten #N24\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"RT @user: Linda, perfeita, guapa, WunderschÃ¶n, Bella, Beautiful !! http\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Dank @user spiele ich jetzt ohne ScheiÃŸ Candy Crush Saga ... es regt mich maÃŸlos auf aber aufhÃ¶ren kann ich auch ned -.-\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Andreas SpÃ¤ck aus der Crowd sucht ein originelles Hochzeitgeschenk... ;-) Wer kann helfen? Merci ! Chuy You... http\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"1\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Nicht nur gg BÃ¼rger, sondern auch gg eigene Mitarbeiter mobben die BehÃ¶rden. Diese FÃ¤lle bearbeitet http\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user OMG Titanium! Unbelievable Jahmene\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"UnnÃ¶tiger scheiÃŸ. Ernsthaft.\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Bei http lï¿½uft zur zeit EminemSing For The Moment also schnell einschalten :)\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"RT @user: liebe @user, dein Duschgel schmeckt nicht #tranzparenztweet\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user Aw, na gut, dann schlaf fein, klein Delalein :)\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user kann man viel gelesen haben davon, nona, aber grundsÃ¤tzlich so natÃ¼rlich lÃ¤cherlich, die Liste! ff\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user kale chips= life\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"ArschlÃ¶cher!!! MOBIL http\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"RT @user: Das beste kommt zum Schluss? IlsÃ¶: \\\"Manchmal kommt der Beste auch erst in den letzten 45min.\\\"â€¦ http\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"RT @user: Wach liegen, Gedankenkarussell, unbeantwortete Fragen, Bangen, Hoffen, Zweifeln, Stolz, Verblendung, Leere, Einsamkeitâ€¦\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user Mich haben se nach vorne geschickt, konnt ne stille stehen ^^\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    }\n",
    "]\n",
    "\n",
    "parsed_hindi = [\n",
    "    {\n",
    "        \"tweet\": \"CONGRESS na ye party kabhi bani hoti na india ka partition hota nd na hi humari country itni khokhli hoti   @ \",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"ha ha ha ha ha ha ha .  .  .  .  . :d ye mast tha .  .  .  . cute aur comic .  .  .  . :)\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"kejariwal tum apana soch ka dayra badho ab is misunderstanding se kam nahi chalane wala hai tumhara\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"jai hind\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"result me konsa bc univrsty tope mari ... jo itna wait kr rha h \",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"rt @mukhijanidhi: hai teri hasti aisi jo dekhe wo mar mite #msgyouthicon #msgrevolution\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Dhyan rakh ?? \",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"aisa school ho to me kabhi ghar hi na jau .  .  . \",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"pratiyogita darpan ki spelling mistake hai :)\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"i'll never forget that first girl i was crazy about in 5th grade .  i still got her valentine day card in my secret safe . \",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"BC porn site ki tarah advertise karo tum bas ..... !!!! male hoe .. \",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"hahaha sai m .  .  ek baar class se nikal jate the to pura school naap ke wapas ate the\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"post of the night: anonymous (id: excv6jkr) 11/02/12(fri)00:52:39 no . 434329650 this thread is like watching a monkey fuck a coconut\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Jaanma main bol rahi hu ki,tum mere twits dekho :/\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Did \",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"rt psharma2525: rt komalinsaan: gurmeetramrahim #lovetoseemsg #msgincinemas schi agr duniya ye movi dekh le to yaha swrg bn skta h\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Woh  bhi \",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"rare ,  300kg meteorite discovered in poland ,  biggest in eastern europe and may provide clues about earth's core: http/URL\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"band karo ye atyachaar. #indvsuae\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"durdarshan wah wah  .  .  .  . \",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Ruk \",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Salman bai ap kese hen ma b ap ka fen hun\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"aur jab koi ek excuse maar deta tha toh dusra kehta saale ye maine socha hua tha tune kyun bol diya\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"kiranji ko delhi ka cm ummidwar ghosit karna modiji ki doodarshita wa samghdaari ko darshata hai . \",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    }\n",
    "]\n",
    "\n",
    "parsed_italian = [\n",
    "    {\n",
    "        \"tweet\": \"@user @user Ma Ferrero? il compagno Ferrero? ma il suo partito esiste ancora? allora stiamo proprio frecati !!!\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Non vedi l'ora che venga qui almeno lo sentirÃ² piÃ¹ spesso e potrÃ² finalmente stare con lui alla faccia di chi mi ha friendzonato.\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user @user solo che poi arriva @user e decide sempre tutto lui\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user Tutta la pasticceria! ğŸ’ Grazie amore mio! â¤\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Quanto mi stai cazzo troia\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"PerchÃ© alla fine siamo una famiglia e la famiglia non va mai abbandonatağŸ’• #WeWillMeetAgain1D http\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user @user @user @user @user calcio Ã¨ la quintessenza della noia. Sport frenetici come basket o volley no\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user Ciao!Mia sorella ha un canale YouTube che si chiama corinnelife97potresti visitarlo e iscriverti se ti piace?GrazieğŸ’š\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"cazzo sono tutti sti tweet in tl di \\\\\\\"innocente\\\\\\\" \\\\\\\"colpevole\\\\\\\" ao\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"La bellezza della mie Steve Madden Fringly ğŸ’— http\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user perÃ² ha detto che Ã¨ collegato positivamente alla pedofilia quindi ci vede qualcosa di buono\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"La piÃ¹ grande prova d'amore Ã¨ sempre stata togliersi dai coglioni.\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user @user Quando invece riaprii i miei occhi mi accorsi che non era un sogno...Era tutto nitido e reale!Grigio e freddo...\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"â€“ tomlinsik;Profilo molto carino in generale abbinato molto bene.\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Io faccio di tutto e lei ha ancora la faccia di ignorarmi perchÃ© sono cosÃ¬ stupida? L'avrei dovuta mandare a fanculo da tempo.\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user @user @user @user @user @user prima ora e dopo ğŸ˜ğŸ˜ğŸ˜š\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user quanto ti odio da uno a dieci\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user @user @user ti vogliamo bene anche noi Harry\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Io mi lamento della gente che scrive ancora \\\\\\\"freddy mercury\\\\\\\" ma anche quella che scrive \\\\\\\"jhonny cash\\\\\\\" non scherza\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Oggi cominciamo cosÃ¬! Con i nostri saluti per voi di venerdÃ¬ scorso dopo \\\\\\\"Notre Dame de Paris\\\\\\\". http\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Dal prossimo anno Audi non sarÃ  piÃ¹ uno sponsor del Milan il testimone passerÃ  alla Piaggio...e via con gli Apetti!!\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"IL MIO DIARIO - Pagine d'estate quando ti innamori piano piano dietro uno schermo. Grazie Greta. @user â¤ï¸ http\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user @user il fatto che una persona che vada in discoteca implichi che questa si droghi Ã¨ una fesseria...\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user che la forza sia con noi\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    }\n",
    "]\n",
    "\n",
    "parsed_portuguese = [\n",
    "    {\n",
    "        \"tweet\": \"todos os meus favoritos na prova de eliminaÃ§Ã£o #MasterChefBR\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Augusto Cury Ã© o carağŸ˜‰ entrevista myito boa do #ConversaComBial\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Mano Vitor, tÃ¡ muito feio! DecepÃ§Ã£o. #MasterChefBR\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Morrendo de amores pelo @user no programa do @user. #TheNoite\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"MIRIAN CHOORANDO PELA PROVA PASSADA #MasterChefBR\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"a cada episÃ³dio q passa Fabrizio continua um gato ne ğŸ˜ #MasterChefBR\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Traz o @user e tira a Mirian Pvr    #MasterChefBR\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"A maior e melhor cantora que vocÃª respeita @user no @user. #SandyLeah #Encontro #SandynoEncontro\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Imagina que insuportÃ¡vel ter de dar de comer pra uma gente que calcula CADA CALORIA que come? Jesus... #MasterChefBR\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Baixa caloria? Frango desfiado e salada. TÃ¡ Ã³timo  #MasterChefBR\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"AÃ­ que agonia essa Marian, meu Deus do cÃ©u, mulher!! #MasterChefBR\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"\\\"Eu adoro fazer piquenique no TEMPROâ€  haha te amo yuko #MasterChefBR\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"\\\"uma mulher mais nova com um homem mais velho Ã© normal\\\" normal dizer que ela ta com ele por dinheiro nÃ©? #Encontro\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Que amor esse @user no #Encontro â¤â¤â¤\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Vanessa da Mata acabou com a fofurice do parto da mala da Bela Gil kkkkkk #AltasHoras\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"fÃ¡tima bernardes maravilhosa no #VideoShowAoVivo\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"AlÃ´ NET, voces tÃ£o fora do ar?! Arrumem por favor que eu quero assistir #MasterChefBR hojeeee\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"#MasterChefBR Hummm deu vontade de comer uma sardinha com pÃ£ozinho kkk partiu abrir uma latinha! ğŸ˜\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Pq o programa nÃ£o ta ao vivo? @user  #MaisVocÃª\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Mr Catra dando um show no The Noite ! Falando da polÃ­tica no paÃ­s ğŸ‘ #TheNoite @user @user\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Se alguÃ©m me fala que Ã© termogÃªnico, eu devolvo o prato na hora #MasterChefBR\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Ai eu me derreto todinha com a PaolağŸ˜­ğŸ˜ #MasterChefBR\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"#AltasHoras assistindo o programa com uma cx de lenÃ§os ao lado! Hajaâ¤ğŸ˜¢\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Legal estÃ¡ prova tomara que tenha mais #DomingoLegal\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    }\n",
    "]\n",
    "\n",
    "parsed_spanish = [\n",
    "    {\n",
    "        \"tweet\": \"@user jajajaja dale, hacete la boluda vos jajaja igual a vos nunca se te puede tomar en serio te mando un abrazo desde PerÃº!\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user MAAAAE RAJADO! Pero lo bueno es q uno se va independizando!y logrando metas\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Bueno hoy fui a almorzar a Nanay con otras 3 dras xq la capacitaciÃ³n mal organizada no nos diÃ³ almuerzo y encima nos mandan a comer 2pm\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user Â¡Hola TomÃ¡s! Â¿HabÃ©is visto los nuevos #dinos de #TierraMagna? Es normal que haya colas antes de que comience el espectÃ¡culo\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user la hijueputa tela se me saliÃ³. yo querÃ­a volver a quedar acostada.\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user pues no estÃ¡ nada mal\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user quizÃ¡ para profesionales no sea mucho,pero hay no remunerados principalmente femenino para quienes es un sueÃ±o, pasa en mi paÃ­s\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Hora de seguir soÃ±ando muy bonito\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Me estoy muriendo. OjalÃ¡ mi jefa me haga trabajar viernes y sÃ¡bado en la maÃ±ana. Porque cerrar va a ser fatal\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"La felicidad tiene un nombr #Tailandia  Si es tu PRIMERA VEZ, apunta estos #consejos!   http http\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user @user justamente ahÃ­ es tu equivocada suposiciÃ³n, qxq estudiÃ³ en 1 colegio ficho serÃ¡ pulcro? La religiÃ³n le importa mda\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user Alpha suerte y ten un feliz aÃ±o 2017\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Que viene Nonpa y fijo es para mayores. Ya lloro.\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user pues sÃ­, sÃ© lo que se siente. Pero piÃ©nselo como el Ãºltimo esfuerzo, que ya ahorita llegan las vacaciones\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user @user @user @user HAHAHAHHA MMM NOS VAMOS EN 2 semanas y no tenemos ni hotel Pa la primera noche ğŸ˜‚ğŸ˜‚ğŸ˜‚\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user Que bonito,y yo tengo una sorpresa para Ti ,que te gustara muchÃ­simo!\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"No me sorprende que 2ne1 se haya separado, man. Pero aÃºn asÃ­ me dio en el cora bc es un grupo legendario y empecÃ© mi era kpop con ellas\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user pero yo te kiero\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user tu te planteas comprar grÃ¡fica. Yo si lo planteo tengo que comprar todo, mi pc es un venerable anciano\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Cambiar \\\"El mejor sistemaâ„¢\\\" y la educaciÃ³n mÃ©dica en EspaÃ±a serÃ¡ complicado, pero con gente como vosotros tengo fe  @user @user\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user @user fallecido hace meses. Hay q contrastar.\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"Que haya una planta de tamarindo afuera de la que posiblemente va a ser la casa en la que vivas de vez en cuando, es una buena seÃ±al\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"no me deis confianza que luego os digo las cosas muy claras y os jode\",\n",
    "        \"actual-sentiment\": \"0\",\n",
    "        \"chatgpt-sentiment\": \"0\",\n",
    "        \"llama2-sentiment\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet\": \"@user ay, gracias. Lo malo es que ese es solo para primer viaje. Pero mil gracias igualmente\",\n",
    "        \"actual-sentiment\": \"2\",\n",
    "        \"chatgpt-sentiment\": \"2\",\n",
    "        \"llama2-sentiment\": \"2\"\n",
    "    }\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Stuff, pt. 2.\n",
    "\n",
    "##### **Commented-Out Code from A3** (BERT & GPT-2 Pretrained Language Model-based Text-Classification Performance, Metrics, & Results)\n",
    "\n",
    "```python\n",
    "##### CS410: Natural Language Processing, Fall 2023 - 11/13/2023\n",
    "##### A3: Pretrained Language Models (PLMs), Dan Jang - Main Implementation\n",
    "#### Objective: Exploring Natural Language Processing (NLP), by building a text-classifier\n",
    "#### for a text classification task, predicting whether a piece of text is \"positive\" or \"negative.\"\n",
    "#### ...focusing on two (2) pretrained language models (PLMs),\n",
    "#### ...BERT (Bidirectional Encoder Representations from Transformers) & OpenAI's GPT-2 (Generative Pretrained Transformer),\n",
    "#### ...and using the respective toenizers to each PLM to perform the text-classification task as aforementioned\n",
    "\n",
    "### 1.1.a) Logistic Regression algorithm using sklearn.linear_model.LogisticRegression\n",
    "### https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "### Returns four (4) thingys:\n",
    "# I.) accuracy_score,\n",
    "# II.) f1_score,\n",
    "# III.) confusion_matrix,\n",
    "# & IV.) classification_report.\n",
    "def plm(model, xtrain, ytrain, xtest, ytest):\n",
    "    \n",
    "    lreg = LogisticRegression()\n",
    "    \n",
    "    lreg.fit(xtrain, ytrain)\n",
    "    predictionresults = lreg.predict(xtest)\n",
    "    \n",
    "    return accuracy_score(ytest, predictionresults), f1_score(ytest, predictionresults), confusion_matrix(ytest, predictionresults), classification_report(ytest, predictionresults)\n",
    "\n",
    "### A3-Specific: Implementation functions for implementing the two pretrained language models, BERT & GPT-2.\n",
    "## Pretrained Language Model (PLM) Tokenizer Implementation Function\n",
    "def plmodel(words, model, tokenizer):\n",
    "    if tokenizer.pad_token is None:\n",
    "        raise ValueError(\"[Debug A3.1 - PLModel()]: Tokenizer has no padding token for the current model!\")\n",
    "    wordlist = tokenizer(words, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        results = model(**wordlist)\n",
    "    \n",
    "    return results.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    # wordlist = words.split()\n",
    "    # vecs = [model[w] for w in wordlist if w in model]\n",
    "    # if vecs:\n",
    "    #     return sum(vecs) / len(vecs)\n",
    "    # else:\n",
    "    #     return [0] * model.vector_size\n",
    "    \n",
    "# ## GPT(ransformer)-2 Model Implementation Function\n",
    "# def bumblebee(words, model):\n",
    "#     beds = [avgwordvec(w, model) for w in words]\n",
    "#     return beds\n",
    "\n",
    "def main(): #trainfile, testfile):\n",
    "    print(\"Welcome, this is the main program for A3: Pretrained Language Models.\")\n",
    "    print(\"Written by Dan J. for CS410: Natural Language Processing, Fall 2023.\")\n",
    "    print(\"\\nWe will use two (2) pretrained language models (PLM), BERT & GPT-2.\\n...to create a text-classifier to guess negative or positive sentimentiality based on various text-reviews of products.\")\n",
    "\n",
    "    # 1.0.I.A) Debug Statements #1a for dataset loading times:\n",
    "    print(\"\\nLoading the training & testing datasets...\")\n",
    "    # with open(trainfile, \"r\") as trainfile:\n",
    "    with open(\"sentiment_train.json\", \"r\") as trainfile:\n",
    "        #traindata = json.load(trainfile)\n",
    "        for row in trainfile:\n",
    "            traindata.append(json.loads(row))\n",
    "        \n",
    "    trainframe = pandas.DataFrame(traindata)\n",
    "        \n",
    "    # with open(testfile, \"r\") as testfile:\n",
    "    with open(\"sentiment_test.json\", \"r\") as testfile:\n",
    "        #testdata = json.load(testfile)\n",
    "        for row in testfile:\n",
    "            testdata.append(json.loads(row))\n",
    "        \n",
    "    testframe = pandas.DataFrame(testdata)\n",
    "\n",
    "    # 1.0.I.B) Debug Statements #1b for dataset loading times:\n",
    "    print(\"Successfully loaded the training & testing datasets!\\n\")\n",
    "    \n",
    "    ## 1.0.1.) Initial Preprocessing of the training & testing data\n",
    "    ## First, we isolate our two (2) columns, \"review_title\" & \"stars.\"\n",
    "    ## Second, we will convert values in the \"stars\" column so that 1 [negative] = 0 & 5 [positive] = 1.\n",
    "    ## This will allow us to make the negative or positive sentiment a binary value-based thingy.\n",
    "    trainframe = trainframe[['review_title', 'stars']]\n",
    "    trainframe['stars'] = trainframe['stars'].apply(lambda x: 1 if x == 5 else 0)\n",
    "    \n",
    "    testframe = testframe[['review_title', 'stars']]\n",
    "    testframe['stars'] = testframe['stars'].apply(lambda x: 1 if x == 5 else 0)\n",
    "    \n",
    "    ## A3-Specific: From our Slack channel (#nlp_f23), using tip to only use 25% of training dataset, evenly split\n",
    "    ## Credits to Classmate Will McIntosh from the Slack thread started by classmate Saurav Kumar Singh\n",
    "    ## & also, full credits to classmate Will McIntosh for the following code for GPU usage:\n",
    "    \n",
    "    #### Credits to Will McIntosh (11/11/2023):\n",
    "    # Testing\n",
    "    print(f\"Is a GPU available: {torch.cuda.is_available()}\")\n",
    "    #print(f\"Is this instance using a GPU?: {next(model.parameters()).is_cuda}\")\n",
    "    #### From Slack, #nlp_f23.\n",
    "\n",
    "    trainframe = trainframe.sample(frac=1, random_state=69)\n",
    "    trainframe = trainframe.iloc[:int(0.25 * len(trainframe))]\n",
    "    \n",
    "    # y2train = trainframe['stars']\n",
    "    # print(\"[A3 Debug Size-Print #3] y2train\", len(y2train))\n",
    "    \n",
    "    ytest = testframe['stars']\n",
    "    # print(\"[A3 Debug Size-Print #4] ytest\", len(ytest))\n",
    "    \n",
    "    ## Evenly split frames\n",
    "    x3train1, x3train2 = train_test_split(trainframe, test_size=0.5, random_state=69)\n",
    "    \n",
    "    y3train1 = x3train1['stars']\n",
    "    y3train2 = x3train2['stars']\n",
    "    \n",
    "    #print(\"[A3 Debug Size-Print #1] x3train1 & x3train2\", len(x3train1), len(x3train2))\n",
    "    ## A3-Specific: Applying BERT & GPT-2 PLM Specific Tokenization\n",
    "    print(\"Tokenizing the training & testing datasets for BERT...\")\n",
    "    x2train1 = x3train1['review_title'].apply(lambda x: plmodel(x, bertie, bertie_tokens))\n",
    "    #x2train1 = trainframe['review_title'].apply(lambda x: plmodel(x, bertie, bertie_tokens))\n",
    "    x2test1 = testframe['review_title'].apply(lambda x: plmodel(x, bertie, bertie_tokens))\n",
    "    print(\"BERT Tokenization has been applied to its training & testing datasets!\")\n",
    "    \n",
    "    #print(\"[A3 Debug Size-Print #2a] x2train1 & x2test1\", len(x2train1), len(x2test1))\n",
    "    \n",
    "    print(\"Tokenizing the training & testing datasets for GPT-2...\")\n",
    "    x2train2 = x3train2['review_title'].apply(lambda x: plmodel(x, bumblebee, bumblebee_tokens))\n",
    "    #x2train2 = trainframe['review_title'].apply(lambda x: plmodel(x, bumblebee, bumblebee_tokens))\n",
    "    x2test2 = testframe['review_title'].apply(lambda x: plmodel(x, bumblebee, bumblebee_tokens))\n",
    "    print(\"GPT-2's tokenization has been applied to its training & testing datasets!\")\n",
    "    \n",
    "    #print(\"[A3 Debug Size-Print #2b] x2train2 & x2test2\", len(x2train2), len(x2test2))\n",
    "\n",
    "    ### 1.0.2b) Run Text-Classification Algorithms & Print the Model Results\n",
    "    print(\"-----\\n\")\n",
    "    print(\"Running text-classification model le training & testing datasets (with pretrained language models, BERT & GPT-2)...\")\n",
    "\n",
    "    \n",
    "    print(\"Running Logistic Regression algorithm, version A.) BERT...\")\n",
    "    bed1accuracy, bed1f1, bed1cmatrix, bed1creport = plm(bertie, x2train1.tolist(), y3train1, x2test1.tolist(), ytest)\n",
    "    print(\"..The data processing from our first PLM, BERT, is done!\")\n",
    "    \n",
    "    print(\"Running Logistic Regression algorithm, version B.) GPT-2...\")\n",
    "    bed2accuracy, bed2f1, bed2cmatrix, bed2creport = plm(bumblebee, x2train2.tolist(), y3train2, x2test2.tolist(), ytest)\n",
    "    print(\"..The data processing from our second PLM, GPT-2, is done!\")\n",
    "    \n",
    "    print(\"...All Done!\")\n",
    "    print(\"-----\\n\")\n",
    "    \n",
    "    print(\"Here are le results [Logistic Regression, with comparative results between two (2) PLMs, BERT & GPT-2]...\\n\")\n",
    "    print(\"Logistic Regression Algorithm, Version A: BERT Pretrained Language Model-based Text-Classification Performance, Metrics, & Results:\")\n",
    "    print(\"...Accuracy was found to be, \", bed1accuracy * percentness, \"%,\")\n",
    "    print(\"...F1 Score was found to be: \", bed1f1, \",\")\n",
    "    print(\"...with a Confusion Matrix: \\n\", bed1cmatrix, \",\")\n",
    "    print(\"...& lastly, the classification Report: \\n\", bed1creport)\n",
    "    print(\"-----\\n\")\n",
    "    \n",
    "    print(\"Logistic Regression Algorithm, Version B: GPT-2 Pretrained Language Model-based Text-Classification Performance, Metrics, & Results:\")\n",
    "    print(\"...Accuracy was found to be, \", bed2accuracy * percentness, \"%,\")\n",
    "    print(\"...F1 Score was found to be: \", bed2f1, \",\")\n",
    "    print(\"...with a Confusion Matrix: \\n\", bed2cmatrix, \",\")\n",
    "    print(\"...& lastly, the classification Report: \\n\", bed2creport)\n",
    "    print(\"-----\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "##### **A3 Results**: Raw Output from [*BERT*](https://arxiv.org/abs/1810.04805) & *OpenAI*'s [*GPT-2*](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) Pretrained Language Models (PLMs), using the Logistic Regression Algorithm\n",
    "    Here are le results [Logistic Regression, with comparative results between two (2) PLMs, BERT & GPT-2]...\n",
    "\n",
    "    Logistic Regression Algorithm, Version A: BERT Pretrained Language Model-based Text-Classification Performance, Metrics, & Results:\n",
    "    ...Accuracy was found to be,  91.55 %,\n",
    "    ...F1 Score was found to be:  0.9158785465405676 ,\n",
    "    ...with a Confusion Matrix: \n",
    "    [[911  89]\n",
    "    [ 80 920]] ,\n",
    "    ...& lastly, the classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.92      0.91      0.92      1000\n",
    "            1       0.91      0.92      0.92      1000\n",
    "\n",
    "        accuracy                           0.92      2000\n",
    "    macro avg       0.92      0.92      0.92      2000\n",
    "    weighted avg       0.92      0.92      0.92      2000\n",
    "\n",
    "    -----\n",
    "\n",
    "    Logistic Regression Algorithm, Version B: GPT-2 Pretrained Language Model-based Text-Classification Performance, Metrics, & Results:\n",
    "    ...Accuracy was found to be,  87.4 %,\n",
    "    ...F1 Score was found to be:  0.872598584428716 ,\n",
    "    ...with a Confusion Matrix: \n",
    "    [[885 115]\n",
    "    [137 863]] ,\n",
    "    ...& lastly, the classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.87      0.89      0.88      1000\n",
    "            1       0.88      0.86      0.87      1000\n",
    "\n",
    "        accuracy                           0.87      2000\n",
    "    macro avg       0.87      0.87      0.87      2000\n",
    "    weighted avg       0.87      0.87      0.87      2000\n",
    "\n",
    "-----\n",
    "\n",
    "### **A2 Results**: Raw Output from ```Word2Vec``` & GloVe Embedding Results, using the Logistic Regression Algorithm\n",
    "\n",
    "    Logistic Regression Algorithm, Version A: Word2Vec Pretrained Model-based Embeddings Performance, Metrics, & Results:\n",
    "    ...Accuracy was found to be,  86.3 %,\n",
    "    ...F1 Score was found to be:  0.86105476673428 ,\n",
    "    ...with a Confusion Matrix: \n",
    "    [[1754  246]\n",
    "    [ 302 1698]] ,\n",
    "    ...& lastly, the classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.85      0.88      0.86      2000\n",
    "            1       0.87      0.85      0.86      2000\n",
    "\n",
    "        accuracy                           0.86      4000\n",
    "    macro avg       0.86      0.86      0.86      4000\n",
    "    weighted avg       0.86      0.86      0.86      4000\n",
    "\n",
    "    -----\n",
    "\n",
    "    Logistic Regression Algorithm, Version B: GloVe Pretrained Model-based Embeddings Performance, Metrics, & Results:\n",
    "    ...Accuracy was found to be,  69.69999999999999 %,\n",
    "    ...F1 Score was found to be:  0.7313829787234042 ,\n",
    "    ...with a Confusion Matrix: \n",
    "    [[1138  862]\n",
    "    [ 350 1650]] ,\n",
    "    ...& lastly, the classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.76      0.57      0.65      2000\n",
    "            1       0.66      0.82      0.73      2000\n",
    "\n",
    "        accuracy                           0.70      4000\n",
    "    macro avg       0.71      0.70      0.69      4000\n",
    "    weighted avg       0.71      0.70      0.69      4000\n",
    "\n",
    "    -----\n",
    "\n",
    "#### ***From A1 Results for Reference:*** Initial Full 80k-Row Processing Results Raw Output\n",
    "\n",
    "    Algorithm #1, Version A: Gaussian NÃ¤ive Bayes Performance, Metrics, & Results:\n",
    "    ...Accuracy was found to be,  59.199999999999996 %,\n",
    "    ...F1 Score was found to be:  0.3664596273291925 ,\n",
    "    ...with a Confusion Matrix: \n",
    "    [[948  52]\n",
    "    [764 236]] ,\n",
    "    ...& lastly, the classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.55      0.95      0.70      1000\n",
    "            1       0.82      0.24      0.37      1000\n",
    "\n",
    "        accuracy                           0.59      2000\n",
    "    macro avg       0.69      0.59      0.53      2000\n",
    "    weighted avg       0.69      0.59      0.53      2000\n",
    "\n",
    "    -----\n",
    "\n",
    "    Algorithm #2, Version A: Logistic Regression Performance, Metrics, & Results:\n",
    "    ...Accuracy was found to be,  92.7 %,\n",
    "    ...F1 Score was found to be:  0.9272908366533865 ,\n",
    "    ...with a Confusion Matrix: \n",
    "    [[923  77]\n",
    "    [ 69 931]] ,\n",
    "    ...& lastly, the classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.93      0.92      0.93      1000\n",
    "            1       0.92      0.93      0.93      1000\n",
    "\n",
    "        accuracy                           0.93      2000\n",
    "    macro avg       0.93      0.93      0.93      2000\n",
    "    weighted avg       0.93      0.93      0.93      2000\n",
    "\n",
    "    -----\n",
    "\n",
    "    Algorithm #1, Version B: Gaussian NÃ¤ive Bayes Performance, Metrics, & Results:\n",
    "    ...Accuracy was found to be,  59.3 %,\n",
    "    ...F1 Score was found to be:  0.36899224806201547 ,\n",
    "    ...with a Confusion Matrix: \n",
    "    [[948  52]\n",
    "    [762 238]] ,\n",
    "    ...& lastly, the classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.55      0.95      0.70      1000\n",
    "            1       0.82      0.24      0.37      1000\n",
    "\n",
    "        accuracy                           0.59      2000\n",
    "    macro avg       0.69      0.59      0.53      2000\n",
    "    weighted avg       0.69      0.59      0.53      2000\n",
    "\n",
    "    -----\n",
    "\n",
    "    Algorithm #2, Version B: Logistic Regression Performance, Metrics, & Results:\n",
    "    ...Accuracy was found to be,  92.80000000000001 %,\n",
    "    ...F1 Score was found to be:  0.9281437125748503 ,\n",
    "    ...with a Confusion Matrix: \n",
    "    [[926  74]\n",
    "    [ 70 930]] ,\n",
    "    ...& lastly, the classification Report: \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "            0       0.93      0.93      0.93      1000\n",
    "            1       0.93      0.93      0.93      1000\n",
    "\n",
    "        accuracy                           0.93      2000\n",
    "    macro avg       0.93      0.93      0.93      2000\n",
    "    weighted avg       0.93      0.93      0.93      2000\n",
    "\n",
    "    -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook a4-Multilingual-Large-Language-Models-dan-jang.ipynb to pdf\n",
      "[NbConvertApp] Writing 378472 bytes to notebook.tex\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] Running xelatex 3 times: ['xelatex', 'notebook.tex', '-quiet']\n",
      "[NbConvertApp] Running bibtex 1 time: ['bibtex', 'notebook']\n",
      "[NbConvertApp] WARNING | b had problems, most likely because there were no citations\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 526137 bytes to a4-Multilingual-Large-Language-Models-dan-jang.pdf\n"
     ]
    }
   ],
   "source": [
    "##### Juypter Notebook -> PDF Conversion thingy\n",
    "\n",
    "#!pip install nbconvert\n",
    "\n",
    "!jupyter nbconvert a4-Multilingual-Large-Language-Models-dan-jang --to pdf "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
